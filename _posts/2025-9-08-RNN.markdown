# RNN 实现详解

本部分详细解释如何实现 RNN 模型。

---

## 📑 目录
- [1. 序列建模基础](#1-序列建模基础)
  - [1.1 序列模型](#11-序列模型)
  - [1.2 统计工具](#12-统计工具)
    - [1.2.1 自回归模型](#121-自回归模型)
    - [1.2.2 隐变量自回归模型](#122-隐变量自回归模型)
    - [1.2.3 马尔可夫模型](#123-马尔可夫模型)
    - [1.2.4 因果关系](#124-因果关系)
  - [1.3 针对代码的 feature-label 对](#13-针对代码的-feature-label-对)
  - [1.4 单步预测和多步预测](#14-单步预测和多步预测)
  - [1.5 绘图示例](#15-绘图示例)

---

## 1. 序列建模基础

### 1.1 序列模型

数据之间存在相关性，顺序的改变会影响数据含义。

---

### 1.2 统计工具

#### 1.2.1 **自回归模型**

随着时间的推移，数据量会增大。为了控制计算复杂度，引入窗口 \(\tau\)（观测序列），使用自己过去的值进行预测，因此有自回归模型：

\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t
\]

---

#### 1.2.2 **隐变量自回归模型**

保留对过去观测的总结 \(h_t\)，并基于此更新预测 \(x_t\) 和 \(h_t\)，得到：

\[
x_t = p(x_t|h_t)
\]

其中 \(h_t\) 是未被实际观测到的状态，称为**隐变量**。

![alt text](image.png)

**如何预测未来？**  
采用观测序列预测下一个时间点，即使用滑动窗口。

注意：未观测到的机制，无法从数据中学到。  
这里还有一些假设（确定性动力系统）：  
- 均值不变  
- 方差不变  
- 自相关只和间隔有关  

---

#### 1.2.3 **马尔可夫模型**

自回归模型的窗口思想是马尔可夫的近似思想：**未来只依赖最近 \(\tau\) 个状态，而不依赖更早历史**。  

特殊情况：\(\tau = 1\)，未来只依赖当前状态：  
\[
P(x_{t+1} \mid x_1,\dots,x_t) = P(x_{t+1} \mid x_t)
\]

整个序列的联合分布：  
\[
P(x_1,\dots,x_T) = P(x_1)\prod_{t=2}^{T} P(x_t \mid x_{t-1})
\]

---

#### 1.2.4 **因果关系**

当概率的公式写好后： 公式的顺序是可以准换顺序的，但是实际上从时间上来看，是无法实现。

---

### 1.3 针对代码的 feature-label 对

每个特征（feature）用前 tau 个连续的值。 针对前 tau 个数据点，无法当作 label，因为前面没有数据可以用去预测，因此。  
对数 = 总的序列长度 - tau  

---

### 1.4 单步预测和多步预测

**单步预测**是指基于已有的历史观测值预测下一个时间点的数值，每次预测后都有真实值可以直接进行对比，误差不会累积。

**多步预测**是基于历史值预测未来多个时间点，通常通过递归（预测值作为下一步输入）或直接（一次性输出多步结果）方式实现，但递归方式容易导致误差逐步放大。

在代码设计上，一种方式是基于完整时间序列进行预测，另一种是采用滑动窗口批量预测，每一行表示一个独立的起始时间点，例如第 0 行输入 \[1,2,3] 预测未来第 1、2 步，第 1 行输入 \[2,3,4] 预测未来第 1、2 步，行与行之间没有时间连续性。这样做的目的不是训练模型，也不是生成完整的未来序列，而是测试模型在不同预测步长下的性能，分析误差随预测步数增加的变化，并绘制对比曲线，帮助评估模型的短期与长期预测能力。

---

### 1.5 绘图示例

假设总的历史步长是 6：

| 历史1 | 历史2 | 1-step | 2-step | 3-step |
| ----- | ----- | ------ | ------ | ------ |
| t1    | t2    | t3     | t4     | t5     |
| t2    | t3    | t4     | t5     | t6     |
| t3    | t4    | t5     | t6     | t7     |

绘图 一个是 x ,一个是 y。

针对 3 step，首先是时间步长  
\[
\tau + (i-1) : T - \text{max\_steps} + i
\]

直接看表，第三步预测的结果是从 t5 开始，到 t7，但是我们的时间步只有 6。  
因此，最终的限制和索引来源于此。

---

## 2. 文本数据处理

文本是序列数据的一种，这里针对文本数据进行的处理方法。

- 文本转换成计算机语言中的字符串 （计算机不认识文本）
- 对字符串进行拆分 **词元**
- 以词元 为基础， 建立一个词表。实际上就是建设一个新的词典，方便模型理解，因为它不认识文本
- 还需要将词元 转换 为数字索引（十进制），最终肯定是二进制，最终进入模型理解和训练。

这边的操作 实际上就是 转译 人类语言 到 机器语言。


### 2.1 Token/

基于已有的数据进行下载。将原始的文本行处理成只包含小写字母和空格的形式
去掉非字母的字符
用空格 替代 单词间的分隔 使用 re,sub function

##### Token 

词元的基本单位，这里分为单个的词 和 字符。
比如： word/ w o r d

下面建立一个词表， 用于词元和 数字索引的 词表库。

这里构建的词语

---

## 3. 语言模型和数据集

### 核心思想
语言模型，将做好的词元 ，看作一个 长度为T 的文本序列，此时，每个位置是一个观测值。
\[
 x_1,x_2....x_T
\]

语言模型的目的就是估计该序列的联合概率分布
\[
P(x_1,x_2,...x_T)
\]

根据链式法则， 联合概率可以分解为：

$$
P(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, x_2, \dots, x_{t-1})
$$

之所以这样做，是因为联合概率的维度太高,你不可能一次性预测整篇文章，但可以一步步预测下一个词，将大问题拆解成小问题。

模型学会这个分布，就能生成文本。 分布就是 概率+ 结果 

核心思想是 统计规律预测序列概率。

### 参数

本质上来说： 既然是概率，那么组成其分散的 是 每个单词的概率 加 条件概率

针对单个单词的概率，采用简单的估计方法:

\[
P(deep) = \frac{单词次数}{总次数}
\]

对于连续两个单词都概率：

\[
P(deep |learnig) = \frac{“deep learning”出现的次数​}{deep出现的次数}
\]

因此，为了针对这种长序列的单词，采用了一个方法，叫做拉普拉斯平滑。

单词 \(w\) 的原始概率为：

\[
P(w) = \frac{\text{次数}}{\text{总词数}}
\]

### 平滑后的概率

平滑后：

\[
P(w) = \frac{\text{次数} + \alpha}{\text{总词数} + \alpha V}
\]

其中：

- \(V\) 是词表大小（即不同单词的数量）  
- \(\alpha\) 控制平滑强度  

**注意：**  
- 当 \(\alpha = 0\) 时，相当于没有平滑  
- 当 \(\alpha\) 很大时，概率接近均匀分布


很明显，一个模型只是简单地基本的统计模型是表现很差的。

### 马尔可夫模型 与 n元语法

这里就要提到之间的，一阶马尔可夫的模型的讨论
在讨论包含深度学习的解决方案之前，我们需要了解更多的概念和术语。

回想一下我们在 8.1 节中对 **马尔可夫模型** 的讨论，并将其应用于语言建模。  

如果序列满足一阶马尔可夫性质，则序列上的分布满足：

\[
P(w_n | w_1, w_2, \dots, w_{n-1}) \approx P(w_n | w_{n-1})
\]

- **阶数越高**，对应的依赖关系就越长。  
- 这种性质推导出了许多可以应用于序列建模的近似公式（公式 8.3.6）。

通常，涉及一个、两个和三个变量的概率公式分别被称为：

- **一元语法（Unigram）**  
- **二元语法（Bigram）**  
- **三元语法（Trigram）**  

一元语法 (unigram)：只看每个词出现的概率，不考虑上下文。
例如，词“猫”的概率 = 出现“猫”的次数 / 总词数

二元语法 (bigram)：看每个词出现的概率，条件是知道前一个词。
例如，P(“睡觉”|“猫”) = “猫 睡觉”出现次数 / “猫”出现次数

三元语法 (trigram)：看每个词出现的概率，条件是知道前两个词。

下面，我们将学习如何去设计更好的模型。

### Zipf 定律

这里还研究，讲解了一下关于

把“频率”做纵轴，并且同时对两轴取对数（双对数坐标）时，如果点大致落在一条直线上，就说明频率服从幂律分布，也就是齐普夫定律（Zipf’s law）。
![alt text](image-1.png)


这里，针对单个词汇。 少数词 想 介词 虚词 the of  频率很高
而我们日常生活中的词，频率很低。

这意味着 模型 的表现会很差。采用数数 和 平滑 模型很垃圾。

实际语言中，任何词理论上都有可能出现，所以要避免 0 概率，就用平滑。

因此， 基于此， 采用 词的组合。也就是我们之前提到的 多元语法。

假设，有 6 个词。 吃 苹果、吃 桌子、吃 电脑、， 两两组合的形式 有 \[6^2\]

但是，实际上存在着一定的规则，和规律回去约束，数据本身自带了规则。因此，模型自己本身就学习到其自带的规则。

### 长序列数据

针对数据处理，序列变得太长，无法一次性全部处理，为了序列方便模型读取，需要进行拆分。

该如何进行拆分， 为了进行一个良好的自由度。


针对这里，为什么采用这种数据处理：

将所有的数据 切成一段一段，然后取。

但是，如果切的都是一样的，会造成某些数据规律没有学习到。

因此，采用了一个随机起点， k. 希望将所有的顺序都学习到位。 避免模型总是看到相同的序列切分。 k 的取值是在（0，tau）。



为了获得覆盖性 和 随机性。采用随机采样， 和顺序分区 策略。

![alt text](image-2.png)


针对实际的数据处理中，采用的是 并行预测。也就意味着 

也就是 意味着 窗口 长度大小是相等的。

X 【1，2，3】
Y 【2，3，4】


数据从 corpus 加载， 实际上进行的是 如何创建一个良好的索引， 通过索引的索引去创造结构。在将数据按照对应的索引去加载进去，这部分其实挺考虑 idea 和 代码是如何实现的。


另外一个也很有意思是关于逻辑，

一个是先分批次，切缝好序列，在把数据加载进去。同时打乱批次 和 批次中的序列索引。

另外一个是先出来好序列，已经按照顺序分好 然后在分好批次，然后在每个批次中，在进行序列的切分

竖着切，切多少次就有多少个 批次。 最终得到了一个样本.



## 循环神经网络介

初始的模型是 n-gram：

\[
    P(w_t|w_1,w_2,...w_t)
\]

然而，随着 n 逐渐增大， 词表的大小指数增长，因此，这里引入了隐变量模型。：

\[
P(w_t \mid w_1, \dots, w_{t-1}) = \sum_{z_t} P(w_t \mid z_t) \, P(z_t \mid w_1, \dots, w_{t-1})
\]

使用隐藏状态去 z 总结过去所有单词对当前预测的影响

\[
    h_t = f(x_t,h_t-1)
\]

注意 隐藏层 和 隐藏状态不是一回事。就是之前所提到的关于之前的事件。

这里从单隐藏层的多层感知机去理解和学习.

设：

- 批量大小（小样本的数量）：\(n\)  
- 输入特征数：\(d\)  
- 隐藏状态维度：\(h\)  
- 输出维度：\(q\)  

---

### 隐藏层计算

\[
H = g(X \cdot W_{xh} + b_h)
\]

其中：

- \(X \in \mathbb{R}^{n \times d}\)  
- \(W_{xh} \in \mathbb{R}^{d \times h}\)  （将输入特征映射到隐藏状态）  
- \(b_h \in \mathbb{R}^{1 \times h}\)  
- \(H \in \mathbb{R}^{n \times h}\)  
- \(g(\cdot)\) 是激活函数（如 ReLU、tanh 等）

> 这里，\(W_{xh}\) 的维度保证了 \(X \cdot W_{xh}\) 输出为 \(n \times h\)，与偏置 \(b_h\) 相加时维度匹配。

---

### 输出层计算

\[
O = H \cdot W_{hq} + b_q
\]

其中：

- \(H \in \mathbb{R}^{n \times h}\) （来自隐藏层）  
- \(W_{hq} \in \mathbb{R}^{h \times q}\) （将隐藏状态映射到输出）  
- \(b_q \in \mathbb{R}^{1 \times q}\)  
- \(O \in \mathbb{R}^{n \times q}\)

> 输出层通过 \(W_{hq}\) 将隐藏状态的维度 \(h\) 转换到输出维度 \(q\)，保证矩阵加法可行。 q 就是 输出的特征数或类别数。

### 有隐藏状态的循环神经网络  

在 RNN 中，我们引入 **隐状态** $\boldsymbol{h}_t$，作为“记忆”：

\[
\boldsymbol{h}_t = g\big(\boldsymbol{x}_t W_{xh} + \boldsymbol{h}_{t-1} W_{hh} + \boldsymbol{b}_h\big)
\]

其中：
- $\boldsymbol{x}_t \in \mathbb{R}^d$：时间步 $t$ 的输入  
- $W_{xh} \in \mathbb{R}^{d \times h}$：输入到隐状态的权重  
- $W_{hh} \in \mathbb{R}^{h \times h}$：前一隐状态到当前隐状态的权重  
- $\boldsymbol{b}_h \in \mathbb{R}^h$：偏置  
- $g(\cdot)$：激活函数（如 $\tanh$、ReLU）  
- $\boldsymbol{h}_t \in \mathbb{R}^h$：当前时间步的隐状态  

> 与 MLP 相比，RNN 多了一项 $\boldsymbol{h}_{t-1} W_{hh}$，这就是循环结构的来源。


### MLP vs RNN 的区别

#### 1. 单个时间步的计算
- **MLP（多层感知机）**  
  隐藏层只依赖当前输入 $\boldsymbol{x}_t$。  
  \[
  \boldsymbol{h}_t = g(\boldsymbol{x}_t W_{xh} + b_h)
  \]

- **RNN（循环神经网络）**  
  隐藏层不仅依赖当前输入 $\boldsymbol{x}_t$，还依赖前一时间步的隐状态 $\boldsymbol{h}_{t-1}$。  
  \[
  \boldsymbol{h}_t = g(\boldsymbol{x}_t W_{xh} + \boldsymbol{h}_{t-1} W_{hh} + b_h)
  \]

👉 区别：RNN 在隐藏层引入了 **隐状态**，从而保留历史信息。

---

#### 2. 跨时间步的计算
- **MLP**  
  每个样本独立处理，彼此之间没有联系。  

- **RNN**  
  多个时间步的隐状态是 **堆叠起来的链式结构**：  
  \[
  \boldsymbol{h}_1 \rightarrow \boldsymbol{h}_2 \rightarrow \cdots \rightarrow \boldsymbol{h}_t
  \]

👉 区别：RNN 的隐状态在时间维度上传递，因此 **序列的时序依赖可以被捕捉**。  

---

#### 3. 总结
- 在单个时间步：  
  RNN = MLP + 隐状态（记忆历史）。  

- 在多个时间步：  
  RNN 将时间步堆叠起来，隐状态在序列中不断传递，使模型能够学习 **时序依赖**。

  重点就在于 隐藏状态的累积



  每个时间步，输出的就是向量，看是多分类，还是其他什么问题。

### 字符级语言模型




### 评判标准之困惑度（Perplexity）






## RNN 从0 开始 实现

思考：整个的逻辑框架怎么实现。

数据导入，参数维度调整和参数初始化。

对单个块函数的构建


### 独特编码 

每个token 都有 一个 索引， 直接输入索引有点麻烦， 
重点将每个词元 转化为特征向量。

one hot coding:

将索引 映射成相互不同的单位向量, 词表数为N. 即词元的索引 是 0- N-1.

除了对应索引位置是 1 之外，其他位置全是 0。

举例：
假设词表大小 len(vocab) = 5，索引范围是 0 ~ 4。

词元索引 = 0
→ 独热向量 = [1, 0, 0, 0, 0]

词元索引 = 2
→ 独热向量 = [0, 0, 1, 0, 0]

词元索引 = 4
→ 独热向量 = [0, 0, 0, 0, 1]


### 梯度裁剪

这里涉及到 梯度消失 和 梯度爆炸。

一个序列数据长度为\(T\),当T 很长的时候会反向梯度爆炸或者消失。
以下需要去推导实现。

首先，基本的参数公式：

\[
    h_t = G（W_hh_{t-1} + W_xx_t）
\]

然后按照链式法则去计算损失函数：

由于隐藏是逐渐积累的，看公式可以知道： 

h1 到 h2 到 h3 到最终的 L

这里的损失函数，是所有的损失函数。

损失函数 是对参数进行偏导
根据路径的不同，每个输出的损失函数是可以相互叠加的。

---

# RNN 梯度慢慢推导（序列理解版）

假设序列长度为 3（为了易于展示），隐藏状态依次为：

$$
h_1, h_2, h_3
$$

损失函数是所有时间步累加的总损失：

$$
L = L_1(h_1) + L_2(h_2) + L_3(h_3)
$$

---

## 1️⃣ 第一步：h1 对 L 的贡献

* $h_1$ 直接影响 $L_1$
* 通过 $h_2$ 间接影响 $L_2$
* 通过 $h_2 \rightarrow h_3$ 间接影响 $L_3$

所以 $h_1$ 对总损失 $L$ 的梯度为：

$$
\frac{dL}{dh_1} = \underbrace{\frac{dL_1}{dh_1}}_{\text{直接}} + 
\underbrace{\frac{dL_2}{dh_2} \cdot \frac{dh_2}{dh_1}}_{\text{间接通过h2}} +
\underbrace{\frac{dL_3}{dh_3} \cdot \frac{dh_3}{dh_2} \cdot \frac{dh_2}{dh_1}}_{\text{间接通过h2→h3}}
$$

* 每一项对应一种“路径”，从 $h_1$ 到损失函数的影响
* 这就是“多条路径的梯度叠加”

---

## 2️⃣ 第二步：h2 对 L 的贡献

* $h_2$ 直接影响 $L_2$
* 通过 $h_3$ 间接影响 $L_3$

梯度为：

$$
\frac{dL}{dh_2} = \underbrace{\frac{dL_2}{dh_2}}_{\text{直接}} + 
\underbrace{\frac{dL_3}{dh_3} \cdot \frac{dh_3}{dh_2}}_{\text{间接通过h3}}
$$

---

## 3️⃣ 第三步：h3 对 L 的贡献

* $h_3$ 只影响 $L_3$：

$$
\frac{dL}{dh_3} = \frac{dL_3}{dh_3}
$$

---



### 整体实现代码







