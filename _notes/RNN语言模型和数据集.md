---
layout: note_with_toc
title: 3. 语言模型和数据集
description: Basic concepts of language models and dataset processing methods
category: Machine Learning
tags: [RNN, Language Models, Datasets, Natural Language Processing]
permalink: /notes/RNN语言模型和数据集/
---

## 3.1 核心思想

语言模型将处理好的词元（可以是单个字母或单词）看作一个长度为 \( T \) 的序列，每个位置的词都是一个观测值：

\[
x_1, x_2, \dots, x_T
\]

语言模型的任务是估计该序列的联合概率分布：

\[
P(x_1, x_2, \dots, x_T)
\]

根据链式法则，这个联合概率可以分解为一系列条件概率的乘积：

\[
P(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, x_2, \dots, x_{t-1})
\]

这样分解是因为联合概率的维度过高，无法一次性预测整篇文本。通过一步步预测下一个词，模型将复杂的大问题拆解成多个小问题。模型通过学习条件概率分布，就能在给定上下文的情况下生成文本。

**核心思想**：通过统计语言规律，预测序列中每个词出现的概率。

## 3.2 参数与概率估计

语言模型建模的是概率分布，这些分布由单词的概率和条件概率组成。

### 单词概率

最简单的频率估计方法：

\[
P(\text{deep}) = \frac{\text{单词 "deep" 出现的次数}}{\text{语料库中所有单词的总次数}}
\]

表示单词出现的相对频率。

### 条件概率

计算连续两个词的概率，例如"learning"在"deep"之后出现的概率：

\[
P(\text{learning} \mid \text{deep}) = \frac{\text{"deep learning" 出现的次数}}{\text{"deep" 出现的次数}}
\]

随着序列长度增加，会出现许多组合从未出现过，导致概率为 0，这就是**零概率问题**。

### 平滑技术

引入 **拉普拉斯平滑（Laplace Smoothing）** 给未出现的词分配非零概率：

\[
P_{\text{smooth}}(w) = \frac{\text{出现次数} + 1}{\text{总词数} + V}
\]

更一般的形式：

\[
P(w) = \frac{\text{次数} + \alpha}{\text{总词数} + \alpha V}
\]

- \(V\) 是词表大小  
- \(\alpha\) 控制平滑强度  
  - \(\alpha = 0\) 时无平滑  
  - \(\alpha\) 很大时接近均匀分布

语言模型通过统计词与词之间的共现规律，利用概率和条件概率描述语言结构；平滑技术保证模型面对未见序列时仍能给出合理概率估计。

## 3.3 马尔可夫模型与 n-gram

### 马尔可夫模型

如果序列满足 **一阶马尔可夫性质**：

\[
P(w_n \mid w_1, w_2, \dots, w_{n-1}) \approx P(w_n \mid w_{n-1})
\]

当前词只依赖前一个词。阶数越高，模型捕捉的依赖关系越长，为序列建模提供近似公式。

### n-gram 模型

根据上下文长度不同，可定义：

- **一元语法（Unigram）**  
  只考虑单个词出现概率，不包含上下文。  
  示例：句子"猫 喜欢 睡觉"中  
  \[
  P(\text{猫}) = \frac{1}{3}
  \]

- **二元语法（Bigram）**  
  考虑每个词在前一个词条件下出现概率。  
  示例：
  \[
  P(\text{睡觉} \mid \text{猫}) = 0
  \]
  \[
  P(\text{喜欢} \mid \text{猫}) = 1
  \]

- **三元语法（Trigram）**  
  考虑每个词在前两个词条件下出现概率。  
  示例：
  \[
  P(\text{睡觉} \mid \text{猫 喜欢}) = 1
  \]

**特点**：

- 通过统计固定长度词组合出现频率，捕捉词之间依赖关系  
- 随着 n 增大：  
  - 模型复杂度增加  
  - 数据稀疏问题加剧  
- 实际应用中需在上下文长度与计算代价间平衡

## 3.4 齐普夫定律

自然语言中，词频遵循 **幂律分布**（Zipf's Law）：

- 高频词（如 `the`、`of`）出现非常频繁  
- 绝大多数词出现频率低  
- 双对数坐标下，词序号与频率点近似落在直线上

**对语言模型的影响**：

- 频率分布极不均衡，未平滑的 n-gram 模型表现差  
- 低频词或未出现的组合概率为 0，需要平滑技术解决

## 3.5 多元语法与组合建模

- 任意词理论上都有可能出现，因此通常使用 **n-gram** 考虑词组合  
- 示例：词集 `吃、苹果、桌子、电脑、香蕉、手机`  
  - 两两组合总数 \(6^2 = 36\)  
  - 但并非所有组合合理（如"吃桌子"不常见）  
- 数据训练可学习语言规律，使模型生成合理概率分布

## 3.6 长序列数据处理

在实际语言建模中，文本序列往往非常长，无法一次性全部输入模型。因此，需要对序列进行拆分，以便模型能够高效读取和训练。

### 1. 序列拆分

- **基本思路**：将长序列切成若干段，然后依次取出训练。
- **问题**：如果每次切分都是固定起点，模型总是看到相同的序列，容易丢失某些数据规律。
- **解决方法**：引入**随机起点** \(k\)。
  - \(k\) 的取值范围在 \((0, \tau)\)，其中 \(\tau\) 是窗口长度。
  - 通过随机起点，确保所有序列顺序都能被模型学习到，增加数据的覆盖性和随机性。

### 2. 窗口滑动与样本生成

- **窗口大小**：保持一致，以便并行训练。
- **生成样本**：
  - 假设序列为 `[1, 2, 3, 4]`，窗口大小为 3：
    ```
    X: [1, 2, 3]
    Y: [2, 3, 4]
    ```
  - 通过滑动窗口，每次生成对应的输入 X 和目标 Y。

### 3. 索引构建

- 数据从语料库加载时，核心在于**创建良好的索引**。
- 索引的作用：
  - 确定每个窗口的起点和终点
  - 保证数据加载顺序正确
  - 支持并行训练时的数据分配
- 利用索引，可以灵活生成训练样本，而不需要每次都操作原始序列。

### 4. 批次处理策略

有两种常见方法：

1. **先分批再切分**：
   - 先将数据分成若干批次
   - 在每个批次内切分序列
   - 可以在切分后打乱批次和序列索引，增加随机性

2. **先切分再分批**：
   - 先按顺序切好所有序列
   - 然后再分成批次
   - 每个批次中再进行序列切分
   - 切分次数与批次数量一致
   - 最终生成训练样本

### 5. 并行训练考虑

- 窗口长度相等保证每个批次可以并行处理
- 随机起点和批次打乱确保训练数据覆盖充分，避免模型过度依赖固定序列模式
- 这种处理方式既保证了数据的**覆盖性**，又保持**随机性**，有利于模型更好地学习长序列中的规律
