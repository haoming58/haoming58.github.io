---
layout: note_with_toc
title: 1. åºåˆ—å»ºæ¨¡åŸºç¡€
description: Basic concepts and principles of sequence modeling
category: Machine Learning
tags: [RNN, Sequence Modeling, Deep Learning, Neural Networks]
permalink: /notes/RNNåºåˆ—å»ºæ¨¡åŸºç¡€/
---

# 1. åºåˆ—å»ºæ¨¡åŸºç¡€

## 1.1 ä»€ä¹ˆæ˜¯åºåˆ—æ¨¡å‹ï¼Ÿ

æœ‰äº›æ•°æ®æ˜¯â€œæœ‰é¡ºåºâ€çš„ï¼Œæ¯”å¦‚ï¼š

* è‚¡ç¥¨ä»·æ ¼æ¯å¤©çš„å˜åŒ–
* ä¸€ä¸ªäººè¯´è¯æ—¶çš„è¯­éŸ³
* å¤©æ°”çš„æ¸©åº¦å˜åŒ–

è¿™äº›æ•°æ®çš„**é¡ºåºä¸èƒ½æ‰“ä¹±**ï¼Œå¦åˆ™æ„ä¹‰å°±å…¨å˜äº†ã€‚
ğŸ‘‰ æ‰€ä»¥æˆ‘ä»¬éœ€è¦**åºåˆ—æ¨¡å‹**ï¼Œæ¥ç†è§£â€œè¿‡å»ä¼šæ€æ ·å½±å“æœªæ¥â€ã€‚


## 1.2 å¸¸è§çš„ç»Ÿè®¡æ–¹æ³•

### 1.2.1 è‡ªå›å½’æ¨¡å‹ï¼ˆARï¼‰

**ç›´è§‚ç†è§£**ï¼š
é¢„æµ‹æœªæ¥ï¼Œé çš„æ˜¯â€œè¿‡å»çš„è‡ªå·±â€ã€‚

* æ¯”å¦‚é¢„æµ‹ä»Šå¤©çš„è‚¡ç¥¨ä»·æ ¼ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒå‰å‡ å¤©çš„ä»·æ ¼ã€‚
* ä½†æ•°æ®å¤ªå¤šä¼šå¾ˆéº»çƒ¦ï¼Œæ‰€ä»¥åªçœ‹æœ€è¿‘çš„ä¸€æ®µï¼ˆæ¯”å¦‚æœ€è¿‘ 3 å¤©ï¼‰ï¼Œè¿™å°±å«**çª—å£ $\tau$**ã€‚

å…¬å¼å†™å‡ºæ¥ï¼š

$$
x_t = P(x_t \mid x_{t-1}, x_{t-2}, \ldots, x_{t-\tau})
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
å°±åƒä½ å¤ä¹ è€ƒè¯•ï¼Œåªéœ€è¦çœ‹æœ€è¿‘å‡ å¤©çš„å¤ä¹ ç¬”è®°ï¼Œä¸å¯èƒ½æŠŠæ‰€æœ‰å­¦è¿‡çš„éƒ½é‡æ–°èƒŒä¸€éã€‚

### 1.2.2 éšå˜é‡è‡ªå›å½’æ¨¡å‹ï¼ˆLatent ARï¼‰

é—®é¢˜æ¥äº†ï¼š
å¦‚æœå†å²ç‰¹åˆ«é•¿ï¼ˆæ¯”å¦‚çœ‹ 1 å¹´çš„è‚¡ä»·ï¼‰ï¼Œå…‰é çª—å£è¿˜æ˜¯å¤ªå¤æ‚ã€‚

äºæ˜¯æˆ‘ä»¬æƒ³äº†ä¸ªåŠæ³•ï¼š
ğŸ‘‰ ç”¨ä¸€ä¸ªâ€œæ€»ç»“ç¬”è®°â€ $h_t$ï¼ŒæŠŠè¿‡å»çš„å†…å®¹éƒ½æ€»ç»“è¿›å»ã€‚
ä»¥åé¢„æµ‹çš„æ—¶å€™ï¼Œå°±ç›´æ¥å‚è€ƒè¿™ä»½æ€»ç»“ï¼Œè€Œä¸æ˜¯çœ‹æ‰€æœ‰å†å²ã€‚

![1758904905325](image/2025_8_26/1758904905325.png)

å…¬å¼ï¼š

$$
x_t \sim P(x_t \mid h_t), \quad h_t = f(h_{t-1}, x_{t-1})
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
å°±åƒä¸Šè¯¾æ—¶ï¼Œä½ ä¸ä¼šæŠŠè€å¸ˆè¯´çš„æ¯ä¸€å¥è¯éƒ½è®°ä¸‹æ¥ï¼Œè€Œæ˜¯åšä¸€ä»½â€œè¯¾å ‚æ€»ç»“â€ã€‚ä»¥åå¤ä¹ è€ƒè¯•æ—¶ï¼Œä½ çœ‹çš„æ˜¯æ€»ç»“ï¼Œè€Œä¸æ˜¯å®Œæ•´è¯¾å ‚å½•éŸ³ã€‚

### 1.2.3 é©¬å°”å¯å¤«æ¨¡å‹

é©¬å°”å¯å¤«æ¨¡å‹çš„æ€æƒ³å¾ˆç®€å•ï¼š
ğŸ‘‰ **æœªæ¥åªä¾èµ–æœ€è¿‘çš„æƒ…å†µï¼Œä¸ä¾èµ–æ›´ä¹…è¿œçš„å†å²ã€‚**

* å¦‚æœåªä¾èµ–â€œä¸Šä¸€åˆ»â€çš„çŠ¶æ€ï¼Œè¿™å«â€œä¸€é˜¶é©¬å°”å¯å¤«â€ã€‚
* æ¯”å¦‚é¢„æµ‹æ˜å¤©å¤©æ°”ï¼Œåªçœ‹ä»Šå¤©çš„å¤©æ°”ï¼Œä¸çœ‹æ›´ä¹…ä¹‹å‰çš„ã€‚

å…¬å¼ï¼š

$$
P(x_{t+1} \mid x_1,\ldots,x_t) = P(x_{t+1} \mid x_t)
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
ä½ å†³å®šæ˜å¤©è¦ä¸è¦å¸¦ä¼ï¼Œé€šå¸¸åªçœ‹ä»Šå¤©çš„å¤©æ°”ï¼Œä¸ä¼šå»ç®¡ä¸€ä¸ªæœˆå‰çš„å¤©æ°”ã€‚

### 1.2.4 å› æœå…³ç³»

æ¦‚ç‡çš„å…¬å¼å¯ä»¥éšä¾¿æ¢é¡ºåºï¼Œä½†æ—¶é—´å¯ä¸æ˜¯ã€‚

ğŸ‘‰ ç°å®ç”Ÿæ´»é‡Œï¼Œ**è¿‡å»å½±å“æœªæ¥ï¼Œä½†æœªæ¥ä¸èƒ½å½±å“è¿‡å»**ã€‚

* è§£é‡Š $P(z_{t+1} \mid z_t)$ï¼ˆæœªæ¥ä¾èµ–äºç°åœ¨ï¼‰æ˜¯åˆç†çš„ï¼›
* è§£é‡Š $P(z_t \mid z_{t+1})$ï¼ˆç°åœ¨ä¾èµ–æœªæ¥ï¼‰å°±ä¸ç¬¦åˆå› æœå…³ç³»ã€‚

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
ä½ ä»Šå¤©æ˜¯å¦ä¼šä¸‹é›¨ï¼Œä¸èƒ½ç”±â€œæ˜å¤©ä¼šä¸ä¼šä¸‹é›¨â€æ¥å†³å®šã€‚

## 1.3 æ€»ç»“

è¿™æ ·è®²ä¸‹æ¥ï¼š

* è‡ªå›å½’æ¨¡å‹ï¼šçœ‹è¿‡å»å‡ æ­¥
* éšå˜é‡æ¨¡å‹ï¼šåšæ€»ç»“ç¬”è®°
* é©¬å°”å¯å¤«æ¨¡å‹ï¼šæœªæ¥åªçœ‹æœ€è¿‘
* å› æœå…³ç³»ï¼šåªèƒ½â€œç”±å‰æ¨åâ€

---



## 1.3 ä»£ç å®è·µ

### 1.3.1 åŸå§‹æ•°æ®
åœ¨ä»£ç å®è·µéƒ¨åˆ†ï¼Œæœ‰2å—ï¼Œä¸€æ˜¯ä»£ç è§£é‡Šæ— æ³•ç›´æ¥å¤åˆ¶è¿è¡Œï¼Œå¦å¤–ä¸€ä¸ªæ˜¯æºä»£ç å¯ä»¥ç›´æ¥å¤åˆ¶ã€‚

ä½¿ç”¨æ­£å¼¦å‡½æ•°å’Œä¸€äº›å¯åŠ æ€§å™ªå£°æ¥ç”Ÿæˆåºåˆ—æ•°æ®ã€‚

```python
%matplotlib inline è¿™ä¸ªç”¨äºå°†ä»£ç çš„æ‰§è¡Œå›¾ç‰‡ç»“æœç›´æ¥æ”¾åœ¨jupyteré‡Œé¢å±•ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥ä»¥ä¸ºçª—å£çš„å½¢å¼å±•ç¤º
import torch å¯¼å…¥torchåº“
from torch import nn å¯¼å…¥nnåº“
from d2l import torch as d2l å¯¼å…¥d2låº“ï¼Œç”¨äºç»˜åˆ¶

#å…±äº§ç”Ÿ 1000 ä¸ªç‚¹
å› ä¸ºï¼Œç”µè„‘çš„æ•°æ®æ˜¯ç¦»æ•£çš„ï¼Œå½“æ•°æ®ç‚¹å¾ˆå¤šï¼Œå°±æˆä¸ºäº†è¿ç»­æ•°æ®ã€‚
T = 1000

#ç”Ÿæˆä¸€ä¸ªç”Ÿæˆä¸€ä¸ªä¸€ç»´å¼ é‡
time = torch.arange(1, T + 1, dtype=torch.float32) 

å¼ é‡çš„æ•°æ®ç±»å‹æ˜¯ 32 ä½æµ®ç‚¹æ•°ï¼Œè¿™é‡Œçš„ä¹Ÿå¯ç†è§£arangeç”Ÿæˆä¸€ç³»åˆ—å€¼ã€1ï¼ŒT+1ï¼‰åŒºé—´æ˜¯å·¦é—­å³å¼€ã€‚ 
æ€»çš„é•¿åº¦å°±æ˜¯Tï¼Œå› ä¸ºï¼Œè¿™é‡Œå–ä¸åˆ° T+1, æ‰€ä»¥ç”Ÿæˆçš„å€¼æ˜¯1ï¼Œ2ï¼Œ3ï¼Œ.....T ã€‚å› æ­¤ï¼Œé•¿åº¦ä¸ºT

# ç”Ÿæˆå¸¦å™ªå£°çš„æ­£å¼¦ä¿¡å·
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
å› ä¸ºï¼Œè¦ç›¸åŠ ï¼Œç»´åº¦ T éœ€è¦ä¿æŒä¸€è‡´ã€‚
x = sin(0.01 * t) + Îµ,   å…¶ä¸­ Îµ ~ N(0, 0.2Â²)
torch.normal(0, 0.2, (T,))
ç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸º T=1000 çš„å™ªå£°å¼ é‡
æ¯ä¸ªå…ƒç´ éƒ½æœä» å‡å€¼ 0ã€æ ‡å‡†å·® 0.2 çš„æ­£æ€åˆ†å¸ƒ

# ç»˜å›¾
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
ç»˜åˆ¶å›¾åƒ  æ¨ªåæ ‡æ˜¯time é•¿åº¦ä¸ºT ï¼Œ çºµåæ ‡æ˜¯ y è½´æ•°æ®ï¼Œéœ€è¦æ˜¯åˆ—è¡¨ï¼Œå³ä½¿åªæœ‰ä¸€æ¡æ›²çº¿ä¹Ÿè¦æ”¾åœ¨åˆ—è¡¨é‡Œï¼Œ åé¢çš„æ˜¯æ ‡ç­¾ï¼Œxlim=[1, 1000] æ˜¯æ¨ªåæ ‡çš„æ˜¾ç¤ºèŒƒå›´
```

```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```


![1758907002850](image/2025_8_26/1758907002850.png)

### 1.3.2 è½¬åŒ–


åŸå§‹æ•°æ®æœ‰äº†ä¹‹åï¼Œå¯¹äºç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œéœ€è¦å°†æ—¶é—´åºåˆ—åˆ‡åˆ†ä¸ºç‰¹å¾ï¼ˆfeatureï¼‰å’Œæ ‡ç­¾ï¼ˆlabelï¼‰ã€‚æ¯ä¸ªç‰¹å¾ä½¿ç”¨å‰ `tau` ä¸ªè¿ç»­çš„å€¼ä½œä¸ºè¾“å…¥ï¼Œè¿™é‡Œçš„ `tau` å°±æ˜¯åµŒå…¥ç»´åº¦ï¼ˆçª—å£å¤§å°ï¼‰ã€‚æ•°å­¦ä¸Šï¼Œå¯ä»¥è¡¨ç¤ºä¸ºï¼š

\[
X_i = [x_i, x_{i+1}, \dots, x_{i+\tau-1}], \quad Y_i = x_{i+\tau}
\]

ä¾‹å¦‚ï¼Œå¦‚æœ `tau = 5`ï¼Œåºåˆ—ä¸º `x = [x1, x2, x3, ..., x10]`ï¼Œåˆ™ï¼š

```python
# ç¬¬1ä¸ªæ•°æ®å¯¹
X1 = [x1, x2, x3, x4, x5]
Y1 = x6

# ç¬¬2ä¸ªæ•°æ®å¯¹
X2 = [x2, x3, x4, x5, x6]
Y2 = x7

# ç¬¬3ä¸ªæ•°æ®å¯¹
X3 = [x3, x4, x5, x6, x7]
Y3 = x8
```


å‰ `tau` ä¸ªæ•°æ®ç‚¹æ— æ³•ä½œä¸ºæ ‡ç­¾ï¼Œå› ä¸ºå¯¹åº”çš„ç‰¹å¾éœ€è¦åœ¨å®ƒä¹‹å‰çš„ `tau` ä¸ªæ•°æ®ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæƒ³å°† `x5` å½“ä½œæ ‡ç­¾ï¼Œåˆ™éœ€è¦ç‰¹å¾ `[x0, x1, x2, x3, x4]`ï¼Œä½† `x0` ä¸å­˜åœ¨ï¼Œå› æ­¤æ— æ³•å½¢æˆå®Œæ•´çš„ç‰¹å¾ï¼æ ‡ç­¾å¯¹ã€‚

è¿™é‡Œä»¥æ ‡ç­¾ä½œä¸ºåŸºç¡€ã€‚ è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œæœ‰å¤šå°‘æ ‡ç­¾ï¼Œå°±æœ‰å¤šå°‘å¯¹ï¼Œè€Œæœ€åˆçš„å‰é¢çª—å£çš„å¤§å°ä¸èƒ½å½“æ ‡ç­¾ï¼Œå› æ­¤ï¼Œ

æ ·æœ¬æ€»æ•°å¯ä»¥ç”¨å…¬å¼è®¡ç®—ï¼š

$$
\text{æ€»æ ·æœ¬æ•°} = \text{åºåˆ—é•¿åº¦} - \tau
$$


### 1.3.3 åŸºæœ¬æ•°æ®å¤„ç†

è¿™é‡Œï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å‰600ä¸ªâ€œç‰¹å¾ï¼æ ‡ç­¾â€å¯¹è¿›è¡Œè®­ç»ƒã€‚

```python

# ============================================================
# 1. å®šä¹‰ç‰¹å¾çŸ©é˜µ
# ============================================================
features = torch.zeros((T - tau, tau))

# ç»´åº¦è¯´æ˜ï¼š
   è¡Œæ•° = (å¤šå°‘å¯¹)
   åˆ—æ•° = (æ¯å¯¹çš„åºåˆ—é•¿åº¦ï¼Œå³çª—å£é•¿åº¦)

# ä¸¾ä¾‹ï¼š
 features = {
   [x1, x2, x3, x4, x5],
   [x2, x3, x4, x5, x6],
   ...
 }

# ============================================================
# 2. æŠŠä¸€ç»´æ—¶é—´åºåˆ— ğ‘¥ è½¬æ¢æˆä¸€ä¸ªç‰¹å¾çŸ©é˜µ  featuresï¼Œæ„é€ æ‰€è°“çš„ã€Œæ»åç‰¹å¾ã€(lag features)
# ============================================================
  ä½¿ç”¨æŒ‰åˆ—å¡«å……æ–¹å¼ï¼ˆä¸æ˜¯æŒ‰è¡Œï¼‰ï¼Œæ€»é•¿åº¦ä¸º T - tau
  æ¯ä¸€åˆ—ç”±åŸå§‹åºåˆ—ä¸­ä¸€æ®µè¿ç»­æ•°æ®ç»„æˆ

for i in range(tau):
    features[:, i] = x[i : T - tau + i]

# è¯´æ˜ï¼š
     featureså–æ‰€æœ‰è¡Œï¼Œi åˆ—  
   - X æ˜¯ä¸€ç»´é•¿åºåˆ—
   - é€šå¸¸æ¥è¯´ï¼Œä»¥å·¦è¾¹çš„ç¬¬ä¸€å…ƒç´ å¯¹åº”çš„ç´¢å¼•iï¼Œ å’Œå³è¾¹ç¬¬ä¸€ä¸ªåŸå§‹çš„ç´¢å¼•å¯¹é½iï¼Œè‹¥æ˜¯æ— æ³•ç†è§£ï¼Œè¯·çœ‹ä¸‹é¢çš„ä¾‹å­
   - ç´¢å¼•iä» 0 å¼€å§‹ï¼Œåˆ°tau-1 ç»“æŸï¼Œå…±tauåˆ—ã€‚
   - ä¹Ÿå¯ä»¥æŒ‰è¡Œæ„é€ ï¼Œå–å†³äºé€‰æ‹©


# ============================================================
# 3. æ„é€ æ ‡ç­¾
# ============================================================
labels = x[tau:].reshape((-1, 1))

# æ ‡ç­¾è¯´æ˜ï¼š
   - ç¬¬ä¸€ä¸ªçª—å£ä¹‹åçš„æ‰€æœ‰åŸå§‹æ•°æ®éƒ½æ˜¯æ ‡ç­¾ï¼Œä¸ºäº†å¯¹åº”featureçš„ç»´åº¦å¯¹åº”
   - æŠŠä¸€ç»´æ•°ç»„è½¬ä¸ºäºŒç»´åˆ—å‘é‡
   - -1 è¡¨ç¤ºè‡ªåŠ¨æ¨ç®—è¡Œæ•°
   -  1 è¡¨ç¤ºåªæœ‰ä¸€åˆ—


# ============================================================
# 4. ç¤ºä¾‹
# ============================================================
x   = [x1, x2, x3, x4, x5, x6]
T   = 6
tau = 3

 i = 0
 x[0 : 3] = [x1, x2, x3] â†’ å¡«åˆ° features[:, 0]

 i = 1
 x[1 : 4] = [x2, x3, x4] â†’ å¡«åˆ° features[:, 1]

 i = 2
 x[2 : 5] = [x3, x4, x5] â†’ å¡«åˆ° features[:, 2]


# ============================================================
# 5. æ„é€ è®­ç»ƒé›†è¿­ä»£å™¨
# ============================================================

batch_size, n_train = 16, 600



# åªæœ‰å‰ n_train ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
train_iter = d2l.load_array(
    (features[:n_train], labels[:n_train]),
    batch_size,
    is_train=True
)
 åªæœ‰å‰n_trainä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œè¿™é‡Œç»´åº¦ä¸æ˜¯2ç»´çš„shapeï¼Œ å› æ­¤ï¼Œç›¸å½“äº1ç»´åº¦çš„ å‰n_trainï¼Œå› ä¸ºæ²¡æœ‰ï¼Œé€—å·
 ä»æ•°æ®é›†ä¸­å–å‰ 600 ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ã€‚å¯ä»¥ç†è§£600ä¸ª
```


```python
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))

batch_size, n_train = 16, 600
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```
### 1.3.4 æ¨¡å‹æ¶æ„

 - åˆå§‹åŒ–

```python
init_weights æ˜¯ä¸€ä¸ª å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–ç½‘ç»œå±‚çš„æƒé‡

def init_weights(m):
    if type(m) == nn.Linear: åˆ¤æ–­è¿™ä¸ªå±‚æ˜¯ä¸æ˜¯ nn.Linearï¼ˆå…¨è¿æ¥å±‚ï¼Œåªå¯¹å…¨è¿æ¥å±‚åˆå§‹åŒ–ï¼Œå·ç§¯å±‚æˆ–å…¶ä»–å±‚ä¸å¤„ç†ï¼‰
        nn.init.xavier_uniform_(m.weight) 
        
  ç”¨ Xavier å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ã€‚
  Xavier åˆå§‹åŒ–çš„ä½œç”¨ï¼šä¿æŒæ¯å±‚è¾“å…¥è¾“å‡ºçš„æ–¹å·®ä¸€è‡´ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
  _ ç»“å°¾è¡¨ç¤ºè¿™ä¸ªå‡½æ•°ä¼š ç›´æ¥ä¿®æ”¹æƒé‡ï¼Œè€Œä¸æ˜¯è¿”å›æ–°å€¼ã€‚
  

def get_net():
    nn.Sequential(...)ï¼šæŒ‰é¡ºåºç»„åˆå±‚
    net = nn.Sequential(nn.Linear(2,10),
                        nn.ReLU(),
                        nn.Linear(10,1))
    net.apply(init_weights) ä¼šé€’å½’åœ°å¯¹ç½‘ç»œä¸­æ¯ä¸ªå±‚è°ƒç”¨ init_weights(m) 
    nn.Linear(2,10) å’Œ nn.Linear(10,1) çš„æƒé‡éƒ½ä¼šç”¨ Xavier åˆå§‹åŒ–
    return net

loss = nn.MSELoss(reduction = 'none') è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³æ–¹è¯¯å·®ï¼Œä¸å¹³å‡
```

```python
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

def get_net():
    net = nn.Sequential(nn.Linear(2,10),
                        nn.ReLU(),
                        nn.Linear(10,1))
    net.apply(init_weights)
    return net

loss = nn.MSELoss(reduction = 'none')
```
- æ¨¡å‹è®­ç»ƒ

```python

def train(net, train_iter, loss, epochs, lr):
#############
# -net â†’ ä½ çš„ç¥ç»ç½‘ç»œæ¨¡å‹

# -train_iter â†’ å°æ‰¹é‡æ•°æ®è¿­ä»£å™¨

# -loss â†’ æŸå¤±å‡½æ•°ï¼ˆè¿™é‡Œæ˜¯ MSELoss(reduction='none')ï¼‰

# -epochs â†’ è®­ç»ƒè½®æ•°

# -lr â†’ å­¦ä¹ ç‡
##############
    trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())
    ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º lr
    å‘Šè¯‰ä¼˜åŒ–å™¨å»æ›´æ–°ç½‘ç»œé‡Œçš„æ‰€æœ‰å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰

    for epoch in range(epochs):
        for i,(X, y) in enumerate (train_iter()):
            trainer.clear_grad() PyTorch çš„æ¢¯åº¦æ˜¯ç´¯åŠ çš„ï¼Œæ¯æ¬¡æ›´æ–°å‰è¦æ¸…é›¶ï¼Œå¦åˆ™ä¼šæŠŠå‰ä¸€æ¬¡çš„æ¢¯åº¦ç´¯åŠ ä¸Šå»ã€‚åªé’ˆå¯¹æ¯ä¸€ä¸ªæ‰¹æ¬¡è®­ç»ƒåï¼Œä½¿ç”¨çš„æ¢¯åº¦æ›´æ–°
            l = loss(net(X), y)
            l.sum().backward() backward()ï¼šè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œå­˜å‚¨åœ¨æ¯ä¸ªå‚æ•°çš„ .grad ä¸­ã€‚
            trainer.step() æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚
        print(f'epoch {epoch + 1}, ' æ¯ä¸ª epoch æ‰“å°ä¸€æ¬¡å¹³å‡è®­ç»ƒæŸå¤±ï¼Œ å› ä¸ºepoch æ˜¯ä¸‹æ ‡0å¼€å§‹
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
```
```python
def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.sum().backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
```


![1759174346320](image/2025_8_26/1759174346320.png)


### 1.3.4 é¢„æµ‹ 



- **å•æ­¥é¢„æµ‹**æ˜¯æŒ‡åŸºäºå·²æœ‰çš„å†å²è§‚æµ‹å€¼é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æ•°å€¼ï¼Œæ¯æ¬¡é¢„æµ‹åéƒ½æœ‰çœŸå®å€¼å¯ä»¥ç›´æ¥è¿›è¡Œå¯¹æ¯”ï¼Œè¯¯å·®ä¸ä¼šç´¯ç§¯ã€‚

```python

onestep_preds = net(features)
è¾“å…¥ä½ çš„ç‰¹å¾å€¼ï¼Œç„¶åå¾—åˆ°é¢„æµ‹å€¼ï¼Œå› ä¸ºï¼Œç½‘ç»œæ˜¯æˆ‘ä»¬ä»¥åŠè®­ç»ƒå¥½çš„ï¼Œå› ä¸ºåœ¨ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº†trainã€‚

features: [x1,x2,x3,x4 ]   onestep_preds = [x5
           x2,x3,x4,x5                     x6    ]
               .
               .                            .
               .                            .                             
                                          
d2l.plot([time, time[tau:]], 
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))

ç»˜å›¾æ—¶éœ€è¦ä¸¤æ¡çº¿ï¼šä¸€æ¡æ˜¯åŸå§‹æ•°æ® x å¯¹åº”æ—¶é—´ timeï¼ˆç”¨ x.detach().numpy() è½¬æˆ numpyï¼‰ï¼Œ
å¦ä¸€æ¡æ˜¯æ¨¡å‹é¢„æµ‹ onestep_preds å¯¹åº”æ—¶é—´ time[tau:]ï¼ˆåŒæ ·ç”¨ detach().numpy() è½¬æˆ numpyï¼‰ï¼Œdetach() æ˜¯æŠŠ tensor ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å†è·Ÿè¸ªæ¢¯åº¦ã€‚
```

```python
onestep_preds = net(features)
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))
```


![1759178315073](image/2025_8_26/1759178315073.png)

- **å¤šæ­¥é¢„æµ‹**æ˜¯åŸºäºå†å²å€¼é¢„æµ‹æœªæ¥å¤šä¸ªæ—¶é—´ç‚¹ï¼Œé€šå¸¸é€šè¿‡é€’å½’ï¼ˆé¢„æµ‹å€¼ä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥ï¼‰æˆ–ç›´æ¥ï¼ˆä¸€æ¬¡æ€§è¾“å‡ºå¤šæ­¥ç»“æœï¼‰æ–¹å¼å®ç°ï¼Œä½†é€’å½’æ–¹å¼å®¹æ˜“å¯¼è‡´è¯¯å·®é€æ­¥æ”¾å¤§ã€‚


åœ¨ä»£ç è®¾è®¡ä¸Šï¼Œå¯ä»¥é‡‡ç”¨ä¸¤ç§æ–¹å¼è¿›è¡Œé¢„æµ‹ï¼šä¸€ç§æ˜¯åŸºäºå®Œæ•´æ—¶é—´åºåˆ—è¿›è¡Œé¢„æµ‹ï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨æ»‘åŠ¨çª—å£æ‰¹é‡é¢„æµ‹ã€‚åœ¨æ»‘åŠ¨çª—å£æ–¹å¼ä¸­ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„èµ·å§‹æ—¶é—´ç‚¹ï¼Œä¾‹å¦‚ç¬¬ 0 è¡Œè¾“å…¥ ([1,2,3]) é¢„æµ‹æœªæ¥ç¬¬ 1ã€2 æ­¥ï¼Œç¬¬ 1 è¡Œè¾“å…¥ ([2,3,4]) é¢„æµ‹æœªæ¥ç¬¬ 1ã€2 æ­¥ï¼Œè¡Œä¸è¡Œä¹‹é—´ä¸å…·æœ‰æ—¶é—´è¿ç»­æ€§ã€‚è¿™ç§è®¾è®¡çš„ç›®çš„ä¸æ˜¯ä¸ºäº†è®­ç»ƒæ¨¡å‹æˆ–ç”Ÿæˆå®Œæ•´çš„æœªæ¥åºåˆ—ï¼Œè€Œæ˜¯ç”¨äºæµ‹è¯•æ¨¡å‹åœ¨ä¸åŒé¢„æµ‹æ­¥é•¿ä¸‹çš„æ€§èƒ½ï¼Œåˆ†æè¯¯å·®éšé¢„æµ‹æ­¥æ•°å˜åŒ–çš„è§„å¾‹ï¼Œå¹¶ç»˜åˆ¶å¯¹æ¯”æ›²çº¿ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹çš„çŸ­æœŸä¸é•¿æœŸé¢„æµ‹èƒ½åŠ›ã€‚ä»…ä»…åªæ˜¯ä¸ºäº†æµ‹è¯•å’Œæ•™å­¦ï¼Œä»¥ä¸€ä¸ªä¾‹å­è§£é‡Šã€‚

```python

multistep_preds = torch.zeros(T) ç”Ÿæˆä¸€ä¸ªé•¿åº¦ä¸º T çš„å¼ é‡

multistep_preds[: n_train + tau] = x[: n_train + tau]

ä¹‹å‰æåˆ°ä¸€å…±æœ‰1000æ­¥ã€‚åªç”¨äº†604æ­¥ï¼Œå› ä¸ºçª—å£ï¼Œè®¾ç½®çš„æ˜¯4.

è¿™é‡Œçš„æ•°æ®åˆ°604 åé¢å…¨æ˜¯0

ä»æ•°æ®ä¸­å–å‡ºå‰ n_train + tau ä¸ªå…ƒç´ 

for i in range(n_train + tau, T): ä¹Ÿå°±æ˜¯æ ‡ç­¾çš„åé¢é¢„æµ‹ç»“æœæ˜¯605 
    multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((1, -1)))
    
    içš„å–å€¼æ˜¯604, 605, 606, ..., 999ã€‚
    
    å½“ i = 604 çš„æ—¶å€™ï¼Œå¯¹åº”è®­ç»ƒæ•°æ®å°±åº”è¯¥æ˜¯600 601 602 603ï¼Œ å°±æ˜¯ i - tauã€‚è¿™ä¸ªåŒºé—´ï¼Œç„¶åå†™æˆnetçš„è¾“å…¥æ•°æ®çš„æ ¼å¼ 1 è¡Œ 4 åˆ—ã€‚ 

    å¾ªç¯åˆ°999æ¬¡

d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))


ç»˜å›¾æ—¶éœ€è¦3æ¡çº¿ï¼šä¸€æ¡æ˜¯åŸå§‹æ•°æ® x å¯¹åº”æ—¶é—´ timeï¼ˆç”¨ x.detach().numpy() è½¬æˆ numpyï¼‰ï¼Œ

å¦ä¸€æ¡æ˜¯æ¨¡å‹é¢„æµ‹ onestep_preds å¯¹åº”æ—¶é—´ time[tau:]ï¼ˆåŒæ ·ç”¨ detach().numpy() è½¬æˆ numpyï¼‰ï¼Œdetach() æ˜¯æŠŠ tensor ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å†è·Ÿè¸ªæ¢¯åº¦ã€‚

æœ€åä¸€æ¡æ˜¯å¤šæ­¥æ¨¡å‹é¢„æµ‹ multistep_preds å¯¹åº”æ—¶é—´ time[n_train + tau]ï¼ˆåŒæ ·ç”¨ detach().numpy() è½¬æˆ numpyï¼‰ï¼Œdetach() æ˜¯æŠŠ tensor ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œä¸å†è·Ÿè¸ªæ¢¯åº¦ã€‚ä»604æ­¥å¼€å§‹

```


```python
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(
        multistep_preds[i - tau:i].reshape((1, -1)))

d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```
![1759247745951](image/2025_8_26/1759247745951.png)


- **Kæ­¥é¢„æµ‹** k = 1ï¼Œ4ï¼Œ16ï¼Œ64 é€šè¿‡å¯¹æ•´ä¸ªåºåˆ—é¢„æµ‹çš„è®¡ç®—ï¼Œ è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹
æ­¥é¢„æµ‹çš„å›°éš¾ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œé€šè¿‡ä¸€ä¸ªä¾‹å­å»ç†è§£ï¼š




| å†å²1 | å†å²2 | 1-step | 2-step | 3-step |
| ----- | ----- | ------ | ------ | ------ |
| t1    | t2    | t3     | t4     | t5     |
| t2    | t3    | t4     | t5     | t6     |
| t3    | t4    | t5     | t6     | t7     |


å‡è®¾æ€»çš„æ—¶é—´æ­¥é•¿ä¸º \(T = 6\)ï¼Œå†å²çª—å£å¤§å° \(\tau = 2\)ï¼Œæ•°æ®åºåˆ—ä¸º t1, t2, t3, t4, t5, t6ï¼Œæœ€æ—©çš„æ ‡ç­¾ä» t3 å¼€å§‹ã€‚å¯¹äº K æ­¥é¢„æµ‹ï¼š  

 **1-step é¢„æµ‹**ï¼šä½¿ç”¨å‰ä¸¤ä¸ªæ—¶é—´æ­¥ t1ã€t2 é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼ t3ï¼Œç„¶åç”¨ t2ã€t3 é¢„æµ‹ t4ï¼Œä»¥æ­¤ç±»æ¨ã€‚  
 **2-step é¢„æµ‹**ï¼šé¦–å…ˆç”¨ t1ã€t2 é¢„æµ‹ t3ï¼Œç„¶åç»§ç»­ä½¿ç”¨ t2ã€t3 é¢„æµ‹ t4ï¼Œå³æ¯ä¸€æ­¥é¢„æµ‹éƒ½ä¾èµ–äºå‰é¢çš„å†å²æˆ–é¢„æµ‹å€¼ã€‚  
 **3-step é¢„æµ‹**ï¼šå°è¯•ä½¿ç”¨ t3ã€t4 é¢„æµ‹ t5ï¼Œå†ç”¨ t4ã€t5 é¢„æµ‹ t6ï¼Œä½†å½“éœ€è¦é¢„æµ‹ t7 æ—¶å¤±è´¥ï¼Œå› ä¸ºæ€»åºåˆ—é•¿åº¦åªæœ‰ 6ï¼Œ æ²¡æœ‰çœŸå®æ ‡ç­¾å¯ä»¥è®­ç»ƒæˆ–éªŒè¯ã€‚

 1-stepï¼š t3 t4 t5
 2-stepï¼š t4 t5 t6
 3-stepï¼š t5 t6 t7

#### ä¸ºä»€ä¹ˆå‡ºç°å…¬å¼ \(T - \tau - \text{max\_steps} + 1\)

å½“æˆ‘ä»¬ä»ä¸€ä¸ªåºåˆ—é•¿åº¦ \(T\) ä¸­é‡‡æ ·æ—¶ï¼š

- æœ‰æ•ˆçš„é¢„æµ‹çª—å£å¿…é¡»æ»¡è¶³ï¼š  

  \[
  t + \text{max\_steps} \leq T
  \]

å› ä¸ºé¢„æµ‹ \(t+1, \dots, t+\text{max\_steps}\) éƒ½å¿…é¡»è½åœ¨å·²æœ‰çš„è§‚æµ‹èŒƒå›´å†…,ä¹Ÿå°±æ˜¯å·²ç»æœ‰çš„æ•°æ®ã€‚

ç»“åˆè¾“å…¥é•¿åº¦ \(\tau\)ï¼š  
- è¾“å…¥çª—å£ä» \(t-\tau+1\) å¼€å§‹ï¼Œåˆ° \(t\) ç»“æŸã€‚  
- æœ€åä¸€æ¬¡èƒ½ç”¨æ¥é¢„æµ‹çš„æ—¶é—´ç‚¹æ˜¯ï¼š  
  \[
  t_{\max} = T - \text{max\_steps}
  \]

- èƒ½é‡‡æ ·çš„çª—å£æ•°é‡å°±æ˜¯ï¼š  
  \[
  N = T - \tau - \text{max\_steps} + 1
  \]

---
```python
# å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹ç‰¹å¾æ„é€ ä¸å¯è§†åŒ–

# 1. èƒŒæ™¯
# åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥è¿›è¡Œå•æ­¥é¢„æµ‹ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹ï¼‰ï¼Œ
# ä¹Ÿå¯ä»¥è¿›è¡Œå¤šæ­¥é¢„æµ‹ï¼ˆé¢„æµ‹æœªæ¥å¤šä¸ªæ—¶é—´ç‚¹ï¼‰ã€‚
#
# å‚æ•°è®¾å®šï¼š
# - æœ€å¤§é¢„æµ‹æ­¥æ•°ï¼šmax_steps = 64
# - å†å²çª—å£é•¿åº¦ï¼štau
#
# ä¸ºäº†ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªç‰¹å¾çŸ©é˜µ featuresã€‚

features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))

# 2. ç‰¹å¾çŸ©é˜µæ„é€ 
# è¡Œæ•°ï¼šè®­ç»ƒæ ·æœ¬æ•°
# - å•æ­¥é¢„æµ‹æ—¶ï¼šT - tau
#   ä¾‹å¦‚åºåˆ— (t1, t2, t3, t4, t5, t6)ï¼Œtau=2 æ—¶ï¼Œå¯ä»¥ç”Ÿæˆ 4 ä¸ªæ ·æœ¬ (t3, t4, t5, t6)ã€‚
# - å¤šæ­¥é¢„æµ‹æ—¶ï¼šT - tau - max_steps + 1
#   ä¾‹å¦‚ tau=2, max_steps=3 æ—¶ï¼Œæ ·æœ¬æ•°ä¸º 2ã€‚
#
# åˆ—æ•°ï¼štau + max_steps
# - å‰ tau åˆ—ï¼šå†å²è§‚æµ‹å€¼
# - å max_steps åˆ—ï¼šé€æ­¥é¢„æµ‹å¾—åˆ°çš„æœªæ¥å€¼

# 3. å¡«å……æ–¹å¼
# 3.1 å†å²éƒ¨åˆ†
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªæ—¶é—´ç‚¹çš„æ»‘åŠ¨çª—å£ã€‚

# 3.2 é¢„æµ‹éƒ¨åˆ†
for i in range(tau, tau + max_steps): 
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)

# ä»ç¬¬ tau åˆ—å¼€å§‹é€æ­¥é¢„æµ‹ï¼š
# - tau åˆ— = 1-stepé¢„æµ‹
# - tau+1 åˆ— = 2-stepé¢„æµ‹
# - ...
# æ¯ä¸€æ­¥é¢„æµ‹éƒ½ä¾èµ–äºå‰ tau ä¸ªè¾“å…¥ï¼ˆåŒ…æ‹¬å†å²å’Œå·²é¢„æµ‹çš„æœªæ¥å€¼ï¼‰ã€‚

# 4. ç¤ºä¾‹è¯´æ˜
# å‡è®¾åºåˆ—ä¸º (t1, t2, t3, t4, t5, t6)ï¼Œtau=2ï¼Œmax_steps=3ï¼š
#
# | å†å²1 | å†å²2 | 1-step | 2-step | 3-step |
# |-------|-------|--------|--------|--------|
# | t1    | t2    | t3     | t4     | t5     |
# | t2    | t3    | t4     | t5     | t6     |

# 5. å¯è§†åŒ–å¤šæ­¥é¢„æµ‹
steps = (1, 4, 16, 64)

d2l.plot(
    [time[tau + i - 1 : T - max_steps + i] for i in steps],   # æ—¶é—´ç‚¹
    [features[:, tau + i - 1].detach().numpy() for i in steps],  # å¯¹åº”é¢„æµ‹å€¼
    'time', 'x',
    legend=[f'{i}-step preds' for i in steps],
    xlim=[5, 1000],
    figsize=(6, 3)
)

# - æ¨ªè½´ï¼štime = [1, 2, ..., T]
#   Python ç´¢å¼•ä» 0 å¼€å§‹ï¼Œå› æ­¤éœ€è¦ tau + i - 1 æ¥å¯¹é½ã€‚
# - çºµè½´ï¼šé¢„æµ‹å€¼
# - å›¾ä¾‹ï¼š1-step preds, 4-step preds, ...

# 6. æ€»ç»“
# 1. æ„é€  features çŸ©é˜µï¼š
#    - å‰ tau åˆ— = å†å²è§‚æµ‹
#    - å max_steps åˆ— = é€’å½’é¢„æµ‹æœªæ¥
# 2. æ ·æœ¬æ•°å— T, tau, max_steps å…±åŒé™åˆ¶
# 3. ç»˜å›¾æ—¶æ ¹æ® steps é€‰æ‹©é¢„æµ‹æ­¥é•¿ï¼Œæ³¨æ„æ—¶é—´å¯¹é½

```