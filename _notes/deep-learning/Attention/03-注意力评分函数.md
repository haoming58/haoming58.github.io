---
layout: note_with_toc
title: 3. 注意力评分函数
description: Gated Recurrent Unit - advanced RNN architecture with reset and update gates
category: Deep Learning
subcategory: Advanced RNN
tags: [RNN, GRU, Gated Networks, Deep Learning, Neural Networks]
permalink: /notes/gated-recurrent-unit/
redirect_from:
  - /notes/门控循环单元（GRU）/
  - /notes/sequence-modeling-basics/
---

# 3. 注意力评分函数

不同注意力函数的不同注意力机制的差别就在于 评分函数的定义不同。

在章节2， 主要提供了： 高斯核的方法。 此外，实际上，还存在一些其他方法，点积和多层感知机的方法。

## 3.1 Mask 函数

这个主要是针对之前提到的特殊填充词，设计的函数。

```python

#@save
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    # X:3D张量，valid_lens:1D或2D张量
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1: # 如果是一个维度，代表只是针对一个queery
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1) 

        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0

        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
```

下面以一个例子进行简单说明：

```python
X = [
  [  # batch 0
    [1.0, 2.0, 3.0, 4.0],   # query 0
    [2.0, 1.0, 0.0, -1.0]   # query 1
  ],
  [  # batch 1
    [0.0, 1.0, 2.0, 3.0],   # query 0
    [3.0, 2.0, 1.0, 0.0]    # query 1
  ]
]

valid_lens = torch.tensor([2, 3])

batch0 有效 key 数 = 2 → 后面 2 个 key 都要被 mask

batch1 有效 key 数 = 3 → 只 mask 最后 1 个 key

valid_lens = [2, 3]  

repeat → [2, 2, 3, 3] 

batch0 有 2 个 query → 各自都用 valid_len = 2

batch1 有 2 个 query → 各自都用 valid_len = 3

X.reshape(-1, 4) 得到 4 行（因为 2×2=4），每行 4 个 key：
Row0: [1.0, 2.0, 3.0, 4.0]   # batch0-query0
Row1: [2.0, 1.0, 0.0, -1.0]  # batch0-query1
Row2: [0.0, 1.0, 2.0, 3.0]   # batch1-query0
Row3: [3.0, 2.0, 1.0, 0.0]   # batch1-query1

对应的valid_len

Row0 → 2 : [1.0, 2.0, -1e6, -1e6]
Row1 → 2 : [2.0, 1.0, -1e6, -1e6]
Row2 → 3 : [0.0, 1.0, 2.0, -1e6]
Row3 → 3 : [3.0, 2.0, 1.0, -1e6]

最后采用归一化就是softmax 进行使用

```

## 3.2 加性注意力


