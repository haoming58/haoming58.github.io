---
layout: note_with_toc
title: 6. 自注意力和位置编码
description: Gated Recurrent Unit - advanced RNN architecture with reset and update gates
category: Deep Learning
subcategory: Advanced RNN
tags: [RNN, GRU, Gated Networks, Deep Learning, Neural Networks]
permalink: /notes/attention/self-attention-positional-encoding/
redirect_from:
  - /notes/门控循环单元（GRU）/
  - /notes/sequence-modeling-basics/

---

# 6. 自注意力和位置编码

在深度学习中，序列编码的目标是将一个词元序列（如一句话）转换为一组具有语义信息的表示。传统方法通常使用卷积神经网络（CNN）或循环神经网络（RNN）来完成这一任务。

## 6.1 传统的序列编码方法

**卷积神经网络（CNN）**
CNN 通过局部卷积操作捕捉 n-gram 等局部特征，具有较强的并行计算能力和较高的效率，但难以建模长距离依赖关系。

**循环神经网络（RNN，如 LSTM、GRU）**
RNN 按时间步顺序处理序列，能够自然地建模顺序和上下文信息，但其计算过程是串行的，在处理长序列时效率较低，且存在梯度消失或爆炸问题。


引入注意力机制后，序列建模不再依赖 CNN 或 RNN 逐步传递信息，而是允许序列中的每个词元直接与序列中的所有其他词元建立联系，从而更灵活地建模全局依赖关系。

**自注意力（Self-Attention）** 是一种特殊的注意力机制，其中查询（Query）、键（Key）和值（Value）均来自同一组输入序列。
设输入序列为
X = (x₁, x₂, …, xₙ)，
则序列中的每个词元同时充当 Query、Key 和 Value。
因此，该机制被称为自注意力（self-attention），也称为内部注意力（intra-attention）。

**自注意力的工作过程**

对于序列中的每一个词元 xᵢ，自注意力的计算过程如下：
1）将 xᵢ 作为查询（Query）；
2）与序列中所有词元对应的键（Key）计算相关性；
3）通过归一化得到注意力权重；
4）对所有值（Value）进行加权求和；
5）生成词元 xᵢ 的新表示。

其直观数学形式为：
Attention(xᵢ) = ∑ⱼ αᵢⱼ vⱼ

其中，αᵢⱼ 表示词元 xᵢ 对词元 xⱼ 的关注程度。
通过这种方式，每个词元都可以直接融合整个序列的上下文信息。


自注意力的输出仍然是一个序列，但序列中每个位置的表示都已经融合了全局上下文信息。与 RNN 相比，自注意力不需要按时间步顺序计算，可以并行处理序列，从而提高计算效率，并且更容易建模长距离依赖。

**顺序信息的问题与位置编码**

需要注意的是，自注意力机制本身并不包含序列的顺序信息。如果仅使用自注意力，对词序不同但词元相同的序列，其建模结果是等价的。
为了解决这一问题，通常将序列的顺序作为补充信息引入模型中。

最常见的方法是**位置编码（Positional Encoding）**，包括基于正弦和余弦函数的固定位置编码，以及可学习的位置向量。最终的输入表示为词向量与位置编码之和，从而使模型能够感知序列顺序。

## 6.2 比较卷积神经网络、循环神经网络和自注意力

| 架构类型 | 计算复杂度（每层） | 顺序操作（并行度） | 最大路径长度（长距离依赖） | 核心特点分析 |
|---------|------------------|------------------|--------------------------|-------------|
| **RNN（循环神经网络）** | $O(n \cdot d^2)$ | $O(n)$（无法并行） | $O(n)$（较长） | 最慢，最难长记忆。必须等 $t-1$ 算完才能算 $t$，无法利用 GPU 并行优势；且信息传递需要经过 $n$ 步，容易发生梯度消失/爆炸。 |
| **CNN（卷积神经网络）** | $O(k \cdot n \cdot d^2)$ | $O(1)$（可并行） | $O(n/k)$ | 局部关注，需堆叠。虽然可以并行，但卷积核 $k$ 通常较小，只能看局部。想看全局（如第 1 个词看第 $n$ 个词），必须堆叠很多层来扩大感受野。 |
| **Self-Attention（自注意力）** | $O(n^2 \cdot d)$ | $O(1)$（可并行） | $O(1)$（最短） | 全局关注，并行计算。任意两个词元直接相连，距离为 1。瓶颈在于序列长度：如果 $n$ 很大，$n^2$ 的计算量会非常恐怖。 |



1. 为什么需要位置编码？

在 RNN 中，词元是一个接一个输入的，模型天然知道“谁在谁前面”。但在 自注意力机制 中，输入是并行处理的。对于模型来说，句子 "I love you" 和 "You love I"，如果只看自注意力的计算，词与词之间的交互方式是一模一样的（排列不变性）。为了让模型知道词元的顺序（Sequence Order），我们必须手动把位置信息“加”进去。

2. 怎么加？直接将位置编码矩阵 $P$ 加到输入嵌入矩阵 $X$ 上：$$X_{final} = X_{embedding} + P$$注意是元素相加（Addition），不是拼接。这意味着 $P$ 的形状必须和 $X$ 一样，也是 $n \times d$。3. 正弦/余弦编码公式解析论文采用了固定的正弦和余弦函数来生成 $P$。假设 $P$ 的第 $i$ 行（表示第 $i$ 个词元的位置），第 $j$ 列（表示该位置向量的第 $j$ 个维度）：偶数维度 ($2j$) 使用正弦函数：$$P_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)$$奇数维度 ($2j+1$) 使用余弦函数：$$P_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)$$公式直觉解释：想象一个多针的时钟。低维（$j$ 小）：分母小，频率高。就像秒针，转得快，对位置变化非常敏感。高维（$j$ 大）：分母大，频率低。就像时针，转得慢，用于区分大范围的位置差异。

3. 为什么不直接使用数字 

因为，简单的数字容易数值保证和不够精确，我们给每个字贴的不是一个数字，而是一排很多个表盘。

场景模拟：
第 1 个字的位置：

秒针指向：1点

分针指向：0点

时针指向：0点

数值稳定：不管你的句子是一万米长，时针分针永远只在表盘里转，数值永远在 -1 到 +1 之间。永远不会爆炸。

相对位置好算（最重要的一点）：

如果你知道现在是 3点00分。

我问你：过15分钟之后，时针分针在哪？

你不需要重新查表，你只需要把现在的分针旋转 90度 就行了。

这就是“线性关系”：对于模型来说，它不需要死记硬背每个位置，它只要学会“旋转”这个操作，就能知道“第5个字”和“第8个字”之间差了多少。

具体细节： 
. 数学推导：为什么是 $X+P$ 而不是拼接？直观上我们担心“相加”会混淆信息。但在线性模型中，相加（Addition）在一定条件下等价于拼接（Concatenation）。假设输入词向量是 $x \in \mathbb{R}^d$，位置向量是 $p \in \mathbb{R}^d$。如果我们使用拼接：我们将它们拼成一个 $2d$ 维的向量 $[x; p]$。后续的线性层（比如计算 Query 的 $W_q$）矩阵形状必须是 $2d \times d$。我们将这个大矩阵拆分为两部分 $W_q^1$ 和 $W_q^2$：$$\text{Attention Input} = [x; p] \cdot \begin{bmatrix} W_q^1 \\ W_q^2 \end{bmatrix} = x \cdot W_q^1 + p \cdot W_q^2$$如果我们使用相加：我们将它们相加得到 $x+p$。后续的线性层 $W_q$ 形状是 $d \times d$。$$\text{Attention Input} = (x + p) \cdot W_q = x \cdot W_q + p \cdot W_q$$结论：你会发现，这两种形式在数学形式上是完全一致的。$x \cdot W_q$ 代表模型关注**“词的内容”**。$p \cdot W_q$ 代表模型关注**“词的位置”**。通过分配律，Transformer 的参数矩阵 $W_q$ 实际上是在学习如何将这两部分信息融合。如果模型需要分别处理语义和位置，它可以学习出一个特定的 $W_q$，将向量空间划分为两个正交的子空间，使得语义和位置互不干扰。2. 核心推导：正弦编码与相对位置这是 Transformer 论文中最精彩的数学部分。我们要证明：对于位置 $t$ 和偏移量 $k$，位置 $t+k$ 的编码向量可以由 $t$ 的编码向量线性表示。为了简化推导，我们只看一个频率对（即两个维度，一个正弦，一个余弦）。设 $\omega_i = \frac{1}{10000^{2i/d}}$ 为角频率。位置 $t$ 的编码向量（在第 $2i$ 和 $2i+1$ 维）表示为：$$PE(t) = \begin{bmatrix} \sin(\omega_i t) \\ \cos(\omega_i t) \end{bmatrix}$$我们需要找到位置 $t+k$ 的编码：$$PE(t+k) = \begin{bmatrix} \sin(\omega_i (t+k)) \\ \cos(\omega_i (t+k)) \end{bmatrix}$$步骤 1：利用三角函数加法定理展开$$\begin{aligned}
\sin(\omega_i (t+k)) &= \sin(\omega_i t + \omega_i k) \\
&= \sin(\omega_i t)\cos(\omega_i k) + \cos(\omega_i t)\sin(\omega_i k)
\end{aligned}$$$$\begin{aligned}
\cos(\omega_i (t+k)) &= \cos(\omega_i t + \omega_i k) \\
&= \cos(\omega_i t)\cos(\omega_i k) - \sin(\omega_i t)\sin(\omega_i k)
\end{aligned}$$步骤 2：写成矩阵乘法形式我们将上面的展开式整理成关于 $\sin(\omega_i t)$ 和 $\cos(\omega_i t)$ 的线性组合：$$\begin{bmatrix} \sin(\omega_i (t+k)) \\ \cos(\omega_i (t+k)) \end{bmatrix} = \begin{bmatrix} \cos(\omega_i k) & \sin(\omega_i k) \\ -\sin(\omega_i k) & \cos(\omega_i k) \end{bmatrix} \cdot \begin{bmatrix} \sin(\omega_i t) \\ \cos(\omega_i t) \end{bmatrix}$$结论：$$PE(t+k) = M_k \cdot PE(t)$$其中 $M_k$ 是一个旋转矩阵（Rotation Matrix）。这意味着，从位置 $t$ 移动到 $t+k$，在数学上等同于将向量旋转了一个角度（角度由偏移量 $k$ 决定）。这个旋转矩阵 $M_k$ 与具体的绝对位置 $t$ 无关，只与相对距离 $k$ 有关。

3. 推导：自注意力机制如何利用这一点？在自注意力机制中，核心操作是计算 Query 和 Key 的点积（Dot Product）。让我们看看加入位置编码后，点积里包含了什么。假设 $q$ 是位置 $t$ 的查询向量，$k$ 是位置 $s$ 的键向量。如果不考虑语义，只看位置部分 $P_t$ 和 $P_s$ 的点积：$$P_t \cdot P_s = P_t^T P_s$$带入我们的正弦余弦定义：$$\begin{aligned}
P_t \cdot P_s &= \sin(\omega t)\sin(\omega s) + \cos(\omega t)\cos(\omega s)
\end{aligned}$$利用三角函数积化和差公式（$\cos(A-B) = \cos A \cos B + \sin A \sin B$）：$$P_t \cdot P_s = \cos(\omega (t - s))$$最终的物理意义：这即使是数学推导最美妙的地方。两个位置向量的点积，仅取决于它们位置的差值 $(t-s)$。这意味着，当模型计算注意力分数时：它不仅能看到“词义相似度”。它还能看到**“相对距离的余弦相似度”**。


1. 回到注意力的核心公式回顾一下自注意力的计算流程。输入不再纯粹是词向量 $X$，而是加入了位置编码的 $H$：$$H = X + P$$当我们计算注意力分数（Attention Score）时，本质上是在计算 Query（查询） 和 Key（键） 的点积。为了简化理解，我们暂时忽略权重矩阵 $W_Q$ 和 $W_K$（假设它们是单位矩阵），专注于核心的点积交互。我们需要计算：第 $t$ 个词（Query） 和 第 $s$ 个词（Key） 的相关性分数。$$Score = H_t \cdot H_s^T$$2. 展开公式：四项交互把 $H = X + P$ 带入上面的公式：$$Score = (X_t + P_t) \cdot (X_s + P_s)^T$$根据分配律（类似于初中数学 $(a+b)(c+d) = ac + ad + bc + bd$），这个式子会炸开成 4 项：$$Score = \underbrace{X_t X_s^T}_{\text{① 内容-内容}} + \underbrace{X_t P_s^T}_{\text{② 内容-位置}} + \underbrace{P_t X_s^T}_{\text{③ 位置-内容}} + \underbrace{P_t P_s^T}_{\text{④ 位置-位置}}$$这一步非常关键！我们可以清楚地看到模型在关注什么：第 ① 项 ($X_t \cdot X_s$)：纯语义匹配含义：完全不看位置，只看词义。例子：看到“苹果”，去找“香蕉”。因为它们都是水果，词向量相似度高，这一项分数就高。这是传统词向量做的事情。第 ④ 项 ($P_t \cdot P_s$)：纯位置匹配（我们刚刚推导的重点）含义：完全不看词义，只看距离。数学关联：这一项就是我们刚才推导的 $\cos(\omega(t-s))$。作用：它给模型提供了一个**“距离偏置” (Distance Bias)**。如果模型想找“紧挨着我的词”，这一项会告诉模型哪些位置离得近，给那些位置加分。第 ② 和 ③ 项 ($X \cdot P$)：语义与位置的混合含义：寻找“特定位置的特定词”。例子：模型可能会学到，“如果第 1 个位置（$P_1$）是动词（$X_{verb}$），那么我要怎么怎么样”。3. 为什么这就能让模型“懂”序列？有了这 4 项的总和，Self-Attention 就不再是“脸盲”了。想象模型在计算“我($t$)”和“猫($s$)”的关系分数：语义层（第1项）：“我”和“猫”有关系吗？算出来分数一般（人和动物）。位置层（第4项）：“我”和“猫”离得近吗？如果句子是“我爱猫”，距离为2。根据 $\cos$ 函数，这会产生一个特定的位置分数。模型训练后发现，在这个特定距离下，通常存在主语和宾语的关系。融合决策：注意力机制会把这些分数加起来，经过 Softmax。如果语义也通且位置也对，最终的 Attention Weight 就会非常高。于是，“我”这个词的向量更新时，就会大量吸收“猫”的信息。4. 总结到这里，关于 Transformer 位置编码的全部拼图就完成了：问题：Self-Attention 也就是个“词袋模型”，没有顺序概念。方案：造一个位置矩阵 $P$，其中 $P_t$ 和 $P_s$ 的点积只与距离 $(t-s)$ 有关（利用正弦余弦的数学性质）。手段：把 $P$ 加到词向量 $X$ 上。结果：在计算注意力 $Score$ 时，数学自动展开成了“语义分 + 位置分”。模型通过训练，学会了如何在不同场景下权衡这两部分分数


0 (偶数)$\sin(\dots)$角度 A (第1组)第一对 CP1 (奇数)$\cos(\dots)$角度 A (第1组)(共享同一个角度 A)2 (偶数)$\sin(\dots)$角度 B (第2组)第二对 CP3 (奇数)$\cos(\dots)$角度 B (第2组)(共享同一个角度 B

对应的是词嵌入的维度

## 6.3 代码实践

```python
#@save
def __init__(self, num_hiddens, dropout, max_len=1000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(dropout)
    
    # 1. 创建一个足够长的画布 P
    # shape = (1, 1000, 512)
    # 这里的 '1' 是为了后面利用广播机制，让任意 batch_size 的数据都能直接相加
    self.P = torch.zeros((1, max_len, num_hiddens))
    
    # 2. 计算角度矩阵 X (这是最难懂的一步，利用了广播/外积)
    # 分子：位置索引 [0, 1, ..., 999]，变成列向量 (1000, 1)
    # 分母：频率衰减项，变成行向量 (256,)
    # 两者相除：触发广播，生成 (1000, 256) 的矩阵
    X = torch.arange(max_len, dtype=torch.float32).reshape(
        -1, 1) / torch.pow(10000, torch.arange(
        0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        
    # 3. 填充正弦和余弦
    # 0::2 表示偶数列 (0, 2, 4...) -> 填入 sin(X)
    # 0::2：Python 切片语法，意思是“从第0列开始，每隔2列取一列”。即 0, 2, 4, 6... (偶数列)。
    # 1::2：意思是从第1列开始，每隔2列取一列。即 1, 3, 5, 7... (奇数列)。
    self.P[:, :, 0::2] = torch.sin(X)
    # 1::2 表示奇数列 (1, 3, 5...) -> 填入 cos(X)
    self.P[:, :, 1::2] = torch.cos(X)

def forward(self, X):
    # X 的形状: (Batch_Size, 实际句子长度, 维度)
    # 例如: (32, 50, 512)

    # 1. 切片 (Slicing) - 裁剪尺子
    # self.P 是预先算好的 1000 长，但现在的句子只有 50 个词。
    # 所以我们只要前 50 个位置编码： self.P[:, :50, :]
    P_sliced = self.P[:, :X.shape[1], :]
    
    # 2. 搬家 (Device)
    # self.P 是在初始化时创建的，默认在 CPU 内存里。
    # 如果 X 在 GPU 上，必须把切出来的 P 也挪到 GPU 上才能相加，否则会报错。
    P_on_device = P_sliced.to(X.device)
    
    # 3. 相加
    # 利用广播机制：(32, 50, 512) + (1, 50, 512)
    # 那个 1 会自动复制 32 份，给每一句话都加上同样的位置编码。
    X = X + P_on_device
    
    return self.dropout(X)
```

模型不仅仅要知道“我在第 5 个位置”，更重要的是要知道“谁在我的前面，谁在我的后面”。

1. 核心必杀技：相对位置的感知能力 (Relative Positioning)这是 Transformer 论文中最强调的一点，也是我们之前讨论“线性投影/旋转矩阵”的根本原因。问题：模型不仅仅要知道“我在第 5 个位置”，更重要的是要知道“谁在我的前面，谁在我的后面”。好处：由于正弦函数的数学特性 $\sin(\alpha+\beta) = \dots$，模型可以非常容易地学会**“关注相对于我距离为 $k$ 的词”**。直观理解：这就好比每个人手里都有一个对讲机。普通编码：必须记住每个人的名字（“呼叫张三”）。如果来了个新人李四，你就不知道怎么喊了。正弦编码：不需要记名字，只需要调频。不管你是谁，只要我把频率调到“+1频道”，我就能联系上我后面那个人；调到“-1频道”，就能联系上我前面那个人。这种规则是通用的。2. 泛化能力：处理比训练集更长的句子 (Extrapolation)这是正弦编码对比“可学习位置编码（Learnable PE）”的最大优势。问题：假设你训练模型时，见过的最长句子只有 100 个词。测试时突然来了一篇 200 个词的文章，怎么办？如果是“可学习编码” (BERT的做法)：模型会傻眼。因为它只学过第 1 到 100 号位置向量长什么样，第 101 号位置它从来没见过，直接报错或瞎猜。好处 (正弦编码)：因为这是一个数学公式（函数）。只要你给公式输入 $pos=200$，它就能立刻算出一个新的向量给你。虽然模型没见过 $pos=200$，但因为正弦波的周期规律是连贯的，模型依然能大概理解这个位置的含义。类比：你只教过小孩子认识钟表上的 1 点到 12 点。死记硬背：到了 13 点他就蒙了。