---
layout: note_with_toc
title: 6. 自注意力和位置编码
description: Gated Recurrent Unit - advanced RNN architecture with reset and update gates
category: Deep Learning
subcategory: Advanced RNN
tags: [RNN, GRU, Gated Networks, Deep Learning, Neural Networks]
permalink: /notes/attention/self-attention-positional-encoding/
redirect_from:
  - /notes/门控循环单元（GRU）/
  - /notes/sequence-modeling-basics/

---

# 6. 自注意力和位置编码

在深度学习中，序列编码的目标是将一个词元序列（如一句话）转换为一组具有语义信息的表示。传统方法通常使用卷积神经网络（CNN）或循环神经网络（RNN）来完成这一任务。

## 6.1 传统的序列编码方法

**卷积神经网络（CNN）**
CNN 通过局部卷积操作捕捉 n-gram 等局部特征，具有较强的并行计算能力和较高的效率，但难以建模长距离依赖关系。

**循环神经网络（RNN，如 LSTM、GRU）**
RNN 按时间步顺序处理序列，能够自然地建模顺序和上下文信息，但其计算过程是串行的，在处理长序列时效率较低，且存在梯度消失或爆炸问题。


引入注意力机制后，序列建模不再依赖 CNN 或 RNN 逐步传递信息，而是允许序列中的每个词元直接与序列中的所有其他词元建立联系，从而更灵活地建模全局依赖关系。

**自注意力（Self-Attention）** 是一种特殊的注意力机制，其中查询（Query）、键（Key）和值（Value）均来自同一组输入序列。
设输入序列为
X = (x₁, x₂, …, xₙ)，
则序列中的每个词元同时充当 Query、Key 和 Value。
因此，该机制被称为自注意力（self-attention），也称为内部注意力（intra-attention）。

**自注意力的工作过程**

对于序列中的每一个词元 xᵢ，自注意力的计算过程如下：
1）将 xᵢ 作为查询（Query）；
2）与序列中所有词元对应的键（Key）计算相关性；
3）通过归一化得到注意力权重；
4）对所有值（Value）进行加权求和；
5）生成词元 xᵢ 的新表示。

其直观数学形式为：
Attention(xᵢ) = ∑ⱼ αᵢⱼ vⱼ

其中，αᵢⱼ 表示词元 xᵢ 对词元 xⱼ 的关注程度。
通过这种方式，每个词元都可以直接融合整个序列的上下文信息。


自注意力的输出仍然是一个序列，但序列中每个位置的表示都已经融合了全局上下文信息。与 RNN 相比，自注意力不需要按时间步顺序计算，可以并行处理序列，从而提高计算效率，并且更容易建模长距离依赖。

**顺序信息的问题与位置编码**

需要注意的是，自注意力机制本身并不包含序列的顺序信息。如果仅使用自注意力，对词序不同但词元相同的序列，其建模结果是等价的。
为了解决这一问题，通常将序列的顺序作为补充信息引入模型中。

最常见的方法是**位置编码（Positional Encoding）**，包括基于正弦和余弦函数的固定位置编码，以及可学习的位置向量。最终的输入表示为词向量与位置编码之和，从而使模型能够感知序列顺序。

## 6.2 比较卷积神经网络、循环神经网络和自注意力

| 架构类型 | 计算复杂度（每层） | 顺序操作（并行度） | 最大路径长度（长距离依赖） | 核心特点分析 |
|---------|------------------|------------------|--------------------------|-------------|
| **RNN（循环神经网络）** | $O(n \cdot d^2)$ | $O(n)$（无法并行） | $O(n)$（较长） | 最慢，最难长记忆。必须等 $t-1$ 算完才能算 $t$，无法利用 GPU 并行优势；且信息传递需要经过 $n$ 步，容易发生梯度消失/爆炸。 |
| **CNN（卷积神经网络）** | $O(k \cdot n \cdot d^2)$ | $O(1)$（可并行） | $O(n/k)$ | 局部关注，需堆叠。虽然可以并行，但卷积核 $k$ 通常较小，只能看局部。想看全局（如第 1 个词看第 $n$ 个词），必须堆叠很多层来扩大感受野。 |
| **Self-Attention（自注意力）** | $O(n^2 \cdot d)$ | $O(1)$（可并行） | $O(1)$（最短） | 全局关注，并行计算。任意两个词元直接相连，距离为 1。瓶颈在于序列长度：如果 $n$ 很大，$n^2$ 的计算量会非常恐怖。 |



1. 为什么需要位置编码？

在 RNN 中，词元是一个接一个输入的，模型天然知道“谁在谁前面”。但在 自注意力机制 中，输入是并行处理的。对于模型来说，句子 "I love you" 和 "You love I"，如果只看自注意力的计算，词与词之间的交互方式是一模一样的（排列不变性）。为了让模型知道词元的顺序（Sequence Order），我们必须手动把位置信息“加”进去。

2. 怎么加？直接将位置编码矩阵 $P$ 加到输入嵌入矩阵 $X$ 上：$$X_{final} = X_{embedding} + P$$注意是元素相加（Addition），不是拼接。这意味着 $P$ 的形状必须和 $X$ 一样，也是 $n \times d$。3. 正弦/余弦编码公式解析论文采用了固定的正弦和余弦函数来生成 $P$。假设 $P$ 的第 $i$ 行（表示第 $i$ 个词元的位置），第 $j$ 列（表示该位置向量的第 $j$ 个维度）：偶数维度 ($2j$) 使用正弦函数：$$P_{i, 2j} = \sin\left(\frac{i}{10000^{2j/d}}\right)$$奇数维度 ($2j+1$) 使用余弦函数：$$P_{i, 2j+1} = \cos\left(\frac{i}{10000^{2j/d}}\right)$$公式直觉解释：想象一个多针的时钟。低维（$j$ 小）：分母小，频率高。就像秒针，转得快，对位置变化非常敏感。高维（$j$ 大）：分母大，频率低。就像时针，转得慢，用于区分大范围的位置差异。

3. 为什么不直接使用数字 

因为，简