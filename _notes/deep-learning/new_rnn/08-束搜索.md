---
layout: note_with_toc
title: 8. 束搜索
description: Beam Search - efficient decoding algorithm for sequence generation balancing quality and speed
category: Deep Learning
subcategory: Advanced RNN
tags: [Beam Search, Decoding, Greedy Search, Sequence Generation, Deep Learning]
permalink: /notes/beam-search/
redirect_from:
  - /notes/束搜索/
---

# 8. 束搜索

## 8.1 核心机制

想象你在玩一个“接龙”游戏。你手里有成千上万个汉字（词表 $\mathcal{Y}$），你需要一个接一个地选字，组成一句通顺的话，直到你选了一个表示“结束”的牌（<eos>）。挑战： 每一个字的选择都会影响下一个字的概率。

目标： 我们要找出一个序列（句子），使得整句话的联合概率（所有字概率乘积）最大。

## 8.2 贪心搜索

在每一个时间步，然后选取输出中的最高概率的一个word，然而存在一个问题就是短视，无法找到全局最优的句子。

$P(C, B) > P(A, B)$ 

## 8.3 穷举搜索

既然贪心不好，那我们能不能把所有可能的句子都列出来，比一比谁的概率高？

理论上，没有问题，但是实际的成本高到离谱。

## 8.4 Beam Search 

取了一个折中。引入一个超参数，在每一步，不再只保留最好的那1个。保留最好的K个候选序列。

详细过程演练假设束宽 $k=2$：

Step 1： 输入开始。模型预测第一个词。我们不只选第一名，而是选出前两名（例如："A" 和 "C"）。我们现在有 2 个候选分支。

Step 2： 扩展分支。对于分支 "A"，我们要看它后面接什么（比如 A->A, A->B, A->C...）。对于分支 "C"，我们也看它后面接什么（比如 C->A, C->B, C->C...）。假设词表有 5 个词，现在我们一共计算了 $2 \times 5 = 10$ 种可能。

Step 3： 剪枝（Pruning）。在这 10 种可能（如 AA, AB, AC... CA, CB...）中，算出它们的累积概率。再次只保留前两名（比如 "AB" 和 "CE"）。其他的 8 种可能全部丢弃。

Step 4： 重复直到结束。

最终得到一个公式:

$$\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L} \mid \mathbf{c})$$

这里有一个细节需要注意：为什么要除以 $L^\alpha$？因为概率都是小于 1 的数（例如 0.5）。

句子越长，乘起来的数越多，结果就越接近 0（或者取对数后越负）。

如果不处理，模型会倾向于选择非常短的句子（因为短句子乘积没那么小）。

所以我们需要用长度 $L$ 对其进行归一化惩罚，让长句子和短句子能公平竞争。


## 8.5 Question

### 8.5.1 我们可以把穷举搜索看作一种特殊的束搜索吗？为什么？

可以，看作 K = 1 的搜索

### 8.5.2  在 9.7节的机器翻译问题中应用束搜索。 束宽是如何影响预测的速度和结果的？

$k$ 越大，搜索的空间越广，越有可能绕过局部的陷阱（局部最优），找到全局概率更高的句子。


### 8.5.3  在 8.5节中，我们基于用户提供的前缀， 通过使用语言模型来生成文本。这个例子中使用了哪种搜索策略？可以改进吗？

在经典的 RNN 文本生成示例（如《动手学深度学习》8.5 节）中，默认的预测函数通常使用的是 贪心搜索 (Greedy Search)。

改进方案 B：随机采样 (Stochastic Sampling) / 温度采样 (Temperature Sampling)

方法： 不直接取 argmax，而是根据概率分布进行随机抽取。可以加入“温度”参数：

温度低（趋向 0）：接近贪心搜索，保守、准确。

温度高：更随机，更有创造力，但也更容易出错。








