---
layout: note_with_toc
title: 1. åºåˆ—å»ºæ¨¡åŸºç¡€
description: Basic concepts and principles of sequence modeling
category: Machine Learning
tags: [RNN, Sequence Modeling, Deep Learning, Neural Networks]
permalink: /notes/sequence-modeling-basics/
redirect_from:
  - /notes/RNNåºåˆ—å»ºæ¨¡åŸºç¡€/
---

# 1. åºåˆ—å»ºæ¨¡åŸºç¡€

## 1.1 ä»€ä¹ˆæ˜¯åºåˆ—æ¨¡å‹ï¼Ÿ

æœ‰äº›æ•°æ®æ˜¯â€œæœ‰é¡ºåºâ€çš„ï¼Œæ¯”å¦‚ï¼š

- è‚¡ç¥¨ä»·æ ¼æ¯å¤©çš„å˜åŒ–
- ä¸€ä¸ªäººè¯´è¯æ—¶çš„è¯­éŸ³
- å¤©æ°”çš„æ¸©åº¦å˜åŒ–

è¿™äº›æ•°æ®çš„**é¡ºåºä¸èƒ½æ‰“ä¹±**ï¼Œå¦åˆ™æ„ä¹‰å°±å…¨å˜äº†ã€‚
 ğŸ‘‰ æ‰€ä»¥æˆ‘ä»¬éœ€è¦**åºåˆ—æ¨¡å‹**ï¼Œæ¥ç†è§£â€œè¿‡å»ä¼šæ€æ ·å½±å“æœªæ¥â€ã€‚

## 1.2 å¸¸è§çš„ç»Ÿè®¡æ–¹æ³•

### 1.2.1 è‡ªå›å½’æ¨¡å‹ï¼ˆARï¼‰

 **ç›´è§‚ç†è§£**ï¼š
 é¢„æµ‹æœªæ¥ï¼Œé çš„æ˜¯â€œè¿‡å»çš„è‡ªå·±â€ã€‚

- æ¯”å¦‚é¢„æµ‹ä»Šå¤©çš„è‚¡ç¥¨ä»·æ ¼ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒå‰å‡ å¤©çš„ä»·æ ¼ã€‚
- ä½†æ•°æ®å¤ªå¤šä¼šå¾ˆéº»çƒ¦ï¼Œæ‰€ä»¥åªçœ‹æœ€è¿‘çš„ä¸€æ®µï¼ˆæ¯”å¦‚æœ€è¿‘ 3 å¤©ï¼‰ï¼Œè¿™å°±å«**çª—å£ $\tau$**ã€‚

å…¬å¼å†™å‡ºæ¥ï¼š

$$
x_t = P(x_t \mid x_{t-1}, x_{t-2}, \ldots, x_{t-\tau})
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
å°±åƒä½ å¤ä¹ è€ƒè¯•ï¼Œåªéœ€è¦çœ‹æœ€è¿‘å‡ å¤©çš„å¤ä¹ ç¬”è®°ï¼Œä¸å¯èƒ½æŠŠæ‰€æœ‰å­¦è¿‡çš„éƒ½é‡æ–°èƒŒä¸€éã€‚

### 1.2.2 éšå˜é‡è‡ªå›å½’æ¨¡å‹ï¼ˆLatent ARï¼‰

é—®é¢˜æ¥äº†ï¼š
å¦‚æœå†å²ç‰¹åˆ«é•¿ï¼ˆæ¯”å¦‚çœ‹ 1 å¹´çš„è‚¡ä»·ï¼‰ï¼Œå…‰é çª—å£è¿˜æ˜¯å¤ªå¤æ‚ã€‚

äºæ˜¯æˆ‘ä»¬æƒ³äº†ä¸ªåŠæ³•ï¼š
 ğŸ‘‰ ç”¨ä¸€ä¸ªâ€œæ€»ç»“ç¬”è®°â€ $h_t$ï¼ŒæŠŠè¿‡å»çš„å†…å®¹éƒ½æ€»ç»“è¿›å»ã€‚
ä»¥åé¢„æµ‹çš„æ—¶å€™ï¼Œå°±ç›´æ¥å‚è€ƒè¿™ä»½æ€»ç»“ï¼Œè€Œä¸æ˜¯çœ‹æ‰€æœ‰å†å²ã€‚

![RNNéšè—çŠ¶æ€æ¦‚å¿µ]({{ '/assets/img/notes/rnn/figures/rnn_hidden_state_concept.png' | relative_url }})

å…¬å¼ï¼š

$$
x_t \sim P(x_t \mid h_t), \quad h_t = f(h_{t-1}, x_{t-1})
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
å°±åƒä¸Šè¯¾æ—¶ï¼Œä½ ä¸ä¼šæŠŠè€å¸ˆè¯´çš„æ¯ä¸€å¥è¯éƒ½è®°ä¸‹æ¥ï¼Œè€Œæ˜¯åšä¸€ä»½â€œè¯¾å ‚æ€»ç»“â€ã€‚ä»¥åå¤ä¹ è€ƒè¯•æ—¶ï¼Œä½ çœ‹çš„æ˜¯æ€»ç»“ï¼Œè€Œä¸æ˜¯å®Œæ•´è¯¾å ‚å½•éŸ³ã€‚

### 1.2.3 é©¬å°”å¯å¤«æ¨¡å‹

é©¬å°”å¯å¤«æ¨¡å‹çš„æ€æƒ³å¾ˆç®€å•ï¼š
 ğŸ‘‰ **æœªæ¥åªä¾èµ–æœ€è¿‘çš„æƒ…å†µï¼Œä¸ä¾èµ–æ›´ä¹…è¿œçš„å†å²ã€‚**

- å¦‚æœåªä¾èµ–â€œä¸Šä¸€åˆ»â€çš„çŠ¶æ€ï¼Œè¿™å«â€œä¸€é˜¶é©¬å°”å¯å¤«â€ã€‚
- æ¯”å¦‚é¢„æµ‹æ˜å¤©å¤©æ°”ï¼Œåªçœ‹ä»Šå¤©çš„å¤©æ°”ï¼Œä¸çœ‹æ›´ä¹…ä¹‹å‰çš„ã€‚

å…¬å¼ï¼š

$$
P(x_{t+1} \mid x_1,\ldots,x_t) = P(x_{t+1} \mid x_t)
$$

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
ä½ å†³å®šæ˜å¤©è¦ä¸è¦å¸¦ä¼ï¼Œé€šå¸¸åªçœ‹ä»Šå¤©çš„å¤©æ°”ï¼Œä¸ä¼šå»ç®¡ä¸€ä¸ªæœˆå‰çš„å¤©æ°”ã€‚

### 1.2.4 å› æœå…³ç³»

æ¦‚ç‡çš„å…¬å¼å¯ä»¥éšä¾¿æ¢é¡ºåºï¼Œä½†æ—¶é—´å¯ä¸æ˜¯ã€‚

 ğŸ‘‰ ç°å®ç”Ÿæ´»é‡Œï¼Œ**è¿‡å»å½±å“æœªæ¥ï¼Œä½†æœªæ¥ä¸èƒ½å½±å“è¿‡å»**ã€‚

- è§£é‡Š $P(z_{t+1} \mid z_t)$ï¼ˆæœªæ¥ä¾èµ–äºç°åœ¨ï¼‰æ˜¯åˆç†çš„ï¼›
- è§£é‡Š $P(z_t \mid z_{t+1})$ï¼ˆç°åœ¨ä¾èµ–æœªæ¥ï¼‰å°±ä¸ç¬¦åˆå› æœå…³ç³»ã€‚

ğŸ“Œ **ç”Ÿæ´»ç±»æ¯”**
ä½ ä»Šå¤©æ˜¯å¦ä¼šä¸‹é›¨ï¼Œä¸èƒ½ç”±â€œæ˜å¤©ä¼šä¸ä¼šä¸‹é›¨â€æ¥å†³å®šã€‚

## 1.3 æ€»ç»“

è¿™æ ·è®²ä¸‹æ¥ï¼š

- è‡ªå›å½’æ¨¡å‹ï¼šçœ‹è¿‡å»å‡ æ­¥
- éšå˜é‡æ¨¡å‹ï¼šåšæ€»ç»“ç¬”è®°
- é©¬å°”å¯å¤«æ¨¡å‹ï¼šæœªæ¥åªçœ‹æœ€è¿‘
- å› æœå…³ç³»ï¼šåªèƒ½â€œç”±å‰æ¨åâ€

---

## 1.4 ä»£ç å®è·µ

### 1.4.1 åŸå§‹æ•°æ®
åœ¨ä»£ç å®è·µéƒ¨åˆ†ï¼Œæœ‰ 2 å—å†…å®¹ï¼šä¸€å—æ˜¯å¸¦è¯¦ç»†æ³¨é‡Šçš„â€œè§£é‡Šç‰ˆä»£ç â€ï¼Œå¦ä¸€å—æ˜¯å¯ä»¥ç›´æ¥å¤åˆ¶çš„â€œå¹²å‡€ä»£ç â€ã€‚æœ¬èŠ‚ä½¿ç”¨æ­£å¼¦å‡½æ•°å åŠ é«˜æ–¯å™ªå£°ç”Ÿæˆè®­ç»ƒç”¨çš„æ—¶é—´åºåˆ—ã€‚

**è§£é‡Šç‰ˆä»£ç ï¼š**
```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

# 1. ç”Ÿæˆæ—¶é—´åºåˆ—ç´¢å¼•
T = 1000
time = torch.arange(1, T + 1, dtype=torch.float32)

# 2. æ„é€ å¸¦å™ªå£°çš„æ­£å¼¦ä¿¡å·
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))

# 3. å¯è§†åŒ–
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```

**ä»£ç è¯´æ˜ï¼š**
- `%matplotlib inline` è®© Jupyter åœ¨å•å…ƒæ ¼å†…ç›´æ¥å±•ç¤ºç»˜å›¾ç»“æœã€‚
- `torch.arange(1, T + 1, dtype=torch.float32)` ç”Ÿæˆé•¿åº¦ä¸º `T` çš„æµ®ç‚¹æ—¶é—´ç´¢å¼•ï¼ˆå·¦é—­å³å¼€åŒºé—´ï¼‰ã€‚
- `torch.normal(0, 0.2, (T,))` äº§ç”Ÿæ ‡å‡†å·®ä¸º 0.2 çš„å™ªå£°ï¼Œä¸æ­£å¼¦ä¿¡å·ç›¸åŠ å¾—åˆ° $x = \sin(0.01t) + \varepsilon$ã€‚
- `d2l.plot` æ¥æ”¶ `time` ä¸ `[x]`ï¼ˆæ³¨æ„å¿…é¡»æ˜¯åˆ—è¡¨ï¼‰ç»˜åˆ¶æŠ˜çº¿å›¾ï¼Œ`xlim` ä¸ `figsize` æ§åˆ¶æ¨ªè½´èŒƒå›´å’Œå›¾åƒå°ºå¯¸ã€‚

**å¹²å‡€ä»£ç ï¼š**
```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```

**å…³é”®è¦ç‚¹ï¼š**
- åºåˆ—æ˜¯ç¦»æ•£é‡‡æ ·çš„è¿ç»­ä¿¡å·ï¼›å™ªå£°æœä»å‡å€¼ 0ã€æ ‡å‡†å·® 0.2 çš„æ­£æ€åˆ†å¸ƒã€‚
- `torch.arange` é»˜è®¤ç”Ÿæˆ `[1, T+1)`ï¼Œå› æ­¤å…ƒç´ ä¸ªæ•°æ°å¥½ä¸º `T`ã€‚
- `d2l.plot` çš„ç¬¬äºŒä¸ªå‚æ•°éœ€è¦åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€æ¡æ›²çº¿ä¹Ÿè¦å†™æˆ `[x]`ï¼Œä»¥ä¾¿ç»Ÿä¸€ç»˜å›¾æ¥å£ã€‚

![æ­£å¼¦æ³¢åºåˆ—å›¾]({{ '/assets/img/notes/rnn/figures/sequence_sinwave_plot.png' | relative_url }})

### 1.4.2 è½¬åŒ–

ä¸ºäº†åœ¨ç›‘ç£å­¦ä¹ æ¡†æ¶ä¸‹è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦æŠŠæ—¶é—´åºåˆ—åˆ‡åˆ†ä¸ºâ€œç‰¹å¾ï¼æ ‡ç­¾â€å¯¹ã€‚çª—å£å¤§å° `tau` å†³å®šäº†æ¯ä¸ªç‰¹å¾åŒ…å«çš„å†å²ä¿¡æ¯ï¼š

$$
X_i = [x_i, x_{i+1}, \dots, x_{i+\tau-1}], \quad Y_i = x_{i+\tau}
$$

**çª—å£æ‹†åˆ†ç¤ºä¾‹ï¼ˆ`tau = 5`ï¼‰**
```python
# ç¬¬1ä¸ªæ•°æ®å¯¹
X1 = [x1, x2, x3, x4, x5]
Y1 = x6

# ç¬¬2ä¸ªæ•°æ®å¯¹
X2 = [x2, x3, x4, x5, x6]
Y2 = x7

# ç¬¬3ä¸ªæ•°æ®å¯¹
X3 = [x3, x4, x5, x6, x7]
Y3 = x8
```

- æœ€å‰é¢çš„ `tau` ä¸ªä½ç½®ä¸èƒ½ä½œä¸ºæ ‡ç­¾ï¼Œå› ä¸ºç¼ºå°‘è¶³å¤Ÿçš„å†å²æ•°æ®ï¼ˆä¾‹å¦‚è¦é¢„æµ‹ `x5` æ—¶éœ€è¦ `[x0, x1, x2, x3, x4]`ï¼‰ã€‚
- æ ‡ç­¾æ•°é‡ä¸æ ·æœ¬æ•°é‡ç›¸ç­‰ï¼Œç­‰äº `åºåˆ—é•¿åº¦ - tau`ï¼š

$$
\text{æ€»æ ·æœ¬æ•°} = \text{åºåˆ—é•¿åº¦} - \tau
$$

### 1.4.3 åŸºæœ¬æ•°æ®å¤„ç†

å°†çª—å£åŒ–æ ·æœ¬æ•´ç†æˆå¯è®­ç»ƒçš„æ•°æ®é›†æ—¶ï¼Œä»ç„¶åˆ†ä¸ºâ€œè§£é‡Šç‰ˆä»£ç â€å’Œâ€œå¹²å‡€ä»£ç â€ä¸¤å¥—å®ç°ã€‚è¿™é‡Œä¾æ—§åªä½¿ç”¨å‰ 600 ä¸ªâ€œç‰¹å¾ï¼æ ‡ç­¾â€å¯¹è¿›è¡Œè®­ç»ƒã€‚

**è§£é‡Šç‰ˆä»£ç ï¼š**
```python
# 1. å®šä¹‰ç‰¹å¾çŸ©é˜µï¼šè¡Œ = æ ·æœ¬æ•°ï¼Œåˆ— = çª—å£é•¿åº¦ tau
features = torch.zeros((T - tau, tau))

# 2. æ„é€ æ»åç‰¹å¾ï¼šæŒ‰åˆ—å¡«å……ï¼Œä¿æŒæ—¶é—´é¡ºåº
for i in range(tau):
    features[:, i] = x[i: T - tau + i]

# 3. æ„é€ æ ‡ç­¾ï¼šçª—å£ç»“æŸåçš„ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œreshape æˆåˆ—å‘é‡
labels = x[tau:].reshape((-1, 1))

# 4. ä»…ä½¿ç”¨å‰ n_train ä¸ªæ ·æœ¬å‚ä¸è®­ç»ƒ
batch_size, n_train = 16, 600
train_iter = d2l.load_array(
    (features[:n_train], labels[:n_train]),
    batch_size,
    is_train=True
)
```

**è¡¥å……è¯´æ˜ï¼š**
- `features` çš„æ¯è¡Œå¯¹åº”ä¸€ä¸ªæ ·æœ¬ï¼Œä¾‹å¦‚ `features = [[x1, x2, x3, x4, x5], [x2, x3, x4, x5, x6], â€¦]`ï¼Œæ„æˆæ‰€è°“çš„ã€Œæ»åç‰¹å¾ã€(lag features)ã€‚
- è‹¥ `tau = 3` ä¸” `T = 6`ï¼Œé‚£ä¹ˆï¼š
  - `i = 0` æ—¶ `x[0:3] = [x1, x2, x3] â†’ features[:, 0]`
  - `i = 1` æ—¶ `x[1:4] = [x2, x3, x4] â†’ features[:, 1]`
  - `i = 2` æ—¶ `x[2:5] = [x3, x4, x5] â†’ features[:, 2]`
- æ ‡ç­¾ `labels = x[tau:]`ï¼Œå³ `[x_{tau+1}, x_{tau+2}, â€¦]`ï¼Œå¹¶é€šè¿‡ `reshape((-1, 1))` è½¬æ¢ä¸ºåˆ—å‘é‡ï¼Œ`-1` ä»£è¡¨è‡ªåŠ¨æ¨æ–­æ ·æœ¬æ•°ã€‚
- `d2l.load_array` ç”Ÿæˆå°æ‰¹é‡è¿­ä»£å™¨ï¼Œåªæœ‰å‰ `n_train` æ¡æ ·æœ¬ä¼šå‚ä¸è®­ç»ƒï¼ˆä¾‹å¦‚ 600 æ¡ï¼‰ã€‚

**å¹²å‡€ä»£ç ï¼š**
```python
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))

batch_size, n_train = 16, 600
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```

### 1.4.4 æ¨¡å‹æ¶æ„

æ¥ä¸‹æ¥æ„å»ºä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¹¶ç»™å‡ºè®­ç»ƒæµç¨‹ã€‚ä»ç„¶ä¿ç•™æ³¨é‡Šè¯¦å°½çš„â€œè§£é‡Šç‰ˆä»£ç â€å’Œä¾¿äºå¤åˆ¶çš„â€œå¹²å‡€ä»£ç â€ã€‚

**è§£é‡Šç‰ˆä»£ç ï¼š**
```python
# åˆå§‹åŒ–å…¨è¿æ¥å±‚æƒé‡ï¼Œä¿æŒè¾“å…¥è¾“å‡ºæ–¹å·®ä¸€è‡´ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

# æŒ‰é¡ºåºå †å ä¸¤å±‚çº¿æ€§æ˜ å°„ä¸ ReLU æ¿€æ´»
def get_net():
    net = nn.Sequential(
        nn.Linear(2, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )
    net.apply(init_weights)
    return ne

# MSELoss(reduction='none')ï¼šä¿ç•™æ¯ä¸ªæ ·æœ¬çš„å¹³æ–¹è¯¯å·®
loss = nn.MSELoss(reduction='none')

# è®­ç»ƒå¾ªç¯ï¼šAdam ä¼˜åŒ–å™¨ + æ¢¯åº¦æ¸…é›¶ + åå‘ä¼ æ’­
def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()        # PyTorch æ¢¯åº¦é»˜è®¤ç´¯åŠ ï¼Œéœ€æ‰‹åŠ¨æ¸…é›¶
            l = loss(net(X), y)
            l.sum().backward()         # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            trainer.step()             # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
```

**å¹²å‡€ä»£ç ï¼š**
```python
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

def get_net():
    net = nn.Sequential(nn.Linear(2, 10),
                        nn.ReLU(),
                        nn.Linear(10, 1))
    net.apply(init_weights)
    return ne

loss = nn.MSELoss(reduction='none')

def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.sum().backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
```

![4æ­¥é¢„æµ‹ç»“æœ]({{ '/assets/img/notes/rnn/figures/prediction_4step.png' | relative_url }})

### 1.4.5 é¢„æµ‹

é¢„æµ‹ä»»åŠ¡å¯ä»¥åˆ†ä¸ºå•æ­¥é¢„æµ‹ä¸å¤šæ­¥é¢„æµ‹ï¼šå•æ­¥é¢„æµ‹æ¯æ¬¡åªé¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œå¤šæ­¥é¢„æµ‹ä¼šé€’å½’åœ°å‘å‰æ¨è‹¥å¹²æ­¥ï¼Œè¯¯å·®ä¹Ÿä¼šéšä¹‹ç´¯ç§¯ã€‚

#### å•æ­¥é¢„æµ‹

**è§£é‡Šç‰ˆä»£ç ï¼š**
```python
# ç›´æ¥å°†æ‰€æœ‰ç‰¹å¾é€å…¥è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¾—åˆ°ä¸€æ­¥é¢„æµ‹ç»“æœ
onestep_preds = net(features)

# ç»˜åˆ¶çœŸå®æ›²çº¿ä¸ä¸€æ­¥é¢„æµ‹æ›²çº¿
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()],
         'time', 'x', legend=['data', '1-step preds'],
         xlim=[1, 1000], figsize=(6, 3))
# æ³¨æ„ï¼šé¢„æµ‹åºåˆ—çš„æ—¶é—´ç´¢å¼•æ˜¯ time[tau:]ï¼Œéœ€è¦ä¸ç‰¹å¾çŸ©é˜µå¯¹é½
```

**å¹²å‡€ä»£ç ï¼š**
```python
onestep_preds = net(features)
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()],
         'time', 'x', legend=['data', '1-step preds'],
         xlim=[1, 1000], figsize=(6, 3))
```

![å•æ­¥vså¤šæ­¥é¢„æµ‹å¯¹æ¯”]({{ '/assets/img/notes/rnn/figures/prediction_1step_vs_multistep.png' | relative_url }})

#### å¤šæ­¥é¢„æµ‹

å¤šæ­¥é¢„æµ‹é€šå¸¸é€šè¿‡é€’å½’å®ç°ï¼šæŠŠæ¨¡å‹çš„é¢„æµ‹å€¼å†ä½œä¸ºä¸‹ä¸€æ¬¡è¾“å…¥ã€‚è¿™æ ·èƒ½å¤Ÿç”Ÿæˆä»»æ„é•¿åº¦çš„æœªæ¥åºåˆ—ï¼Œä½†è¯¯å·®ä¼šé€æ­¥æ”¾å¤§ã€‚

**è§£é‡Šç‰ˆä»£ç ï¼š**
```python
# 1. å…ˆå¤åˆ¶å·²æœ‰è§‚æµ‹ï¼Œç»´æŒå‰ n_train + tau ä¸ªçœŸå®æ•°å€¼
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]

# 2. ä»ç¬¬ n_train + tau ä¸ªæ—¶é—´ç‚¹å¼€å§‹é€’å½’é¢„æµ‹
for i in range(n_train + tau, T):
    history = multistep_preds[i - tau:i].reshape((1, -1))
    multistep_preds[i] = net(history)

# 3. ç»˜åˆ¶çœŸå®æ›²çº¿ã€ä¸€é˜¶é¢„æµ‹ã€å¤šæ­¥é¢„æµ‹å¯¹æ¯”
d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(),
          onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()],
         'time', 'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
# å¾ªç¯ä¸­çš„ i ä¾æ¬¡éå† 604, 605, ..., 999ï¼Œæ¯ä¸€æ­¥éƒ½ä¾èµ–å‰ tau ä¸ªå†å²/é¢„æµ‹å€¼
```

**å¹²å‡€ä»£ç ï¼š**
```python
multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(
        multistep_preds[i - tau:i].reshape((1, -1)))

d2l.plot([time, time[tau:], time[n_train + tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy(),
          multistep_preds[n_train + tau:].detach().numpy()],
         'time', 'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```

![Kæ­¥é¢„æµ‹å¯¹æ¯”]({{ '/assets/img/notes/rnn/figures/prediction_k_steps.png' | relative_url }})

**æ€è€ƒï¼šå¤šæ­¥é¢„æµ‹ä¸æ ·æœ¬æ•°é‡**

| å†å²1 | å†å²2 | 1-step | 2-step | 3-step |
| ----- | ----- | ------ | ------ | ------ |
| t1    | t2    | t3     | t4     | t5     |
| t2    | t3    | t4     | t5     | t6     |
| t3    | t4    | t5     | t6     | t7     |

- **1-step é¢„æµ‹**ï¼šä½¿ç”¨ `[t1, t2]` é¢„æµ‹ `t3`ï¼Œå†ç”¨ `[t2, t3]` é¢„æµ‹ `t4`ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- **2-step é¢„æµ‹**ï¼šå…ˆé¢„æµ‹ `t3`ï¼Œå†é¢„æµ‹ `t4`ï¼Œæ¯ä¸€æ­¥éƒ½ä¾èµ–äºå‰ä¸€æ¬¡çš„çœŸå®å€¼æˆ–é¢„æµ‹å€¼ã€‚
- **3-step é¢„æµ‹**ï¼šå°è¯•é¢„æµ‹ `t5ã€t6`ï¼Œè‹¥åºåˆ—é•¿åº¦åªæœ‰ 6ï¼Œåˆ™æ— æ³•ç»§ç»­å¾—åˆ° `t7` çš„çœŸå®æ ‡ç­¾ç”¨äºéªŒè¯ã€‚

å½“åºåˆ—é•¿åº¦ä¸º `T`ã€çª—å£ä¸º `tau`ã€æœ€è¿œé¢„æµ‹æ­¥æ•°ä¸º `max_steps` æ—¶ï¼Œå¯é‡‡æ ·çš„çª—å£æ•°é‡ä¸ºï¼š

$$
N = T - \tau - \text{max\_steps} + 1
$$

**å¤šæ­¥é¢„æµ‹ç‰¹å¾æ„é€ ä¸å¯è§†åŒ–ï¼ˆè§£é‡Šç‰ˆä»£ç ï¼‰ï¼š**
```python
# ç”Ÿæˆç”¨äº K æ­¥é¢„æµ‹çš„ç‰¹å¾çŸ©é˜µ
features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))

# 1. å†å²éƒ¨åˆ†ï¼ˆå‰ tau åˆ—ï¼‰
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# 2. é€’å½’é¢„æµ‹éƒ¨åˆ†ï¼ˆå max_steps åˆ—ï¼‰
for i in range(tau, tau + max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)

# 3. ç»˜åˆ¶ä¸åŒé¢„æµ‹æ­¥é•¿çš„å¯¹æ¯”æ›²çº¿
steps = (1, 4, 16, 64)
d2l.plot(
    [time[tau + i - 1: T - max_steps + i] for i in steps],
    [features[:, tau + i - 1].detach().numpy() for i in steps],
    'time', 'x', legend=[f'{i}-step preds' for i in steps],
    xlim=[5, 1000], figsize=(6, 3)
)
# å‰ tau åˆ—æ˜¯å†å²è§‚æµ‹ï¼Œå max_steps åˆ—æ˜¯é€’å½’é¢„æµ‹
# æ ·æœ¬æ•°å— Tã€tauã€max_steps åŒæ—¶é™åˆ¶
# é€šè¿‡ä¸åŒæ­¥é•¿æ›²çº¿å¯ä»¥è§‚å¯Ÿè¯¯å·®éšé¢„æµ‹è·ç¦»çš„ç´¯ç§¯æƒ…å†µ
```