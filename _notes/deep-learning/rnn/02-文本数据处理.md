---
layout: note_with_toc
title: 2. 文本数据处理
description: Text data preprocessing and tokenization techniques
category: Machine Learning
tags: [RNN, Text Processing, Tokenization, Natural Language Processing]
permalink: /notes/RNN文本数据处理/
---

# 2. 文本数据处理

文本数据是一种**序列数据**，因此可以使用**序列建模**的方法进行处理。在建模之前，需要理解一些基本概念。

### 核心要点

1. **文本转换**
   计算机无法直接理解原始文本，需要将文本转换为计算机可处理的**字符串格式**。

2. **分词（Tokenization）**
   将字符串拆分为更小的单位，称为**词元（token）**。词元可以是单词、子词或者字符。

3. **词典构建（Lexicon）**
   基于词元构建**词典**，通常会用到**词形还原**来规范化词形。这样有助于模型理解文本，也便于后续的数值转换。

4. **数值化（Numerical Conversion）**
   词元需要被转换为**数值索引（整数）**。计算机最终以二进制存储数值，模型也只能处理数值输入。

---

## 2.1 词元（Token）

文本预处理一般包括以下步骤：

1. **文本清洗**

   * 将所有字母转换为**小写**。
   * 移除所有**非字母字符**。
   * 将单词间的分隔符（如标点符号）替换为空格。

   可使用 Python 的 `re.sub()` 函数实现。

2. **词元定义**
   词元的基本单位可以是**单词**或者**字符**。

   示例：

   * 单词级词元：`"word"` → `"word"`
   * 字符级词元：`"word"` → `"w", "o", "r", "d"`

3. **词典创建**
   分词完成后，构建**词典**，将每个词元映射为**数值索引**，用于模型训练。

   示例：

   | 词元  | 索引 |
   | --- | -- |
   | the | 1  |
   | cat | 2  |
   | sat | 3  |
   | on  | 4  |
   | mat | 5  |

---

## 2.2 代码实践

### 2.2.1 基本库

**解释版代码：**
```python
# 1. 统计与容器工具
import collections

# 2. 正则表达式工具
import re

# 3. d2l 深度学习工具库
from d2l import torch as d2l
```

**干净代码：**
```python
import collections
import re
from d2l import torch as d2l
```

**关键说明：**
- `collections` 提供了 `Counter`、`defaultdict` 等数据结构，适合统计词频。
- `re` 支持文本清洗、模式匹配和替换。
- `d2l` 是《动手学深度学习》提供的辅助工具库。


### 2.2.2 读取数据

下载与清洗《时间机器》文本，输出按行的语料列表。

**解释版代码：**
```python
# 配置数据集：包含 URL 与 SHA-1 校验值，便于自动下载与校验
d2l.DATA_HUB['time_machine'] = (
    d2l.DATA_URL + 'timemachine.txt',
    '090b5e7e70c295757f55df93cb0a180b9691891a'
)

def read_time_machine():  #@save
    """将时间机器数据集加载到文本行的列表中"""
    path = d2l.download('time_machine')   # 若未下载会自动获取并校验
    with open(path, 'r') as f:
        lines = f.readlines()
    # 正则：仅保留英文字母，其他字符替换为空格；strip 去除首尾空白；lower 转小写
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower()
            for line in lines]

lines = read_time_machine()
print(f'# 文本总行数: {len(lines)}')
print(lines[0])
print(lines[10])
```

**干净代码：**
```python
d2l.DATA_HUB['time_machine'] = (
    d2l.DATA_URL + 'timemachine.txt',
    '090b5e7e70c295757f55df93cb0a180b9691891a'
)

def read_time_machine():  #@save
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower()
            for line in lines]

lines = read_time_machine()
print(f'# 文本总行数: {len(lines)}')
print(lines[0])
print(lines[10])
```

**关键说明：**
- `DATA_HUB` 统一管理数据集元数据，`d2l.download()` 会自动下载并进行 SHA-1 校验。
- `re.sub('[^A-Za-z]+', ' ', line)` 仅保留字母字符，其余替换为空格。
- `.strip().lower()` 分别用于去除首尾空白和转换为小写。

### 2.2.3 词元化

将文本行转换为基本单位——**词元（token）**，可选择按单词或字符切分。

**解释版代码：**
```python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元"""
    if token == 'word':
        return [line.split() for line in lines]   # 按空白拆分单词
    elif token == 'char':
        return [list(line) for line in lines]     # 拆分成单个字符
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

**干净代码：**
```python
def tokenize(lines, token='word'):  #@save
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

**关键说明：**
- `line.split()` 将字符串按空白字符拆分，例如 `"Hello world Python" → ['Hello', 'world', 'Python']`。
- `list(line)` 将字符串逐字符拆分，例如 `"Hello" → ['H', 'e', 'l', 'l', 'o']`。
- 清洗后行中若无字母，则对应 `tokens[i]` 为 `[]`。

### 2.2.4 词表化

构建词表（Vocabulary）类，实现词元与数值索引之间的映射。

**初始化方法**：

```python
class Vocab:
    """文本词表"""
    
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        """
        参数:
        tokens: 输入的文本序列，例如 ["我", "爱", "学习", "学习"]
        min_freq: 词的最小出现频率，低于此值不加入词表
        reserved_tokens: 预留的特殊符号列表，如 ["<pad>", "<bos>", "<eos>"]
        """
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []

        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}

        # 将高频词加入词表
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
```

**核心数据结构**：

1. `self._token_freqs`：存储词频信息，格式为 `[('词', 频率), ...]`
2. `self.idx_to_token`：索引到词元的映射（列表）
3. `self.token_to_idx`：词元到索引的映射（字典）

**示例**：
```python
# 添加新词 'hello' 的过程：
# 步骤1: idx_to_token.append('hello')
# 步骤2: token_to_idx['hello'] = len(idx_to_token) - 1

# 结果：
# idx_to_token = ['<unk>', '<pad>', 'hello']
# token_to_idx = {'<unk>': 0, '<pad>': 1, 'hello': 2}
```

**其他方法**：

```python
    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        """根据词元获取索引"""
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        """根据索引获取词元"""
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):
        """未知词元的索引为0"""
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs
```

**辅助函数 - 统计词频**：

```python
def count_corpus(tokens):  #@save
    """统计词元的频率"""
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将二维列表展平成一维列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

**说明**：
- `__getitem__`：支持 `vocab[token]` 语法，返回词元对应的索引
- `to_tokens`：根据索引返回词元
- `count_corpus`：使用 `Counter` 统计词频，支持1D和2D列表

**使用示例**：
```python
tokens1 = ["I", "am", "you", "I"]
print(count_corpus(tokens1))
# 输出: Counter({'I': 2, 'am': 1, 'you': 1})

tokens2 = [["I", "am"], ["you", "are", "I"]]
print(count_corpus(tokens2))
# 输出: Counter({'I': 2, 'am': 1, 'you': 1, 'are': 1})
``` 

```python
class Vocab:  #@save
    """文本词表"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # 未知词元的索引为0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """统计词元的频率"""
    # 这里的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成一个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

### 2.2.5 整合使用

**创建词表对象**：

```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
```

**查看词元和索引的映射**：

```python
for i in [0, 10]:
    print('文本:', tokens[i])
    print('索引:', vocab[tokens[i]])
```


**加载语料库函数**：

```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词表"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    
    # 将所有文本行展平到一个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab
```

**说明**：
- `corpus`：词元索引的一维列表
- `vocab[token]` 调用 `__getitem__` 方法将词元转换为索引
- `max_tokens`：限制语料库长度，默认 -1 表示使用全部

**使用示例**：

```python
corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

### 2.2.6 问题讨论

#### 问题一：常见的词元化方法

**1. 正则表达式词元化（Regex Tokenization）**

```python
import re
text = "ChatGPT is great, isn't it?"
tokens = re.findall(r"\b\w+\b", text) 去掉标点、识别单词、数字
print(tokens)

['ChatGPT', 'is', 'great', 'isn', 't', 'it']
```

**2. NLTK 分词器（Word Tokenizer）**

```python
import nltk
from nltk.tokenize import word_tokenize 
nltk.download('punkt')
text = "ChatGPT is great, isn't it?"
tokens = word_tokenize(text) 支持缩写、标点
print(tokens)

['ChatGPT', 'is', 'great', ',', "isn't", 'it', '?']
```

**3. 中文分词：jieba**

```python
import jieba
text = "我爱自然语言处理"
tokens = jieba.lcut(text) 针对中文设计，效果优秀。
print(tokens)

['我', '爱', '自然语言处理']
```

#### 问题二：改变 min_freq 参数的影响

修改 `Vocab` 的 `min_freq` 参数会影响词表大小。

```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """返回时光机器数据集的词元索引列表和词表"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens, min_freq=100)  # 可以设置为 100/1000/10000
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

**示例说明**：

假设语料中的词频统计结果如下：

| 单词       | 频次  |
| -------- | --- |
| the      | 120 |
| deep     | 30  |
| learning | 20  |
| python   | 5   |
| awesome  | 1   |
| amazing  | 1   |

### 情况 1：`min_freq=1`

→ 所有单词都保留
**词表大小 = 6**

### 情况 2：`min_freq=5`

→ 只保留出现≥5次的单词
**词表大小 = 4**（即 `['the', 'deep', 'learning', 'python']`）

### 情况 3：`min_freq=20`

→ 只保留出现≥20次的单词
**词表大小 = 2**（即 `['the', 'deep']`）

---
