---
layout: note_with_toc
title: 6 通过时间反向传播
description: Building RNN from scratch with detailed implementation
category: Machine Learning
tags: [RNN, Deep Learning, Neural Networks, PyTorch, Implementation]
permalink: /notes/通过时间反向传播/
---

# 6. 通过时间反向传播

这部份将会模型的反向传播 和 detach 更好地解释和使用。

## 6.1 通过时间反向传播简介

反向传播，本身是从结果往后面求梯度，基于这里的数据是序列有时间信息，因此，从时间的后面计算，这里就可以理解为从时间穿越。

当然，还需要考虑的是计算图。


### 6.1.1 隐藏状态的更新（Activation of the Hidden State）：

$$h_t = f(W_h h_{t-1} + W_x x_t)$$

* **$h_t$**: 第 $t$ 时刻的**隐藏状态**。它捕获了从序列开始到 $t$ 时刻的所有信息。
* **$h_{t-1}$**: 上一个时刻（$t-1$）的隐藏状态。
* **$x_t$**: 当前时刻 $t$ 的**输入**。
* **$W_x$**: 连接输入 $x_t$ 到隐藏层 $h_t$ 的**权重矩阵**。
* **$W_h$**: 连接上一时刻隐藏状态 $h_{t-1}$ 到当前隐藏层 $h_t$ 的**权重矩阵**。
    * **关键点**: $W_x$ 和 $W_h$ 在所有时间步 $t$ 都是**共享**的，这是 RNN 的核心特征。
* **$f$**: 非线性激活函数（如 $\tanh$ 或 $\text{ReLU}$），用于引入非线性，使网络能学习更复杂的模式。

$$y_t = g(W_y h_t)$$

* **$y_t$**: 第 $t$ 时刻的**输出**（例如，下一个词的概率分布）。
* **$W_y$**: 连接隐藏状态 $h_t$ 到输出 $y_t$ 的**权重矩阵**。
* **$g$**: 另一个非线性激活函数（例如，对于分类任务，通常是 $\text{softmax}$）。

### 6.1.2 目标函数（Objective/Loss Function）


$$L = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t)$$

* **$L$**: 整个序列的**总损失**。
* **$T$**: 序列的**总时间步长**。
* **$\ell(y_t, \hat{y}_t)$**: 在**单个时间步 $t$** 上的损失函数（例如，交叉熵损失或均方误差）。
    * **$\hat{y}_t$**: 第 $t$ 时刻的**真实标签或目标值**（Ground Truth）。
    * **$y_t$**: 第 $t$ 时刻的**模型预测输出**。

这里实际上就是每个时间步上损失函数之和加在一起使得

**训练目标**：通过调整权重参数 $W_x, W_h, W_y$，使总损失 $L$ **最小化**。

## 6.2 梯度推导

有三个基本参数：$W_h, W_x, W_y$


对于输出层权重 $W_y$，由于它只影响**当前时间步 $t$ 的输出 $y_t$**，所以梯度计算相对直接，不需要追溯到过去的隐状态：

$$\frac{\partial L}{\partial W_y} = \sum_{t} \frac{\partial \ell_t}{\partial y_t} \frac{\partial y_t}{\partial W_y}$$

* $L$: 总损失。
* $\ell_t$: 时间步 $t$ 的损失。


$$\frac{\partial L_t}{\partial W_h} = \frac{\partial L_t}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial W_h}$$



\[ \mathbf{L} \xrightarrow{\text{关于 } y} \mathbf{y}_t \xrightarrow{\text{关于 } h} \mathbf{h}_t \xrightarrow{\text{关于 } W_h} \mathbf{W}_h\]


这里的关键是在于最后一项：


$$h_t = f(W_h h_{t-1} + W_x x_t + b)$$

问题的关键就在于产生了递归 



### 6.1.4 关键的链式法则展开：误差信号的递推





 









