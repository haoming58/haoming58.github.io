---
layout: note_with_toc
title: 6 通过时间反向传播
description: BPTT算法详解：梯度推导、计算效率分析与截断策略
category: Machine Learning
subcategory: RNN Basics
tags: [RNN, Deep Learning, BPTT, Backpropagation, Gradient Computation]
permalink: /notes/backpropagation-through-time/
redirect_from:
  - /notes/通过时间反向传播/
---

# 6. 通过时间反向传播

本章将带你深入理解RNN的反向传播算法（BPTT），我们会一起探讨梯度推导的原理、计算效率的优化方法，以及实际应用中的截断策略和`detach()`的使用技巧。

## 6.1 通过时间反向传播简介

反向传播（Backpropagation）本质上是从输出向输入逆向计算梯度。当数据是序列且包含时间信息时，梯度会从时间的后面往前计算，这就是**通过时间反向传播（BPTT）**的核心思想。

要理解BPTT，关键是要弄清楚**计算图在时间轴上的展开**以及梯度如何通过时间步递归传播。

### 6.1.1 基本概念

RNN的特点是在所有时间步共享参数，这导致梯度计算需要考虑多条路径的贡献。

### 6.1.2 隐藏状态的更新

$$h_t = f(W_h h_{t-1} + W_x x_t)$$

* $h_t$：第 $t$ 时刻的隐藏状态，它捕获了从序列开始到 $t$ 时刻的所有信息
* $h_{t-1}$：上一个时刻（$t-1$）的隐藏状态
* $x_t$：当前时刻 $t$ 的输入
* $W_x$：连接输入 $x_t$ 到隐藏层 $h_t$ 的权重矩阵
* $W_h$：连接上一时刻隐藏状态 $h_{t-1}$ 到当前隐藏层 $h_t$ 的权重矩阵
    * 关键点：$W_x$ 和 $W_h$ 在所有时间步 $t$ 都是共享的，这是 RNN 的核心特征
* $f$：非线性激活函数（如 $\tanh$ 或 $\text{ReLU}$），用于引入非线性，使网络能学习更复杂的模式

**输出层：**

$$y_t = g(W_y h_t)$$

* $y_t$：第 $t$ 时刻的输出（例如，下一个词的概率分布）
* $W_y$：连接隐藏状态 $h_t$ 到输出 $y_t$ 的权重矩阵
* $g$：另一个非线性激活函数（例如，对于分类任务，通常是 $\text{softmax}$）

### 6.1.3 目标函数


$$L = \sum_{t=1}^{T} \ell(y_t, \hat{y}_t)$$

* $L$：整个序列的总损失
* $T$：序列的总时间步长
* $\ell(y_t, \hat{y}_t)$：在单个时间步 $t$ 上的损失函数（例如，交叉熵损失或均方误差）
    * $\hat{y}_t$：第 $t$ 时刻的真实标签或目标值（Ground Truth）
    * $y_t$：第 $t$ 时刻的模型预测输出

总损失是每个时间步损失之和。

训练目标：通过调整权重参数 $W_x, W_h, W_y$，使总损失 $L$ 最小化。

## 6.2 链式法则基础

在正式推导BPTT之前，我们先来了解两个重要的基础概念：**多路径梯度**和**链式法则的简化**。这两个概念会帮助我们更好地理解后面的内容。

### 6.2.1 多路径梯度

在神经网络中，一个变量往往通过多条路径影响最终结果。假设 $L$ 依赖于 $Y$ 和 $Z$，而 $Y$ 和 $Z$ 都依赖于 $W$：

$$L = f(Y, Z) \quad \text{且} \quad Y = g(W), Z = h(W)$$

当我们计算 $L$ 对 $W$ 的梯度时，会考虑所有路径的贡献：

$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Y} \frac{dY}{dW} + \frac{\partial L}{\partial Z} \frac{dZ}{dW}$$

这里有两个要点：
- 我们使用偏导数 $\partial$，因为函数有多个输入
- 需要将所有路径的贡献求和 $\sum$

直观理解：你的努力 $W$ 通过两条路径影响成绩 $L$：
1. 路径一：$W \to$ 学习时间 $Y$ $\to$ 成绩 $L$
2. 路径二：$W \to$ 睡眠时间 $Z$ $\to$ 成绩 $L$

总影响 = 路径一的贡献 + 路径二的贡献

### 6.2.2 链式法则的简化与抽象

在 BPTT 中，当我们计算参数（如 $W_h$）对**当前时刻损失 $\ell_t$** 的梯度时，完整的链式法则是：

$$\frac{\partial \ell_t}{\partial W_h} = \left( \frac{\partial \ell_t}{\partial y_t} \cdot \frac{\partial y_t}{\partial h_t} \right) \cdot \frac{\partial h_t}{\partial W_h}$$

(即 $W_h \to h_t \to y_t \to \ell_t$)

为了让推导过程更加清晰，我们可以将 $h_t \to y_t \to \ell_t$ 这个“中间部分”打包成一个整体：

$$\frac{\partial \ell_t}{\partial h_t} \equiv \left( \frac{\partial \ell_t}{\partial y_t} \cdot \frac{\partial y_t}{\partial h_t} \right)$$

这样做的原因是：
* 这条路径 $h_t \to y_t \to \ell_t$ 是局部的、非递归的
* 它只发生在当前时间步 $t$ 内部，不依赖其他时刻
* $\frac{\partial \ell_t}{\partial h_t}$ 可以理解为“$h_t$ 对当前损失 $\ell_t$ 的局部影响”

现在来看核心问题

BPTT 的难点其实在于时间轴上的依赖关系：
- 来自未来的递归：$h_t$ 如何被未来的 $\ell_{t+1}, \dots, \ell_T$ 影响（通过 $\frac{\partial L}{\partial h_{t+1}}$ 传回）
- 来自过去的长链：$h_t$ 如何依赖过去的状态（通过 $\frac{\partial h_t}{\partial h_{t-1}} \dots$ 传导）

简化后的核心公式

通过上述简化，我们得到BPTT的核心递归结构：

$$\frac{\partial L}{\partial h_t} = \underbrace{\frac{\partial \ell_t}{\partial h_t}}_{\text{局部影响}} + \underbrace{\frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}}_{\text{来自未来的递归}}$$

## 6.3 梯度推导

### 6.3.1 参数梯度的基本形式

RNN有三个需要训练的参数：$W_h, W_x, W_y$

输出层权重 $W_y$：由于它只影响当前时间步的输出，梯度计算相对直接：

$$\frac{\partial L}{\partial W_y} = \sum_{t=1}^T \frac{\partial \ell_t}{\partial y_t} \frac{\partial y_t}{\partial W_y}$$

隐藏层权重 $W_h$ 和 $W_x$：由于参数在所有时间步共享，计算梯度时需要累加所有路径的贡献：

$$\frac{\partial L}{\partial W_h} = \sum_{t=1}^T \frac{\partial \ell_t}{\partial W_h}$$

### 6.3.2 引入中间变量 $\frac{\partial L}{\partial h_t}$ 的必要性

引入 $\frac{\partial L}{\partial h_t}$ 可以帮助我们**解耦（decouple）**计算过程，从而避免重复计算。

1. 如果没有 $\frac{\partial L}{\partial h_t}$（效率低下的做法）

假设我们要计算 $W_h$ 和 $W_x$ 对 $L$ 的梯度，并且我们从 $t=T$ 算到 $t=1$。

当我们在 $t=1$ 时刻时：
* **计算 $\frac{\partial L}{\partial W_h}$：** 我们需要追溯 $W_h$ 通过 $h_1 \to h_2 \to \dots \to h_T$ 影响 $L$ 的所有路径。这个计算涉及一个极其复杂的、横跨 $T$ 个时间步的乘法链。
* **计算 $\frac{\partial L}{\partial W_x}$：** $W_x$ 也要通过 $h_1 \to h_2 \to \dots \to h_T$ 影响 $L$。这个计算涉及另一个、但几乎完全相同的、横跨 $T$ 个时间步的乘法链。

这里的问题是：计算 $W_h$ 和 $W_x$ 的梯度时，我们会重复计算从 $h_1$ 到 $h_T$ 的所有梯度传播项，这会浪费很多计算资源。

2. 使用 $\frac{\partial L}{\partial h_t}$（更高效的方法）

现在我们引入 $\frac{\partial L}{\partial h_t}$ 作为中间变量的梯度。

当我们在 $t=1$ 时刻时：

* 第一步：计算并存储 $\frac{\partial L}{\partial h_1}$
    我们使用递归公式（依赖于 $\frac{\partial L}{\partial h_2}, \frac{\partial L}{\partial h_3}, \dots$ 这些之前已经算好的值）来计算 $\frac{\partial L}{\partial h_1}$。这个 $\frac{\partial L}{\partial h_1}$ 包含了所有未来时刻的误差信息。
    * *（递归公式：$\frac{\partial L}{\partial h_t} = \frac{\partial \ell_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t}$）*

* 第二步：计算 $\frac{\partial L}{\partial W_h}$
    $$\frac{\partial L}{\partial W_h} \propto \frac{\partial L}{\partial h_1} \cdot \frac{\partial h_1}{\partial W_h}$$
    （我们使用 $\frac{\partial L}{\partial h_1}$）

* 第三步：计算 $\frac{\partial L}{\partial W_x}$
    $$\frac{\partial L}{\partial W_x} \propto \frac{\partial L}{\partial h_1} \cdot \frac{\partial h_1}{\partial W_x}$$
    （我们再次使用 $\frac{\partial L}{\partial h_1}$）

这样做的好处：$\frac{\partial L}{\partial h_t}$ 只需计算一次，就可以被多个参数的梯度计算重用，非常高效。

## 6.4 计算效率分析

### 6.4.1 T=10 的实例

考虑一个长度为 $T=10$ 的序列，对比两种计算方法的效率差异。

模型设置：
* $h_t = W_h h_{t-1} + W_x x_t$
* $L = \sum_{t=1}^{10} \ell_t$ (假设 $\ell_t = \frac{1}{2}h_t^2$)

目标：计算 $\frac{\partial L}{\partial W_h}$ 和 $\frac{\partial L}{\partial W_x}$。

### 6.4.2 方法一：朴素方法（低效）

关注计算中**最长、最昂贵**的传播路径：从 $t=1$ 的输入，一直传播到 $t=10$ 的最终损失 $\ell_{10}$。

这个最长的路径是：
$$\frac{\partial \ell_{10}}{\partial h_1} = \frac{\partial \ell_{10}}{\partial h_{10}} \cdot \frac{\partial h_{10}}{\partial h_9} \cdot \frac{\partial h_9}{\partial h_8} \cdot \dots \cdot \frac{\partial h_2}{\partial h_1}$$

这个计算涉及 9 次矩阵乘法（在我们的例子中是 $h_{10} \cdot W_h \cdot W_h \cdot \dots \cdot W_h = h_{10} \cdot (W_h)^9$）。这是一个非常昂贵的计算。（*推导：$\frac{\partial \ell_{10}}{\partial h_{10}} = \frac{\partial (\frac{1}{2}h_{10}^2)}{\partial h_{10}} = h_{10}$ | $\frac{\partial h_t}{\partial h_{t-1}} = \frac{\partial (W_h h_{t-1} + W_x x_t)}{\partial h_{t-1}} = W_h$*）

**A. 计算 $\frac{\partial L}{\partial W_x}$**
总梯度 $\frac{\partial L}{\partial W_x} = \sum_{t=1}^{10} \frac{\partial \ell_t}{\partial W_x}$。
当我们计算 $W_x$ 在 $t=1$ 时的贡献 $\frac{\partial \ell_{10}}{\partial W_x}$ 时，我们需要：
$$\left( \frac{\partial \ell_{10}}{\partial h_{10}} \cdot \dots \cdot \frac{\partial h_2}{\partial h_1} \right) \cdot \frac{\partial h_1}{\partial W_x}\big|_{\text{local}}$$
* 我们必须计算 $h_{10} \cdot (W_h)^9$ 这一项。

**B. 计算 $\frac{\partial L}{\partial W_h}$**
总梯度 $\frac{\partial L}{\partial W_h} = \sum_{t=1}^{10} \frac{\partial \ell_t}{\partial W_h}$。
当我们计算 $W_h$ 在 $t=1$ 时的贡献 $\frac{\partial \ell_{10}}{\partial W_h}$ 时，我们需要：
$$\left( \frac{\partial \ell_{10}}{\partial h_{10}} \cdot \dots \cdot \frac{\partial h_2}{\partial h_1} \right) \cdot \frac{\partial h_1}{\partial W_h}\big|_{\text{local}}$$
* 我们**再一次**必须计算 $h_{10} \cdot (W_h)^9$ 这一项。

低效点：为 $W_x$ 和 $W_h$ 重复计算相同的长链乘积 $h_{10} \cdot (W_h)^9$。

这不仅发生在 $\ell_{10} \to h_1$ 这条路径上，计算 $\ell_{10} \to h_2$、$\ell_9 \to h_1$、$\ell_8 \to h_3$……每一条跨时间的误差传播路径，都必须为不同参数重复计算。

### 6.4.3 方法二：BPTT算法（高效）

首先计算并存储所有时间步的误差信号 $\frac{\partial L}{\partial h_t}$，然后重用它们计算各参数的梯度。

阶段一：计算并存储 $\frac{\partial L}{\partial h_t}$（倒序递归）

我们从 $t=10$ 开始倒序计算，只计算一次。

1. $t=10$: $\frac{\partial L}{\partial h_{10}} = \frac{\partial \ell_{10}}{\partial h_{10}} = h_{10}$
   * 存储 $\frac{\partial L}{\partial h_{10}}$
2. $t=9$: $\frac{\partial L}{\partial h_9} = \frac{\partial \ell_9}{\partial h_9} + \frac{\partial L}{\partial h_{10}} \cdot \frac{\partial h_{10}}{\partial h_9} = h_9 + (\frac{\partial L}{\partial h_{10}} \cdot W_h)$
   * 存储 $\frac{\partial L}{\partial h_9}$
... (依此类推) ...
10. $t=1$: $\frac{\partial L}{\partial h_1} = \frac{\partial \ell_1}{\partial h_1} + \frac{\partial L}{\partial h_2} \cdot \frac{\partial h_2}{\partial h_1} = h_1 + (\frac{\partial L}{\partial h_2} \cdot W_h)$
    * 存储 $\frac{\partial L}{\partial h_1}$

关键点：当我们计算 $\frac{\partial L}{\partial h_1}$ 时，它已经自动包含了 $h_{10} \cdot (W_h)^9$、$h_9 \cdot (W_h)^8$ 等所有来自未来的、昂贵的长链乘积的总和。

阶段二：使用 $\frac{\partial L}{\partial h_t}$ 计算参数（高效重用）

现在我们有了所有打包好的误差信号 $\frac{\partial L}{\partial h_t}$ ($t=1$ 到 $10$)。

**A. 计算 $\frac{\partial L}{\partial W_x}$**
$$\frac{\partial L}{\partial W_x} = \sum_{t=1}^{10} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_x}\big|_{\text{local}}$$
$$\frac{\partial L}{\partial W_x} = \sum_{t=1}^{10} \left( \frac{\partial L}{\partial h_t} \cdot x_t \right)$$
*(这是一个简单的循环，只涉及**本地**计算和**查表**（读取 $\frac{\partial L}{\partial h_t}$）)*

**B. 计算 $\frac{\partial L}{\partial W_h}$**
$$\frac{\partial L}{\partial W_h} = \sum_{t=1}^{10} \frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_h}\big|_{\text{local}}$$
$$\frac{\partial L}{\partial W_h} = \sum_{t=1}^{10} \left( \frac{\partial L}{\partial h_t} \cdot h_{t-1} \right)$$
*(这同样是一个简单的本地计算和查表)*

### 6.4.4 深入理解：$O(T^2)$ vs $O(T)$

您可能会观察到：对于**单一路径** $\ell_{10} \to h_1$，我可以只计算一次 $C = h_{10} \cdot (W_h)^9$ 并重用它。

“方法一”的**真正低效**之处在于：$W_x$ 和 $W_h$ 在 $t=1$ 时的局部贡献，需要**所有未来损失**（$\ell_2, \ell_3, \dots, \ell_{10}$）传播回来的**总和**，而不仅仅是 $\ell_{10}$ 这一条路径。

要计算 $t=1$ 的总贡献，您必须**手动计算并求和 10 条不同的传播链**：

1.  **$\ell_1 \to h_1$ 的贡献：**
    $C_1 = \frac{\partial \ell_1}{\partial h_1} = h_1$
2.  **$\ell_2 \to h_1$ 的贡献：**
    $C_2 = \frac{\partial \ell_2}{\partial h_2} \cdot \frac{\partial h_2}{\partial h_1} = h_2 \cdot W_h$
3.  **$\ell_3 \to h_1$ 的贡献：**
    $C_3 = \frac{\partial \ell_3}{\partial h_3} \cdot \frac{\partial h_3}{\partial h_2} \cdot \frac{\partial h_2}{\partial h_1} = h_3 \cdot (W_h)^2$
...
10. **$\ell_{10} \to h_1$ 的贡献：**
    $C_{10} = \frac{\partial \ell_{10}}{\partial h_{10}} \cdot \dots \cdot \frac{\partial h_2}{\partial h_1} = h_{10} \cdot (W_h)^9$

在“方法一”中，要计算 $W_x$ 和 $W_h$ 在 $t=1$ 的总贡献，您必须：
1.  **手动计算所有 10 条路径** $C_1, C_2, \dots, C_{10}$。
2.  **将它们求和：** $C_{\text{total}} = C_1 + C_2 + \dots + C_{10}$。
3.  **用于 $W_x$：** $\text{Gradient}_{W_x} = C_{\text{total}} \cdot \frac{\partial h_1}{\partial W_x}\big|_{\text{local}}$
4.  **用于 $W_h$：** $\text{Gradient}_{W_h} = C_{\text{total}} \cdot \frac{\partial h_1}{\partial W_h}\big|_{\text{local}}$

“方法一”需要**手动计算并存储 $O(T^2)$ 条路径**（$10+9+8+\dots+1 = 55$ 条）。

**BPTT的动态规划优势**

BPTT算法通过递归公式自动完成所有求和：

$$\frac{\partial L}{\partial h_1} = \frac{\partial \ell_1}{\partial h_1} + \frac{\partial L}{\partial h_2} \cdot \frac{\partial h_2}{\partial h_1}$$

这个 $\frac{\partial L}{\partial h_1}$ 在数学上等价于：

$$\frac{\partial L}{\partial h_1} = C_1 + C_2 + \dots + C_{10}$$

但只需 $O(T)$ 时间就能计算完成，然后可以被所有参数的梯度计算重用。

### 6.4.5 复杂度总结

| 方法 | 时间复杂度 | 序列长度×10 | 计算量变化 |
|------|-----------|------------|-----------|
| 方法一（朴素） | $O(T^2)$ | $T \to 10T$ | ×100 |
| 方法二（BPTT） | $O(T)$ | $T \to 10T$ | ×10 |

**核心思想**：计算一次，多次重用。$\frac{\partial L}{\partial h_t}$ 是BPTT实现高效梯度计算的关键。

## 6.5 截断BPTT

### 6.5.1 为什么需要截断

完整的BPTT在实际应用中存在问题：
- **计算代价高**：长序列需要大量内存和计算
- **梯度消失/爆炸**：长链乘积导致梯度不稳定
- **长期依赖难以学习**：实践中很难捕获非常远的依赖

因此，实际应用中常采用**截断策略**进行近似计算。

### 6.5.2 常规截断（Truncated BPTT）

**核心思想**：只计算最近 $\tau$ 步的梯度，使用滑动窗口方式。这是工业界和学术界最常用的解决方案。

**机制（detach()的作用）**：

- **前向传播**：隐状态 $h_t$ 依然携带之前所有时刻的信息，可以从头流到尾
  - 比喻：看连续剧时，你会带着上一集的记忆去看下一集
  
- **反向传播**：梯度只回溯 $\tau$ 步，不再往前传播
  - 比喻：写影评时，只回顾最近几集的内容，不会修正更早集数的记忆

`detach()` 是梯度的"停止标志"：
- 允许信息（$h_t$）在前向传播时通过
- 阻止梯度（$\frac{\partial L}{\partial h_t}$）在反向传播时流过

![alt text](../../../assets/img/notes/rnn/figures/8.7.1.png)

### 6.5.3 随机截断

随机截断是另一种策略，在每个时间步以一定概率截断梯度流，可以看作是常规截断的随机化版本。

**核心思想：** 我们想要一个**无偏（unbiased）**的梯度估计。我们希望模型有时能看到长距离依赖，但又不想总是承受长距离的计算和不稳定性。

**做法（好比抽查）：**

你还是看那部 1000 分钟的电影。

但这次，你每看完一分钟，就掷骰子：
- 90% 的概率，你停止回顾（截断）
- 10% 的概率，你继续往前回顾

**机制（无偏估计）：**

- **短链（高概率）**：大多数时候，你只回顾几分钟（梯度链很短），计算稳定且快速
- **长链（低概率）**：极少数时候，你（通过随机）被允许回顾 100 分钟
- **补偿（加权）**：为了让这种抽查在数学上是公平的（无偏），那些低概率的长链梯度会被赋予极高的权重

*比喻：你很少听取某个下属的意见，但一旦你听了，你会给这个意见 10 倍的重视，以确保平均来看你没有忽视他。*de

## 6.6 总结

BPTT是训练RNN的核心算法，通过以下方式实现高效的梯度计算：

1. **引入中间变量** $\frac{\partial L}{\partial h_t}$，避免重复计算
2. **递归计算**：从 $t=T$ 到 $t=1$ 倒序计算误差信号
3. **重用机制**：所有参数共享同一组误差信号
4. **截断策略**：实际应用中使用截断BPTT平衡效率和效果

**时间复杂度**：从朴素方法的 $O(T^2)$ 降低到 $O(T)$。
