---
layout: note_with_toc
title: 3. 语言模型和数据集
description: Language models and dataset processing for RNN
category: Machine Learning
tags: [RNN, Language Model, N-gram, Dataset, Deep Learning]
permalink: /notes/RNN语言模型和数据集/
---

# 3. 语言模型和数据集

## 3.1 核心思想

语言模型将处理好的词元（可以是单个字母或单词）看作一个长度为 $T$ 的序列，每个位置的词都是一个观测值：

$$
x_1, x_2, \dots, x_T
$$

语言模型的任务是估计该序列的联合概率分布：

$$
P(x_1, x_2, \dots, x_T)
$$

根据链式法则，这个联合概率可以分解为一系列条件概率的乘积：

$$
P(x_1, x_2, \dots, x_T) = \prod_{t=1}^{T} P(x_t \mid x_1, x_2, \dots, x_{t-1})
$$

这样分解是因为联合概率的维度过高，无法一次性预测整篇文本。通过一步步预测下一个词，模型将复杂的大问题拆解成多个小问题。模型通过学习条件概率分布，就能在给定上下文的情况下生成文本。

**核心思想**：通过统计语言规律，预测序列中每个词出现的概率。

## 3.2 参数与概率估计

最简单的频率估计方法：

$$
P(\text{deep}) = \frac{\text{单词 "deep" 出现的次数}}{\text{语料库中所有单词的总次数}}
$$

表示单词出现的相对频率。

### 3.2.2 条件概率

计算连续两个词的概率，例如"learning"在"deep"之后出现的概率：
$$
P(\text{learning} \mid \text{deep}) = \frac{\text{"deep learning" 出现的次数}}{\text{"deep" 出现的次数}}
$$

随着序列长度增加，会出现许多组合从未出现过，导致概率为 0，这就是**零概率问题**。

### 3.2.3 平滑技术

引入 **拉普拉斯平滑（Laplace Smoothing）** 给未出现的词分配非零概率：

$$
P_{\text{smooth}}(w) = \frac{\text{出现次数} + 1}{\text{总词数} + V}
$$

更一般的形式：

$$
P(w) = \frac{\text{次数} + \alpha}{\text{总词数} + \alpha V}
$$

- $V$ 是词表大小  
- $\alpha$ 控制平滑强度  
  - $\alpha = 0$ 时无平滑  
  - $\alpha$ 很大时接近均匀分布

语言模型通过统计词与词之间的共现规律，利用概率和条件概率描述语言结构；平滑技术保证模型面对未见序列时仍能给出合理概率估计。

## 3.3 马尔可夫模型与 n-gram

### 马尔可夫模型

如果序列满足 **一阶马尔可夫性质**：

$$
P(w_n \mid w_1, w_2, \dots, w_{n-1}) \approx P(w_n \mid w_{n-1})
$$

当前词只依赖前一个词。阶数越高，模型捕捉的依赖关系越长，为序列建模提供近似公式。


根据上下文长度不同，可定义：

- **一元语法（Unigram）**  
  只考虑单个词出现概率，不包含上下文。  
  示例：句子"猫 喜欢 睡觉"中  
  $$
  P(\text{猫}) = \frac{1}{3}
  $$

- **二元语法（Bigram）**  
  考虑每个词在前一个词条件下出现概率。  
  示例：
  $$
  P(\text{睡觉} \mid \text{猫}) = 0
  $$
  $$
  P(\text{喜欢} \mid \text{猫}) = 1
  $$

- **三元语法（Trigram）**  
  考虑每个词在前两个词条件下出现概率。  
  示例：
  $$
  P(\text{睡觉} \mid \text{猫 喜欢}) = 1
  $$

**特点**：

- 通过统计固定长度词组合出现频率，捕捉词之间依赖关系  
- 随着 n 增大：  
  - 模型复杂度增加  
  - 数据稀疏问题加剧  
- 实际应用中需在上下文长度与计算代价间平衡

## 3.4 齐普夫定律

自然语言中，词频遵循 **幂律分布**（Zipf's Law）：

- 高频词（如 `the`、`of`）出现非常频繁  
- 绝大多数词出现频率低  
- 双对数坐标下，词序号与频率点近似落在直线上

**对语言模型的影响**：

- 频率分布极不均衡，未平滑的 n-gram 模型表现差  
- 低频词或未出现的组合概率为 0，需要平滑技术解决

## 3.5 多元语法与组合建模

- 任意词理论上都有可能出现，因此通常使用 **n-gram** 考虑词组合  
- 示例：词集 `吃、苹果、桌子、电脑、香蕉、手机`  
  - 两两组合总数 \(6^2 = 36\)  
  - 但并非所有组合合理（如“吃桌子”不常见）  
- 数据训练可学习语言规律，使模型生成合理概率分布


## 3.6 代码实践

### 3.6.1 自然语言统计

**基本统计**：
```python
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
# 将所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
```

![词频统计结果]({{ '/assets/img/notes/rnn/figures/vocab_frequency_stats.png' | relative_url }})


**词频图绘制**：

```python
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```

![alt text]({{ '/assets/img/notes/rnn/figures/image-8.png' | relative_url }})

**Zipf 定律分析**：

在自然语言中，词频与排名的关系可表示为：

$$
n(x) \propto \frac{1}{x}
$$

其中，$n(x)$ 表示排名为 $x$ 的词的出现频率。在对数–对数坐标下，取对数后：

$$
\log n(x) = \log k - \log x
$$

这样，原本的双曲线关系变成了**一条直线**，方便观察和分析。这说明词频分布遵循幂律规律——少数高频词出现频繁，而多数低频词出现稀少。

这告诉我们想要通过计数统计和平滑来建模单词是不可行的，因为这样建模的结果会大大高估尾部单词的频率。


### 3.6.2 二元语法统计

```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
```
![alt text]({{ '/assets/img/notes/rnn/figures/image-9.png' | relative_url }})

### 3.6.3 三元语法统计

```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
```
![alt text]({{ '/assets/img/notes/rnn/figures/image-10.png' | relative_url }})

### 3.6.4 对比分析

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

![alt text]({{ '/assets/img/notes/rnn/figures/image-11.png' | relative_url }})

图表显示，n-gram 的阶数越高，词频分布越稀疏。这意味着我们可以通过这个模型学习和预测。

## 3.7 长序列数据处理

在实际语言建模中，文本序列往往非常长，无法一次性全部输入模型。因此，需要对序列进行拆分，以便模型能够高效读取和训练。

### 1. 序列拆分

- **基本思路**：将长序列切成若干段，然后依次取出训练。
- **问题**：如果每次切分都是固定起点，模型总是看到相同的序列，容易丢失某些数据规律。
- **解决方法**：引入**随机起点** \(k\)。
  - \(k\) 的取值范围在 \((0, \tau)\)，其中 \(\tau\) 是窗口长度。
  - 通过随机起点，确保所有序列顺序都能被模型学习到，增加数据的覆盖性和随机性。

### 2. 窗口滑动与样本生成

- **窗口大小**：保持一致，以便并行训练。
- **生成样本**：
  - 假设序列为 `[1, 2, 3, 4]`，窗口大小为 3：
    ```
    X: [1, 2, 3]
    Y: [2, 3, 4]
    ```
  - 通过滑动窗口，每次生成对应的输入 X 和目标 Y。

### 3. 索引构建

- 数据从语料库加载时，核心在于**创建良好的索引**。
- 索引的作用：
  - 确定每个窗口的起点和终点
  - 保证数据加载顺序正确
  - 支持并行训练时的数据分配
- 利用索引，可以灵活生成训练样本，而不需要每次都操作原始序列。

### 4. 批次处理策略

有两种常见方法：

1. **先分批再切分**：
   - 先将数据分成若干批次
   - 在每个批次内切分序列
   - 可以在切分后打乱批次和序列索引，增加随机性

2. **先切分再分批**：
   - 先按顺序切好所有序列
   - 然后再分成批次
   - 每个批次中再进行序列切分
   - 切分次数与批次数量一致
   - 最终生成训练样本

### 5. 并行训练考虑

- 窗口长度相等保证每个批次可以并行处理
- 随机起点和批次打乱确保训练数据覆盖充分，避免模型过度依赖固定序列模式
- 这种处理方式既保证了数据的**覆盖性**，又保持**随机性**，有利于模型更好地学习长序列中的规律

**批次处理逻辑示意图**：


![alt text]({{ '/assets/img/notes/rnn/figures/batch_logic_clear-1.png' | relative_url }})

#### 3.6.6 顺序分区

**核心思路**：
- 从随机偏移量开始，将语料库纵向切分成 `batch_size` 行
- 然后在每行上按 `num_steps` 滑动窗口生成批次
- 与随机采样不同，这种方法保持了序列的相对顺序

**示例说明**：

假设：
```python
corpus = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
batch_size = 2
num_steps = 3
```

处理步骤：
1. `num_tokens = ((18 - 1) // 2) * 2 = 16`
2. 重塑为 2 行：
   ```
   [[0, 1, 2, 3, 4, 5, 6, 7],
    [8, 9, 10, 11, 12, 13, 14, 15]]
   ```
3. `num_batches = 8 // 3 = 2` 个批次
4. 第一个批次：
   ```
   X: [0, 1, 2], [8, 9, 10]
   Y: [1, 2, 3], [9, 10, 11]
   ```
5. 第二个批次：
   ```
   X: [3, 4, 5], [11, 12, 13]
   Y: [4, 5, 6], [12, 13, 14]
   ```

![alt text]({{ '/assets/img/notes/rnn/figures/image-12.png' | relative_url }})

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    """使用顺序分区生成一个小批量子序列"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

#### 3.6.7 数据加载器封装


```python
class SeqDataLoader:
    """加载序列数据的迭代器"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        # 根据参数选择随机或顺序采样
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        # 加载语料库和词表
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    """返回时光机器数据集的迭代器和词表"""
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```

---

## 3.8 练习题


### 1️⃣ 四元语法需要存储多少词频和相邻的词频？

**问题背景**：假设训练数据集中有 100,000 个单词，需要存储 1-gram、2-gram、3-gram、4-gram 的所有频率。

**理论存储量**：

$$
N_{\text{max}} = V + V^2 + V^3 + V^4
$$

其中 $V = 100{,}000$，理论上所有可能的 4-gram 组合为：

$$
100{,}000^4 = 10^{20}
$$

**实际存储**：
- 实际语料中绝大部分组合不会出现
- 因此只存储出现过的 n-gram
- 通过平滑、截断、压缩、哈希或神经网络等方法处理稀疏性


### 2️⃣ 如何对一系列对话建模？

**方法1：统计语言模型（n-gram）**

使用条件概率：

$$
P(w_t \mid w_{t-1}, w_{t-2}, \dots)
$$

- **优点**：简单直观  
- **缺点**：上下文受限、稀疏问题严重

**方法2：神经语言模型（RNN / LSTM / Transformer）**

- 通过神经网络学习上下文依赖关系  
- 参数共享，能建模长距离依赖  
- 适合多轮对话建模

### 3️⃣ 一元、二元、三元语法的齐普夫定律指数是否不同？

**答案**：是的，不同。

齐普夫定律表达式：

$$
f_i \propto \frac{1}{i^\alpha}
$$

其中 $f_i$ 为词频，$i$ 为词排名。

| 语法类型 | 典型指数 $\alpha$ |
|-----------|-------------|
| 一元语法 | ≈ 1.0 |
| 二元语法 | ≈ 1.1–1.2 |
| 三元语法 | ≈ 1.3–1.5 |

**估计方法**：

在对数–对数坐标中拟合直线：

$$
\log f_i = -\alpha \log i + C
$$

斜率的绝对值即为 $\alpha$

### 4️⃣ 读取长序列数据的其他方法

除了随机采样和顺序分区，还有以下方法：

1. **滑动窗口采样**：允许窗口重叠，增加样本数量
2. **按句子/段落分割**：保持语义完整性
3. **按对话轮次采样**：适用于多轮对话任务
4. **动态长度采样**：改变窗口大小（可变 `num_steps`）

### 5️⃣ 为什么使用随机偏移量是个好主意？

使用随机偏移量的优点：

1. **防止模型记住固定分割点**：避免模型过度依赖特定位置
2. **提高样本多样性**：每个 epoch 生成不同的序列切分
3. **减少过拟合**：增强模型的泛化能力

### 6️⃣ 它能实现完美的均匀分布吗？

**答案**：❌ 不能完全实现。

**原因**：
- 偏移量通常只在 `[0, num_steps - 1]` 范围内随机
- 语料库的开头和结尾部分被采样的概率略低
- 分布仍会有略微偏差

### 7️⃣ 如何让分布更均匀？

**为什么需要均匀分布**：
- 模型能更好地学到低频但重要的词组合
- 避免模型过度"偏向高频类"
- 使模型学习更稳定、更全面

**具体方法**：

1. **扩大随机偏移范围**：增加偏移量的随机性
2. **每个 epoch 打乱语料顺序**：增加样本多样性
3. **随机选择句子起点**：保证语义完整性
4. **使用掩码语言建模**：如 BERT 的 MLM 任务

### 8️⃣ 如果序列样本是完整句子，会有什么问题？如何解决？

**问题**：
- 每个句子长度不同，无法组成统一批次（batch）

**解决方案**：

1. **Padding（填充）**：短句补 0，简单但可能浪费计算
2. **Bucketing（分桶）**：按句长分桶，相近长度的句子组成一批
3. **Dynamic Batching（动态批次）**：动态调整 batch 大小
4. **Packed Sequence（打包序列）**：如 PyTorch 的 `pack_padded_sequence`，高效处理变长序列
