---
layout: note_with_toc
title: 5 循环神经网络简洁实现
description: Building RNN from scratch with detailed implementation
category: Machine Learning
tags: [RNN, Deep Learning, Neural Networks, PyTorch, Implementation]
permalink: /notes/循环神经网络简洁实现
---

# 5. 循环神经网络简洁实现

这部分是针对上面复杂的模型的高度浓缩，为了使用更加地方便, 并且在上一节的代码中，并没有涉及到多层的意义。

## 5.1 基本库加载
```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35

train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

```python
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)
```

## 5.2 定义基础模板
```python
#@save
class RNNModel(nn.Module): # 继承自 torch.nn.Module
    """循环神经网络模型"""

    def __init__(self, rnn_layer, vocab_size, **kwargs):

    '''
    定义了内部自动初始化函数

    rnn_layer: 一个 RNN 层对象，可能是 nn.RNN, nn.LSTM, 或 nn.GRU

    vocab_size: 词表大小（比如有多少不同的词）

    **kwargs：kwargs 是 “keyword arguments”（关键字参数）的缩写 

    ** 表示“接收任意数量的关键字参数”，这些参数会被自动打包成一个 字典（dict）

    未来你可能想在子类或别的地方，往构造函数里传更多参数

    def func(a, b, **kwargs):
    print(a, b)
    print(kwargs)

    func(1, 2, x=10, y=20)

        1 2
    {'x': 10, 'y': 20}

    '''


        super(RNNModel, self).__init__(**kwargs)

        把这些参数再传给父类 nn.Module 的构造函数。

        这样如果父类（或更上层的类）需要这些关键字参数，就能正确接收。

        “参数名 = 值” 这个就是关键字参数

        self.rnn = rnn_layer # 1个 RNN 层对象，可能是 nn.RNN, nn.LSTM, 或 nn.GRU

        self.vocab_size = vocab_size 这个就是

        self.num_hiddens = self.rnn.hidden_size 
        
        从 RNN 层中读取隐藏层大小，这里就决定了有多少个隐藏层

        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1

        单向 RNN：时间步 t 只看过去的输入（从左到右）。

        
        双向 RNN：时间步 t 同时看过去和未来（从左→右 + 右→左）

        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

        
        self.linear 一个线性层把它转换为词表大小的输出



    def forward(self, inputs, state):

        首先，每个输出是 inputs.shape = (batch_size, time_steps)


        X = F.one_hot(inputs.T.long(), self.vocab_size) 转化成one-hot 向量 torch.int64 = long()

        X = X.to(torch.float32) one-hot 默认是整型，要转成浮点型

        Y, state = self.rnn(X, state) 前向传播

        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)

        # 它的输出形状是(时间步数*批量大小,词表大小)。 把时间步和批次合并：

        output = self.linear(Y.reshape((-1, Y.shape[-1]))) 

        取 Y 的最后一个维度的大小

        return output, state

        由于这里存在后续对不同RNN的类型，因此，要求也不尽相同。

        nn.RNN / nn.GRU  只有一个隐藏状态张量 H 

        nn.LSTM， 有两个：隐藏状态 H 和 记忆单元 C


    def begin_state(self, device, batch_size=1):

        if not isinstance(self.rnn, nn.LSTM):

            # nn.GRU以张量作为隐状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)

        
            hidden state 形状

            RNN 堆的层数: 

            num_directions: 1（单向）或 2（双向）

            num_hiddens，隐藏向量的长度，或者个数



        LSTM 的输入 hidden state 需要 两部分，返回一个 元组


        '''
        self.num_layers：RNN 堆的层数

        self.num_directions：1（单向）或 2（双向）

        所以第一维 = 总层数 × 方向数

        batch_size：一次喂多少样本

        self.num_hiddens：隐藏层神经元数量（hidden size）

        '''

        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```

```python
#@save
class RNNModel(nn.Module):
    """循环神经网络模型"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
        # 它的输出形状是(时间步数*批量大小,词表大小)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```

## 5.2 测试

```python

device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)

```


```python

num_epochs, lr = 500, 1

d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)

```

![alt text]({{ '/assets/img/notes/rnn/figures/image-7.png' | relative_url }})


## 5.3 问题

### 尝试使用高级API，能使循环神经网络模型过拟合吗？

首先，需要理解过拟合： 

过拟合本质上是 模型容量过大，或者 训练数据量不足，导致模型在训练集上表现很好，但在测试集/验证集上泛化能力差。

    影响 RNN 过拟合的因素：

    模型复杂度：

    隐藏层单元数 (hidden_size) 太大

    堆叠层数 (num_layers) 太多

    双向 RNN → 参数数量翻倍

    训练数据量不足

    训练轮数太多（epoch 太多）

    缺少正则化（dropout、权重衰减等）

高级 API 并不会自动防止过拟合，参数是本身需要你自己调整

### 如果在循环神经网络模型中增加隐藏层的数量会发生什么？能使模型正常工作吗？

```python

需要在这里，添加隐藏层的层数。

rnn_layer = nn.RNN(len(vocab), num_hiddens)


默认参数中：

input_size = len(vocab)

hidden_size = num_hiddens

num_layers = 1（默认一层）

bidirectional = False（默认单向）

rnn_layer = nn.RNN(len(vocab), num_hiddens，num_layers = 10)

```