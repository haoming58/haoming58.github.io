---
layout: note_with_toc
title: 2. æ–‡æœ¬æ•°æ®å¤„ç†
description: Text data preprocessing and tokenization techniques
category: Machine Learning
tags: [RNN, Text Processing, Tokenization, Natural Language Processing]
permalink: /notes/RNNæ–‡æœ¬æ•°æ®å¤„ç†/
---

æ–‡æœ¬æ•°æ®æ˜¯ä¸€ç§**åºåˆ—æ•°æ®**ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨**åºåˆ—å»ºæ¨¡**çš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚åœ¨å»ºæ¨¡ä¹‹å‰ï¼Œéœ€è¦ç†è§£ä¸€äº›åŸºæœ¬æ¦‚å¿µã€‚

### æ ¸å¿ƒè¦ç‚¹

1. **æ–‡æœ¬è½¬æ¢**
   è®¡ç®—æœºæ— æ³•ç›´æ¥ç†è§£åŸå§‹æ–‡æœ¬ï¼Œéœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºè®¡ç®—æœºå¯å¤„ç†çš„**å­—ç¬¦ä¸²æ ¼å¼**ã€‚

2. **åˆ†è¯ï¼ˆTokenizationï¼‰**
   å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºæ›´å°çš„å•ä½ï¼Œç§°ä¸º**è¯å…ƒï¼ˆtokenï¼‰**ã€‚è¯å…ƒå¯ä»¥æ˜¯å•è¯ã€å­è¯æˆ–è€…å­—ç¬¦ã€‚

3. **è¯å…¸æ„å»ºï¼ˆLexiconï¼‰**
   åŸºäºè¯å…ƒæ„å»º**è¯å…¸**ï¼Œé€šå¸¸ä¼šç”¨åˆ°**è¯å½¢è¿˜åŸ**æ¥è§„èŒƒåŒ–è¯å½¢ã€‚è¿™æ ·æœ‰åŠ©äºæ¨¡å‹ç†è§£æ–‡æœ¬ï¼Œä¹Ÿä¾¿äºåç»­çš„æ•°å€¼è½¬æ¢ã€‚

4. **æ•°å€¼åŒ–ï¼ˆNumerical Conversionï¼‰**
   è¯å…ƒéœ€è¦è¢«è½¬æ¢ä¸º**æ•°å€¼ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰**ã€‚è®¡ç®—æœºæœ€ç»ˆä»¥äºŒè¿›åˆ¶å­˜å‚¨æ•°å€¼ï¼Œæ¨¡å‹ä¹Ÿåªèƒ½å¤„ç†æ•°å€¼è¾“å…¥ã€‚

---

## 2.1 è¯å…ƒï¼ˆTokenï¼‰

æ–‡æœ¬é¢„å¤„ç†ä¸€èˆ¬åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ–‡æœ¬æ¸…æ´—**

   * å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸º**å°å†™**ã€‚
   * ç§»é™¤æ‰€æœ‰**éå­—æ¯å­—ç¬¦**ã€‚
   * å°†å•è¯é—´çš„åˆ†éš”ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰æ›¿æ¢ä¸ºç©ºæ ¼ã€‚

   å¯ä½¿ç”¨ Python çš„ `re.sub()` å‡½æ•°å®ç°ã€‚

2. **è¯å…ƒå®šä¹‰**
   è¯å…ƒçš„åŸºæœ¬å•ä½å¯ä»¥æ˜¯**å•è¯**æˆ–è€…**å­—ç¬¦**ã€‚

   ç¤ºä¾‹ï¼š

   * å•è¯çº§è¯å…ƒï¼š`"word"` â†’ `"word"`
   * å­—ç¬¦çº§è¯å…ƒï¼š`"word"` â†’ `"w", "o", "r", "d"`

3. **è¯å…¸åˆ›å»º**
   åˆ†è¯å®Œæˆåï¼Œæ„å»º**è¯å…¸**ï¼Œå°†æ¯ä¸ªè¯å…ƒæ˜ å°„ä¸º**æ•°å€¼ç´¢å¼•**ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚

   ç¤ºä¾‹ï¼š

   | è¯å…ƒ  | ç´¢å¼• |
   | --- | -- |
   | the | 1  |
   | cat | 2  |
   | sat | 3  |
   | on  | 4  |
   | mat | 5  |

---

## 2.2 ä»£ç å®è·µ

### 2.2.1 åŸºæœ¬åº“ 
```python
import collections 

æä¾›äº†å¾ˆå¤šæ–¹ä¾¿çš„æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚ï¼š

Counterï¼šå¯ä»¥ç”¨æ¥ç»Ÿè®¡è¯é¢‘ï¼Œéå¸¸é€‚åˆåšæ–‡æœ¬æ•°æ®å¤„ç†ã€‚
defaultdictï¼šå¸¦é»˜è®¤å€¼çš„å­—å…¸ã€‚

import re 
ç”¨äºæ–‡æœ¬æ¸…æ´—ã€æ¨¡å¼åŒ¹é…ã€æ›¿æ¢ç­‰æ“ä½œã€‚
æŠŠæ–‡æœ¬ä¸­æ‰€æœ‰éå­—æ¯å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼š

from d2l import torch as d2l
```
```python
import collections
import re
from d2l import torch as d2l
```


### 2.2.2 è¯»å–æ•°æ® 

```python
import re
from d2l import torch as d2l  # ç¡®ä¿ä½ å·²å®‰è£… d2l åº“

# ------------------------------
# 1. é…ç½®æ•°æ®é›†ä¸‹è½½ä¿¡æ¯
# ------------------------------
d2l.DATA_HUB['time_machine'] = (
    d2l.DATA_URL + 'timemachine.txt',  # ä¸‹è½½åœ°å€
    '090b5e7e70c295757f55df93cb0a180b9691891a'  # SHA-1 æ ¡éªŒå€¼
)

# è§£é‡Šï¼š
# ä»å³å¾€å·¦çœ‹ï¼š
# d2l.DATA_URL = 'https://d2l-data.s3-accelerate.amazonaws.com/'
# d2l.DATA_URL + 'timemachine.txt' = 'https://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt'
# '090b5e7e70c295757f55df93cb0a180b9691891a' æ˜¯ SHA-1 æ ¡éªŒå€¼ï¼Œç”¨äºéªŒè¯æ–‡ä»¶å®Œæ•´æ€§
# å½“ä¸‹è½½æ–‡ä»¶æ—¶ï¼Œd2l.download ä¼šè‡ªåŠ¨è®¡ç®—æœ¬åœ°æ–‡ä»¶çš„ SHA-1ï¼Œå¦‚æœä¸ä¸€è‡´ä¼šé‡æ–°ä¸‹è½½

# DATA_HUB æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥å­˜å‚¨æ•°æ®é›†ä¿¡æ¯
DATA_HUB = {
    'time_machine': (
        'https://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt',
        '090b5e7e70c295757f55df93cb0a180b9691891a'
    ),
    'dataset_name2': ('url2', 'sha1_checksum2'),  # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ•°æ®é›†
}

# ------------------------------
# 2. å®šä¹‰è¯»å–å‡½æ•°
# ------------------------------
def read_time_machine():  # @save
    """å°†ã€Šæ—¶é—´æœºå™¨ã€‹æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œåˆ—è¡¨ä¸­ï¼Œå¹¶è¿›è¡Œç®€å•é¢„å¤„ç†"""
    
    # ä½¿ç”¨ with è¯­å¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰æ‰“å¼€æ–‡ä»¶
    # ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨åœ¨ä½¿ç”¨å®Œæ–‡ä»¶åå…³é—­æ–‡ä»¶ï¼Œé¿å…å ç”¨ç³»ç»Ÿèµ„æº
    # open(file_path, 'r') ä¸­ 'r' è¡¨ç¤ºåªè¯»æ¨¡å¼
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()  # æŒ‰è¡Œè¯»å–æ–‡ä»¶ï¼Œè¿”å›åˆ—è¡¨ï¼Œæ¯è¡Œæ˜¯åˆ—è¡¨å…ƒç´ 

    # æ•°æ®é¢„å¤„ç†ï¼š
    # 1. åªä¿ç•™è‹±æ–‡å­—æ¯ï¼Œå°†å…¶ä»–å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    # 2. å»æ‰è¡Œé¦–å°¾ç©ºæ ¼
    # 3. å…¨éƒ¨è½¬æ¢ä¸ºå°å†™
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

    # è§£æè¯´æ˜ï¼š
    # re.sub(pattern, repl, string)  -> ç”¨ repl æ›¿æ¢ string ä¸­æ‰€æœ‰åŒ¹é… pattern çš„éƒ¨åˆ†
    # '[^A-Za-z]+' -> åŒ¹é…æ‰€æœ‰éå­—æ¯å­—ç¬¦ï¼ˆè¿ç»­ä¸€æ¬¡æˆ–å¤šæ¬¡ï¼‰
    # strip()      -> å»æ‰å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
    # lower()      -> è½¬æ¢ä¸ºå°å†™
    # åˆ—è¡¨æ¨å¯¼å¼   -> éå† lines åˆ—è¡¨ï¼Œæ¯è¡Œè¿›è¡Œå¤„ç†å¹¶ç”Ÿæˆæ–°çš„åˆ—è¡¨

    æ€»çš„æ¥è¯´ï¼Œé™¤äº† å­—æ¯ä»¥å¤–çš„å…¶ä»–å­—ç¬¦ï¼Œç”¨ç©ºæ ¼å»è¡¨ç¤ºæˆ–è€…ä»£æ›¿


# ------------------------------
# 3. ä½¿ç”¨å‡½æ•°è¯»å–å¹¶æŸ¥çœ‹æ•°æ®
# ------------------------------

lines = read_time_machine() ä½¿ç”¨å‡½æ•°ï¼Œè·å¾—å¹²å‡€çš„æ–‡æœ¬åˆ—è¡¨ã€‚

print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')
print('ç¬¬1è¡Œ:', lines[0])
print('ç¬¬11è¡Œ:', lines[10])
```

```python
#@save
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')

def read_time_machine():  #@save
    """å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½åˆ°æ–‡æœ¬è¡Œçš„åˆ—è¡¨ä¸­"""
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

lines = read_time_machine()
print(f'# æ–‡æœ¬æ€»è¡Œæ•°: {len(lines)}')
print(lines[0])
print(lines[10])
```

### 2.2.3 è¯å…ƒåŒ–

åœ¨linesåˆ—è¡¨ä¸­ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€è¡Œæ–‡æœ¬ï¼Œç°åœ¨è¦åšçš„å°±æ˜¯ï¼Œå°†ä¸€è¡Œçš„æ–‡æœ¬ï¼Œè½¬æ¢åˆ°æœ€å°çš„åŸºæœ¬çš„å•ä½ï¼Œè¯å…ƒtokenï¼š WORD /W O R D


```python 
def tokenize(lines, token='word'):  #@save 
    """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ""" # é»˜è®¤ æ˜¯ word


    if token == 'word': å¦‚æœ token = wordï¼Œ ä¸€ä¸ªè¯è¯­ï¼Œ theï¼Œworldï¼Œä¹‹ç±»

        return [line.split() for line in lines]

        å°†å­—ç¬¦ä¸²æŒ‰ç…§ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ \tã€æ¢è¡Œ \n ç­‰ï¼‰æ‹†åˆ†æˆä¸€ä¸ªåˆ—è¡¨

        line = "  Hello   world  Python "

        ['Hello', 'world', 'Python']

    elif token == 'char': è¿™ä¸ªå°±æ˜¯é’ˆå¯¹å•ä¸ªçš„å­—æ¯ï¼Œ A,a ä¹‹ç±»

        return [list(line) for line in lines] 

        å°†å­—ç¬¦ä¸²æ‹†åˆ†æˆå•ä¸ªå­—ç¬¦ï¼Œç”Ÿæˆä¸€ä¸ªåˆ—è¡¨

        line = "Hello"

        ['H', 'e', 'l', 'l', 'o']

    else:

        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)


tokens = tokenize(lines) è¿™é‡Œé»˜è®¤çš„word

for i in range(11):
    print(tokens[i])
['the', 'time', 'machine', 'by', 'h', 'g', 'wells']
[]
[]
[]
[]

è¿™é‡Œçš„ç©ºæ ¼å°±æ˜¯ä¹‹å‰æåˆ°çš„ä¸æ˜¯å­—æ¯ç±»åˆ«çš„ä¸œè¥¿ï¼Œå°±ä¼šæ›¿æ¢æˆ[].

```

```python 
def tokenize(lines, token='word'):  #@save
    """å°†æ–‡æœ¬è¡Œæ‹†åˆ†ä¸ºå•è¯æˆ–å­—ç¬¦è¯å…ƒ"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('é”™è¯¯ï¼šæœªçŸ¥è¯å…ƒç±»å‹ï¼š' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])
```

### 2.2.3 è¯è¡¨åŒ–

è¯è¡¨åŒ–å‡½æ•°æœ‰ç‚¹å¤æ‚ï¼Œæ‹†åˆ†è®²è§£ï¼Œä»¥åŠä¸è¦å¸Œæœ›é€šè¿‡ä¸€æ¬¡å°±èƒ½å®Œæ•´åœ°ç†è§£åº”ç”¨

```python 
class Vocab:
    """æ–‡æœ¬è¯è¡¨"""
    
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        """
        åˆå§‹åŒ–å‡½æ•°
        
        self:
          è®©å˜é‡å˜æˆå®ä¾‹å˜é‡ï¼ˆå±äºå¯¹è±¡ï¼Œä¸æ˜¯ä¸´æ—¶çš„ï¼‰
          è®©æ–¹æ³•å˜æˆå®ä¾‹æ–¹æ³•ï¼ˆå¿…é¡»é€šè¿‡å¯¹è±¡æ¥è°ƒç”¨ï¼‰
        
        å‚æ•°:
        tokens: è¾“å…¥çš„æ–‡æœ¬åºåˆ—ï¼Œé€šå¸¸æ˜¯åˆ†å¥½è¯çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ["æˆ‘", "çˆ±", "å­¦ä¹ ", "å­¦ä¹ "]
        min_freq: è¯çš„æœ€å°å‡ºç°é¢‘ç‡ï¼Œå¦‚æœä¸€ä¸ªè¯å‡ºç°æ¬¡æ•°å°äºè¿™ä¸ªå€¼ï¼Œå°±ä¸åŠ å…¥è¯è¡¨
        reserved_tokens: é¢„ç•™çš„ç‰¹æ®Šç¬¦å·åˆ—è¡¨ï¼Œæ¯”å¦‚ ["<pad>", "<bos>", "<eos>"]
        """
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []

        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)  # è¿”å›å­—å…¸ {'è¯': é¢‘ç‡}
        
        # sorted(iterable, key=None, reverse=False)
        # iterableï¼šå¯è¿­ä»£å¯¹è±¡ï¼ˆåˆ—è¡¨ã€å…ƒç»„ã€å­—å…¸ã€å­—ç¬¦ä¸²ç­‰ï¼‰
        # keyï¼šä¸€ä¸ªå‡½æ•°ï¼Œå‘Šè¯‰PythonæŒ‰ä»€ä¹ˆè§„åˆ™æ’åº
        # key=lambda x: x[1] è¡¨ç¤ºæŒ‰ç…§å…ƒç»„é‡Œçš„ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆè¯é¢‘ï¼‰æ’åº
        # counter.items()ï¼š [('å­¦ä¹ ', 2), ('æˆ‘', 1), ('çˆ±', 1)] 
        # reverseï¼šæ˜¯å¦åè½¬ï¼ˆé»˜è®¤Falseâ†’å‡åºï¼›Trueâ†’é™åºï¼‰
        # æ ¹æ®è¯é¢‘é™åºæ’åºï¼Œå¾—åˆ°[(è¯, é¢‘ç‡), ...]çš„åˆ—è¡¨
        # _ è¡¨ç¤ºå†…éƒ¨ä½¿ç”¨ï¼Œä½†æ˜¯ä¹Ÿå¯è°ƒç”¨
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        self.idx_to_token = ['<unk>'] + reserved_tokens
    
        """
        {key_expression: value_expression for item in iterable if condition}
        """
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}

        # åˆ›å»ºè¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸
        # enumerate() çš„ä½œç”¨æ˜¯åœ¨éå†å¯è¿­ä»£å¯¹è±¡æ—¶ï¼ŒåŒæ—¶å¾—åˆ°å…ƒç´ çš„ç´¢å¼•å’Œå€¼
        # enumerate(self.idx_to_token) è¿”å›ï¼š[(0, '<unk>'), (1, '<pad>'), (2, '<bos>'), (3, '<eos>')]
        # token: idx for idx, token æ˜¯å­—å…¸æ¨å¯¼å¼çš„å¿«æ·æ–¹å¼


        # å°†é«˜é¢‘è¯åŠ å…¥è¯è¡¨
        for token, freq in self._token_freqs: # {'è¯': é¢‘ç‡} ä¸€ç³»åˆ—çš„å­—å…¸ï¼Œæå–åˆ°token, freqï¼Œæ¯ä¸€ä¸ªè¯å…ƒ
            if freq < min_freq: # è¿™é‡Œçš„ç›®çš„æ˜¯å¯¹äºé‚£äº›ä¸æ€ä¹ˆç”¨çš„è¯è¯­ç›´æ¥å°±ä¸ç®¡
                break

            if token not in self.token_to_idx: # è¿™é‡Œå°±æ˜¯æŠŠæ²¡æœ‰åœ¨è¯è¡¨é‡Œé¢çš„åŠ å…¥è¿›å»é‡å¤2æ­¥æ“ä½œæ­¥éª¤ã€‚

                self.idx_to_token.append(token) 
      # å…ˆæ˜¯åŠ å…¥è¯å…ƒï¼Œå‡è®¾åˆ—è¡¨é•¿åº¦åŸæ¥æ˜¯ Nï¼Œé‚£ä¹ˆæ–°åŠ å…¥çš„è¯å…ƒç´¢å¼•å°±æ˜¯ Nï¼ˆå› ä¸º Python çš„åˆ—è¡¨ç´¢å¼•ä» 0 å¼€å§‹ï¼‰
                self.token_to_idx[token] = len(self.idx_to_token) - 1 åŠ¨æ€æ·»åŠ 
                # å› ä¸ºåˆ—è¡¨ç´¢å¼•ä» 0 å¼€å§‹ï¼Œæœ€åä¸€ä¸ªå…ƒç´ çš„ç´¢å¼•æ˜¯ len-1ï¼Œæ·»åŠ ç´¢å¼•åˆ°å¯¹åº”çš„å­—å…¸çš„å­—ç¬¦
    """
          idx_to_token = ['<pad>', '<unk>', 'the', 'cat']
          token_to_idx = {'<pad>':0, '<unk>':1, 'the':2, 'cat':3}

          self.idx_to_token.append('dog')
          # idx_to_token = ['<pad>', '<unk>', 'the', 'cat', 'dog']

          self.token_to_idx['dog'] = len(self.idx_to_token) - 1
          # len(idx_to_token) = 5
          # len(idx_to_token) - 1 = 4
          # token_to_idx = {'<pad>':0, '<unk>':1, 'the':2, 'cat':3, 'dog':4}

   """
        æ€»ç»“ï¼šæ€»çš„æ¥è¯´ï¼Œæœ‰3ä¸ªå…³é”®ç‚¹æˆ–å­˜å‚¨ç»“æ„ï¼Œæ‰€ç”¨åˆ°çš„æ•°æ®ç»“æ„æ˜¯åˆ—è¡¨ã€å…ƒç»„ã€å­—å…¸çš„ç›¸äº’ç»“åˆï¼š

        1. self._token_freqsï¼šå­˜å‚¨è¯é¢‘ä¿¡æ¯ï¼Œæ ¼å¼ä¸º [('è¯', é¢‘ç‡), ('è¯', é¢‘ç‡), ...]
        2. self.idx_to_tokenï¼šå­˜å‚¨ç´¢å¼•åˆ°è¯å…ƒçš„æ˜ å°„ï¼ŒåŒ…æ‹¬ç‰¹æ®Šè¯å…ƒ
        3. self.token_to_idxï¼šå­˜å‚¨è¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„

        å¤„ç†æµç¨‹ï¼š
        - é¦–å…ˆåŸºäºå®Œæ•´è¯é¢‘ç»Ÿè®¡åˆ›å»ºåŸºç¡€è¯è¡¨
        - ç„¶åé€šè¿‡forå¾ªç¯é€ä¸ªåˆ†æé«˜é¢‘è¯å¹¶æ·»åŠ åˆ°è¯è¡¨
        - æ¯æ¬¡æ·»åŠ æ–°è¯æ—¶ï¼ŒåŒæ—¶æ›´æ–°ä¸¤ä¸ªæ˜ å°„ç»“æ„ä»¥ä¿æŒåŒæ­¥

        ä¸¾ä¾‹è¯´æ˜æ·»åŠ æ–°è¯ 'hello' çš„è¿‡ç¨‹ï¼š

        # ç¬¬ä¸€æ­¥ï¼šå°†è¯å…ƒåŠ å…¥åˆ—è¡¨
        idx_to_token.append('hello')
        # idx_to_token ç°åœ¨ = ['<unk>', '<pad>', '<bos>', '<eos>', 'hello']

        # ç¬¬äºŒæ­¥ï¼šåœ¨å­—å…¸ä¸­è®°å½•å¯¹åº”çš„ç´¢å¼•
        token_to_idx['hello'] = len(idx_to_token) - 1
        # len(idx_to_token) = 5 â†’ ç´¢å¼• = 4
        # token_to_idx = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, 'hello': 4}

       è¿™é‡Œï¼Œæ‰‹åŠ¨æŠŠæ•°å€¼è¾“å…¥è¿›å»ç»™å­—å…¸ï¼Œç„¶åå°±æ˜¯hello æ²¡æœ‰valueï¼Œæ‰€ä»¥å°±è¢«èµ‹äºˆå€¼äº†ã€‚ç´¢å¼•ã€‚
       ç‰¹æ®Šè¯å…ƒæœ‰å›ºå®šçš„ä¼˜å…ˆç´¢å¼•ï¼ˆ0,1,2,3ï¼‰
```

```python
    def __len__(self):
        return len(self.idx_to_token)

       è¿™å°±æ˜¯ç›´æ¥è¿”å›è¯è¡¨æ•´ä½“çš„é•¿åº¦æ˜¯å¤šå°‘ã€‚

       æ—¢ç„¶æœ‰äº†è¯è¡¨ï¼Œä¸‹é¢è¦åšçš„å°±æ˜¯2ç‚¹ï¼š

       1. æ ¹æ®ç´¢å¼•æ‰¾åˆ°è¯å…ƒ
       2. æ ¹æ®è¯å…ƒæ‰¾åˆ°ç´¢å¼•
      

    def __getitem__(self, tokens): æ ¹æ®è¯å…ƒæ‰¾åˆ°ç´¢å¼•

        é¦–å…ˆçœ‹æ˜¯ä¸€ä¸ªè¯ï¼Œè¿˜æ˜¯å¤šä¸ªè¯ã€‚å…ˆåˆ¤æ–­
            if not isinstance(tokens, (list, tuple)):
                # åˆ¤æ–­ä¼ å…¥çš„æ˜¯å•ä¸ª token è¿˜æ˜¯ token åˆ—è¡¨ã€‚
                # å¦‚æœ tokens ä¸æ˜¯ list æˆ– tupleï¼Œå°±è®¤ä¸ºæ˜¯å•ä¸ª token

                # tokens = "I"
                # æˆ–è€…äºŒç»´åˆ—è¡¨çš„æƒ…å†µ
                # tokens = [["I", "am", "this"], ["You", "are", "that"]]

                return self.token_to_idx.get(tokens, self.unk)
                ict.get(key, default) çš„ä½œç”¨:value = some_dict.get(key, default)
                # ä½¿ç”¨è¯å…ƒåˆ°æ•°å­—çš„æ˜ å°„è¡¨ self.token_to_idx æ¥æŸ¥æ‰¾ tokens å¯¹åº”çš„ç´¢å¼•
                # å¦‚æœ tokens ä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œå°±è¿”å› self.unk å°±æ˜¯è®¾ç½®çš„defautï¼ˆé€šå¸¸æ˜¯æœªçŸ¥ token çš„ç´¢å¼•ï¼‰
                ict.get(key, default) çš„ä½œç”¨:value = some_dict.get(key, default)
                token_to_idx è¿™ä¸ªå˜é‡æ˜¯å­—å…¸
                self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
                è¿™é‡Œ0å°±ä»£è¡¨æœªçŸ¥ã€‚
            
            return [self.__getitem__(token) for token in tokens] é’ˆå¯¹å¤šä¸ªè¯ï¼Œè·å–å¤šä¸ªç´¢å¼•ã€‚
    
            # å¦‚æœ tokens æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œå°±é€’å½’è°ƒç”¨ __getitem__ï¼Œå°±æ˜¯å¾ªç¯ï¼Œä¸€è¡Œä¸­çš„æ¯ä¸€ä¸ªè¯ã€‚

            # å°†åˆ—è¡¨é‡Œçš„æ¯ä¸ª tokenï¼ˆæˆ–å­åˆ—è¡¨ï¼‰éƒ½è½¬æ¢æˆç´¢å¼• æ•´ç†
            ä¸‹é¢æ˜¯ä¾‹å­ï¼š
            vocab[["I", ["am", "you"]]]  # è¿”å› [1, [2, 3]]ï¼Œåˆ—è¡¨ä¸­å¯ä»¥åµŒå¥—åˆ—è¡¨

    def to_tokens(self, indices): æ ¹æ®ç´¢å¼•åˆ°å¯¹åº”çš„è¯ï¼Œå…¶è§„åˆ™å’Œä¸Šé¢ç±»ä¼¼
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0ï¼Œè¿™é‡Œå°±åˆšå¥½ç”¨åˆ°def __getitem__(self, tokens)
        return 0

    @property å¤–éƒ¨å¯ä»¥è¯»å– token_freqsï¼Œä½†ä¸èƒ½ç›´æ¥ä¿®æ”¹ _token_freqsï¼Œå°±æ˜¯self._token_freqs æ˜¯ç§æœ‰å˜é‡

    def token_freqs(self):
        return self._token_freqs

    def count_corpus(tokens):  # @save è¿™ä¸ªç¨‹åºå°±æ˜¯ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡
        """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡"""
        # tokens å¯ä»¥æ˜¯ 1D åˆ—è¡¨ï¼ˆ["I", "am", "you"]ï¼‰æˆ– 2D åˆ—è¡¨ï¼ˆ[["I","am"],["you","are"]])
        
        if len(tokens) == 0 or isinstance(tokens[0], list):
            # å¦‚æœ tokens ä¸ºç©ºï¼Œæˆ–è€… tokens[0] æ˜¯åˆ—è¡¨ï¼Œè¯´æ˜ tokens æ˜¯äºŒç»´åˆ—è¡¨
            # éœ€è¦æŠŠäºŒç»´åˆ—è¡¨å±•å¹³æˆä¸€ç»´åˆ—è¡¨
            tokens = [token for line in tokens for token in line]
            # è¿™é‡Œä½¿ç”¨äº†åˆ—è¡¨æ¨å¯¼å¼ï¼š
            # line éå† tokens ä¸­çš„æ¯ä¸€è¡Œï¼ˆæ¯ä¸ªå­åˆ—è¡¨ï¼‰
            # token éå†æ¯ä¸€è¡Œä¸­çš„ token
            # æœ€ç»ˆç”Ÿæˆä¸€ä¸ªä¸€ç»´çš„ tokens åˆ—è¡¨

        # ä½¿ç”¨ collections.Counter æ¥ç»Ÿè®¡æ¯ä¸ª token å‡ºç°çš„æ¬¡æ•°
        return collections.Counter(tokens)

                tokens1 = ["I", "am", "you", "I"]
        print(count_corpus(tokens1))  
        # è¾“å‡º: Counter({'I': 2, 'am': 1, 'you': 1})

        tokens2 = [["I", "am"], ["you", "are", "I"]]
        print(count_corpus(tokens2))
        # è¾“å‡º: Counter({'I': 2, 'am': 1, 'you': 1, 'are': 1})
        ä¸¥æ ¼æ¥è¯´ Counter æ˜¯å­—å…¸çš„å­ç±»ï¼Œæ‰€ä»¥å¯ä»¥æŠŠå®ƒå½“ä½œå­—å…¸æ¥ä½¿ç”¨ï¼š
``` 

```python
class Vocab:  #@save
    """æ–‡æœ¬è¯è¡¨"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡"""
    # è¿™é‡Œçš„tokensæ˜¯1Dåˆ—è¡¨æˆ–2Dåˆ—è¡¨
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä¸€ä¸ªåˆ—è¡¨
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

### 2.2.4 æ•´åˆ

```python
vocab = Vocab(tokens)
print(list(vocab.token_to_idx.items())[:10])
æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ‰€ä»¥ .items() è¿”å›çš„æ˜¯å­—å…¸çš„é”®å€¼å¯¹è§†å›¾ï¼Œlist() æŠŠå­—å…¸è§†å›¾è½¬æ¢æˆåˆ—è¡¨
```

```python
æœ‰10ä¸ªå­åˆ—è¡¨
for i in [0, 10]: è¿™ä¸ªæ˜¯è¡¨ç¤ºå–[0] [10] ä¸¤ä¸ª
    print('æ–‡æœ¬:', tokens[i])
    print('ç´¢å¼•:', vocab[tokens[i]])
```


```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨"""
    lines = read_time_machine() è¯»æ•°æ®é›†
    tokens = tokenize(lines, 'char') æ¸…ç†å¹²å‡€å’Œè¯å…ƒåŒ–
    vocab = Vocab(tokens) ç”Ÿæˆè¯è¡¨

    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­

    corpus = [vocab[token] for line in tokens for token in line] 
    vocab[token] æ˜¯ vocab.__getitem__(token)
    å½“ä½ ä½¿ç”¨ [] è®¿é—®å¯¹è±¡æ—¶ï¼ŒPython è‡ªåŠ¨è°ƒç”¨å¯¹è±¡çš„ __getitem__ æ–¹æ³•
    è¿™äº›å°±å±äºè¯­æ³•çš„ä¸€äº›åŸºæœ¬åŠŸ
    tokens = [["t", "h", "e"], ["t", "i", "m", "e"]]
    {"<unk>":0, "t":1, "h":2, "e":3, "i":4, "m":5}
    corpus = [1, 2, 3, 1, 4, 5, 3]  # ä¸€ç»´ç´¢å¼•åˆ—è¡¨


    if max_tokens > 0: å¦‚æœ max_tokens è®¾å®šäº†æœ€å¤§é•¿åº¦ï¼Œå°±åªä¿ç•™å‰ max_tokens ä¸ª tokenã€‚ï¼Œé»˜è®¤ä¸º -1ï¼Œè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨è¯å…ƒ
        corpus = corpus[:max_tokens] å–å‰max_tokens çš„æ•°å­—ï¼Œç´¢å¼•
    return corpus, vocab
1
corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

### 2.2.4 é—®é¢˜
 - é—®é¢˜ä¸€ å¸¸è§çš„è¯å…ƒåŒ–
 1. æ­£åˆ™è¡¨è¾¾å¼è¯å…ƒåŒ–ï¼ˆRegex Tokenizationï¼‰

```python
import re
text = "ChatGPT is great, isn't it?"
tokens = re.findall(r"\b\w+\b", text) å»æ‰æ ‡ç‚¹ã€è¯†åˆ«å•è¯ã€æ•°å­—
print(tokens)

['ChatGPT', 'is', 'great', 'isn', 't', 'it']
```

 2. NLTK åˆ†è¯å™¨ï¼ˆWord Tokenizerï¼‰

```python
import nltk
from nltk.tokenize import word_tokenize 
nltk.download('punkt')
text = "ChatGPT is great, isn't it?"
tokens = word_tokenize(text) æ”¯æŒç¼©å†™ã€æ ‡ç‚¹
print(tokens)

['ChatGPT', 'is', 'great', ',', "isn't", 'it', '?']
```

 3. ä¸­æ–‡åˆ†è¯ï¼šjieba

```python
import jieba
text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = jieba.lcut(text) é’ˆå¯¹ä¸­æ–‡è®¾è®¡ï¼Œæ•ˆæœä¼˜ç§€ã€‚
print(tokens)

['æˆ‘', 'çˆ±', 'è‡ªç„¶è¯­è¨€å¤„ç†']
```

 - é—®é¢˜äºŒ æ”¹å˜Vocabå®ä¾‹çš„min_freqå‚æ•°
ä¸ºäº†ï¼Œæ–¹ä¾¿ï¼Œæˆ‘ç›´æ¥ä¿®æ”¹ä¸‹é¢çš„
 ```python
def load_corpus_time_machine(max_tokens=-1):  #@save
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¯å…ƒç´¢å¼•åˆ—è¡¨å’Œè¯è¡¨"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokensï¼Œmin_freq=100/1000/10000) è‡ªå·±æ¢å•Š
    # å› ä¸ºæ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œ
    # æ‰€ä»¥å°†æ‰€æœ‰æ–‡æœ¬è¡Œå±•å¹³åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```

æ˜æ˜¾çœ‹å‡ºï¼Œä¼šå¯¹è¯è¡¨çš„å¤§å°æœ‰å½±å“ï¼š


ä¸‹é¢æˆ‘ä»¬æ¥è¯¦ç»†è§£é‡Šå®ƒå¯¹ **è¯è¡¨å¤§å°ï¼ˆvocabulary sizeï¼‰** çš„å½±å“ğŸ‘‡

ä¸¾ä¸ªä¾‹å­ï¼š

å‡è®¾è¯­æ–™ä¸­çš„è¯é¢‘ç»Ÿè®¡ç»“æœå¦‚ä¸‹ï¼š

| å•è¯       | é¢‘æ¬¡  |
| -------- | --- |
| the      | 120 |
| deep     | 30  |
| learning | 20  |
| python   | 5   |
| awesome  | 1   |
| amazing  | 1   |

### æƒ…å†µ 1ï¼š`min_freq=1`

â†’ æ‰€æœ‰å•è¯éƒ½ä¿ç•™
**è¯è¡¨å¤§å° = 6**

### æƒ…å†µ 2ï¼š`min_freq=5`

â†’ åªä¿ç•™å‡ºç°â‰¥5æ¬¡çš„å•è¯
**è¯è¡¨å¤§å° = 4**ï¼ˆå³ `['the', 'deep', 'learning', 'python']`ï¼‰

### æƒ…å†µ 3ï¼š`min_freq=20`

â†’ åªä¿ç•™å‡ºç°â‰¥20æ¬¡çš„å•è¯
**è¯è¡¨å¤§å° = 2**ï¼ˆå³ `['the', 'deep']`ï¼‰

---
